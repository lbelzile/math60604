
### Transformation de la variable réponse {#sec-transfo}

Si la réponse est strictement positive, certaines options peuvent atténuer le manque d'additivité, plus particulièrement les relations multiplicatives entre la moyenne et la variance. Si les données sont asymétriques et que la réponse est strictement positive, un modèle log-linéaire peut être plus approprié et les paramètres peuvent être interprétés.


Écrivons le modèle log-linéaire
\begin{align*}
\ln Y = \beta_0+ \beta_1 X_1 +\cdots + \beta_pX_p + \varepsilon;
\end{align*}
à l'échelle originale de la variable réponse, cela représente
\begin{align*}
Y &= \exp\left(\beta_0 +\beta_1 X_1 +\cdots + \beta_pX_p\right)\cdot \exp(\varepsilon),
\end{align*}
et donc
\begin{align*}
\mathsf{E}(Y \mid \mathbf{X}) = \exp(\beta_0 +\beta_1 X_1 +\cdots + \beta_pX_p) \times \mathsf{E}\{\exp(\varepsilon) \mid \mathbf{X}\}.
\end{align*}
Si $\varepsilon \mid \mathbf{x} \sim \mathsf{normal}(\mu,\sigma^2)$, alors $\mathsf{E}\{\exp(\varepsilon) \mid \mathbf{x}\}= \exp(\mu+\sigma^2/2)$ et $\exp(\varepsilon)$ suit une loi lognormale. Une augmentation d'une unité de $X_j$ mène à une augmentation moyenne de $\beta_j$ de $\ln Y$ sans interaction ni terme nonlinéaire pour $X_j$, et cela se traduit par un facteur multiplicatif de $\exp(\beta_j)$ à l'échelle de $Y$. Si $\beta_j=0$, $\exp(\beta_j)=1$ et il n'y a pas de changement, si $\beta_j < 0$, $\exp(\beta_j)<1$ et la moyenne décroît avec $X_j$, et si $\beta_j > 0$, $\exp(\beta_j)>1$ et la moyenne augmente avec $X_j$.

Comparez le rapport de $\mathsf{E}(Y \mid X_1=x+1)$ à $\mathsf{E}(Y \mid X_1=x)$,
\begin{align*}
\frac{\mathsf{E}(Y \mid X_1=x+1, X_2, \ldots, X_p)}{\mathsf{E}(Y \mid X_1=x,  X_2, \ldots, X_p)} = \frac{\exp\{\beta_1(x+1)\}}{\exp(\beta_1 x)} = \exp(\beta_1).
\end{align*}
Ainsi, $\exp(\beta_1)$ donne le rapport des moyennes de $Y$ quand $X_1=x+1$ par rapport à $X_1=x$, *ceteris paribus* (avec les restrictions habituelles). L'interprétation (par rapport à la référence, dépend du signe de $\beta_j$: le pourcentage de diminution est $1-\exp(\beta_j)$ si $\beta_j <0$ et le pourcentage d'augmentation est $\exp(\beta_j)-1$ si $\beta_j>0$.


Parfois, on veut considérer une transformation à la fois de la réponse et d'une variable explicative positive continue, un modèle log-log. Considérons le cas où on prend le logarithme de $Y$ et $X_1$, avec
\begin{align*}
Y= X_1^{\beta_1}\exp(\beta_0 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon)
\end{align*}
En prenant la dérivée par rapport à $X_1>0$, on obtient
\begin{align*}
\frac{\partial Y}{\partial X_1}&= \beta_1 X_1^{\beta_1-1}\exp(\beta_0 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon)= \frac{\beta_1 Y}{X_1}
\end{align*}
et réarranger cette expression nous donne
\begin{align*}
\frac{\partial X_1}{X_1}\beta_1 = \frac{\partial Y}{Y};
\end{align*}
une mesure **d'élasticité partielle**: le coefficient $\beta_1$ est un pourcentage de changement de $Y$ pour chaque pourcentage d'augmentation de $X_1$, *ceteris paribus*.

:::{#exm-loglog}

## Modèle de Cobb-Douglas

Considérons par exemple la fonction de production Cobb--Douglas [@Douglas:1976], qui spécifie que la production économique $Y$ est liée au travail $L$ et au capital $C$ par l'intermédiaire de $\mathsf{E}(Y \mid L, C) = \beta_0C^{\beta}L^{1-\beta}$ avec $\beta \ dans (0,1)$.Si nous prenons les logarithmes des deux côtés (puisque tous les arguments sont positifs), alors 
$\mathsf{E}(\ln Y \mid L, C) = \beta_0^* + \beta_1 \ln C + (1-\beta_1)\ln L$. Nous pourrions ajuster un modèle linéaire avec la réponse $\ln Y - \ln L$ et la variable explicative $\ln C - \ln L$, pour obtenir une estimation du coefficient $\beta_1$, tandis que $\beta_0^*=\ln \beta_0$. Une optimisation sous contrainte serait potentiellement nécessaire pour estimer les paramètres du modèle linéaire résultant si les estimations se situent en dehors de l'espace des paramètres.

:::

:::{#prp-boxcox}

## Transformation de Box--Cox

Avec des données strictement positives, on peut utiliser une transformation de Box--Cox [@Box.Cox:1964],
\begin{align*}
y(\lambda)= \begin{cases}
(y^{\lambda}-1)/\lambda, & \lambda \neq 0\\
\ln(y), & \lambda=0.
\end{cases}
\end{align*}
Les cas de figure $\lambda=-1$ (inverse), $\lambda=1$ (identité) et $\lambda=0$ (logarithme) sont les plus importants car les modèles résultants sont interprétables.


Si on suppose que $\boldsymbol{Y}(\lambda) \sim \mathsf{normal}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}_n)$, alors la vraisemblance est
\begin{align*}
L(\lambda, \boldsymbol{\beta}, \sigma; \boldsymbol{y}, \mathbf{X}) &= (2\pi\sigma^2)^{-n/2} J(\lambda, \boldsymbol{y}) \times\\& \quad \exp \left[ - \frac{1}{2\sigma^2}\{\boldsymbol{y}(\lambda) - \mathbf{X}\boldsymbol{\beta}\}^\top\{\boldsymbol{y}(\lambda) - \mathbf{X}\boldsymbol{\beta}\}\right],
\end{align*}
où $J$ dénote le Jacobien de la transformation Box--Cox, $J(\lambda, \boldsymbol{y})=\prod_{i=1}^n y_i^{\lambda-1}$.

## Profilage de $\lambda$

Pour chaque valeur de $\lambda$, on obtient les estimateurs du maximum de vraisemblance usuels en remplaçant $\boldsymbol{y}$ par $\boldsymbol{y}(\lambda)$.

La log vraisemblance profilée pour $\lambda$ est
\begin{align*}
\ell_{\mathsf{p}}(\lambda) = -\frac{n}{2}\ln(2\pi \widehat{\sigma}^2_\lambda) - \frac{n}{2} + (\lambda - 1)\sum_{i=1}^n \ln(y_i)
\end{align*}

Nous ne pouvons pas comparer les modèles ajustés à $Y_i$ par rapport à $\ln Y_i$ en utilisant, par exemple, des critères d'information ou des tests, parce que les modèles ont des réponses différentes. Nous pouvons toutefois utiliser la vraisemblance de Box--Cox, qui inclut le **Jacobien** de la transformation, pour évaluer la qualité de l'ajustement et comparer le modèle avec $\lambda=1$ par rapport à $\lambda=0$.



La transformation de Box-Cox n'est pas une panacée et doit être réservée aux cas où la transformation réduit l'hétéroscédasticité (variance inégale) ou crée une relation linéaire entre les explications et la réponse : la théorie fournit une explication convaincante des données. Plutôt qu'un choix *ad hoc* de transformation, on pourrait prendre une transformation logarithmique si la valeur 0$ est incluse dans l'intervalle de confiance à 95%, car cela améliore l'interprétabilité.

:::



:::{#exm-poisonboxcox}

## Transformation de Box--Cox pour données sur les poisons


@Box.Cox:1964 modélisent le temps de survie de 48 animaux sur la base d'un essai aléatoire. Les données sur les `poisons` sont équilibrées, 3 poisons ayant été administrés avec 4 traitements à 4 animaux chacun. Nous pourrions envisager une ANOVA à deux facteurs sans interaction, étant donné le peu d'observations pour chaque combinaison. Le modèle s'écrit alors
\begin{align*}
Y &= \beta_0 + \beta_1 \texttt{poison}_2 + \beta_2\texttt{poison}_3  +\beta_3\texttt{treatment}_2 \\ &\qquad+ \beta_4\texttt{treatment}_3
+\beta_5\texttt{treatment}_4 + \varepsilon
\end{align*}


Le tracé des valeurs ajustées par rapport aux résidus montre que le modèle n'est pas additif (panneau du milieu de la @fig-poisonplots); il y a également des indications que la variance augmente avec la réponse moyenne. Le modèle est inadéquat: les temps de survie les plus faibles sont sous-estimés, ce qui signifie que les résidus sont positifs, de même que les réponses moyennes. Un test formel de non-additivité indique également la non-additivité [@Davison:2003, Exemple 8.24]. Dans l'ensemble, l'ajustement du modèle est médiocre et toute conclusion tirée de celui-ci est douteuse.

On pourrait envisager d'utiliser un Box--Cox pour trouver une transformation appropriée des résidus afin d'améliorer la normalité. Une analyse des résidus dans les quatre premiers graphiques de @fig-poisonplots montre des signes d'hétéroscédasticité en fonction du poison et du traitement. Ceci est évident en regardant le graphique des résidus ordinaires, qui montre une augmentation de la variance avec le temps de survie. Le tracé quantile-quantile dans le tracé du milieu à droite montre quelques signes d'écart par rapport à la normalité, mais la non-linéarité et l'hétéroscédasticité le masquent. 

```{r}
#| label: fig-poisonplots
#| echo: false
#| eval: true
#| fig-cap: "Diagnostics graphiques pour les données de poisons. Panneau du haut: vraisemblance profilée pour $\\lambda$. Panneau du milieu: ($\\lambda=1$, temps de survie) et du bas $(\\lambda=-1$, vitesse d'absorption). Les diagnostics pour les modèles représentent les résidus ordinaires versus valeurs ajustées et diagramme quantile-quantile des résidus studentisés. "
#| fig-align: 'center'
poisons <- SMPracticals::poisons
poisonlm1 <- lm(time ~ poison + treat, data = poisons)
poisonlm2 <- lm(I(1/time) ~ poison + treat, data = poisons)

poisons$resid1 <- resid(poisonlm1)
poisons$rstudent1 <- rstudent(poisonlm1)
poisons$resid2 <- resid(poisonlm2)
poisons$rstudent2 <- rstudent(poisonlm2)
poisons$fitted1 <- fitted(poisonlm1)
poisons$fitted2 <- fitted(poisonlm2)
g1 <- ggplot(data = poisons) +
  geom_point(aes(x = fitted1, y = resid1), position = position_jitter(width = 0.05)) +
  labs(y = "résidus ordinaires", x = "valeurs ajustées")
dp <- list(df=poisonlm1$df.residual-1)
di <- "t"
de <- FALSE
library(qqplotr)
g2 <- ggplot(data = poisons, aes(sample = rstudent1)) +
 stat_qq_band(distribution = di, dparams = dp,
              detrend = de, identity = TRUE,
              bandType = "boot", B = 9999) +
 stat_qq_line(distribution = di, dparams = dp,
              detrend = de, identity = TRUE) +
 stat_qq_point(distribution = di, dparams = dp,
               detrend = de, identity = TRUE) +
 labs(x = "quantiles théoriques",
      y = "quantiles empiriques")
g4 <- ggplot(data = poisons, aes(sample = rstudent2)) +
 stat_qq_band(distribution = di, dparams = dp,
              detrend = de, identity = TRUE,
              bandType = "boot", B = 9999) +
 stat_qq_line(distribution = di, dparams = dp,
              detrend = de, identity = TRUE) +
 stat_qq_point(distribution = di, dparams = dp,
               detrend = de, identity = TRUE) +
 labs(x = "quantiles théoriques",
      y = "quantiles empiriques")
g3 <- ggplot(data = poisons) +
  geom_point(aes(x = fitted2, y = resid2), position = position_jitter(width = 0.05)) +
  labs(y = "résidus ordinaires", x = "valeurs ajustées")
boxcox_gg <- function(fitted.lm, showlambda = TRUE, lambdaSF = 3, grid = seq(-2,2, by = 0.1), scale.factor = 0.5) {
      boxcox_object <- MASS::boxcox(fitted.lm, lambda = grid, plotit = FALSE)
    x <- unlist(boxcox_object$x)
    y <- unlist(boxcox_object$y)
    xstart <- x[-1]
    ystart <- y[-1]
    xend <- x[-(length(x))]
    yend <- y[-(length(y))]
    boxcox_unlist <- data.frame(xstart, ystart, xend, yend)
    best_lambda <- x[which.max(y)]
    rounded_lambda <- round(best_lambda, lambdaSF)
    min_y <- min(y)
    accept_inds <- which(y > max(y) - 1/2 * qchisq(0.95, 1))
    accept_range <- x[accept_inds]
    conf_lo <- round(min(accept_range), lambdaSF)
    conf_hi <- round(max(accept_range), lambdaSF)
    plot <- ggplot(data = boxcox_unlist) + geom_segment(aes(x = xstart,
        y = ystart, xend = xend, yend = yend), linewidth = scale.factor) +
        labs(x = expression(lambda), y = "log vraisemblance profilée") +
        geom_vline(xintercept = best_lambda, linetype = "dotted",
            linewidth = scale.factor/2) + geom_vline(xintercept = conf_lo,
        linetype = "dotted", linewidth = scale.factor/2) + geom_vline(xintercept = conf_hi,
        linetype = "dotted", linewidth = scale.factor/2) + geom_hline(yintercept = y[min(accept_inds)],
        linetype = "dotted", linewidth = scale.factor/2)
    if (showlambda) {
        return(plot +
                 annotate("text", x = best_lambda, label = as.character(rounded_lambda), y = min_y) +
              annotate("text", x = conf_lo, label = as.character(conf_lo), y = min_y) +
              annotate("text", x = conf_hi,
            label = as.character(conf_hi), y = min_y))
    } else {
        return(plot)
    }
}
g5 <- boxcox_gg(poisonlm1, grid = seq(-1.5,0.1, by = 0.01))
 g5 / (g1 + g2) / (g3 + g4)
```


L'intervalle de confiance à 95\% basé sur la log-vraisemblance profilée pour le paramètre du modèle Box--Cox contient $\lambda=-1$. La réciproque de la variable réponse $Y^{-1}$ indique la vitesse d'action du poison, selon le type et le traitement. Les diagnostics graphiques de ce modèles, présentés dans le panneau du bas de la @fig-poisonplots ne montrent aucune structure résiduelle.

:::

<!--
Linear regression is the most famous and the most widely used statistical model around.  The name may appear reductive, but many tests statistics (*t*-tests, ANOVA, Wilcoxon, Kruskal--Wallis) [can be formulated using a linear regression](https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf), while [models as diverse as trees, principal components and deep neural networks are just linear regression model in disguise](https://threadreaderapp.com/thread/1286420597505892352.html). What changes under the hood between one fancy model to the next are the optimization method (e.g., ordinary least squares, constrained optimization or stochastic gradient descent) and the choice of explanatory variables entering the model (spline basis for nonparametric regression, indicator variable selected via a greedy search for trees, activation functions for neural networks).
-->
