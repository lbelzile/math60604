# Données corrélées et longitudinales {#donnees-correlees-longitudinales}

On couvrira dans ce chapitre une extension du modèle linéaire afin de relaxer le postulat d'indépendance entre observations. Ce dernier ne tient en effet pas la route si la base de données contient des mesures répétées sur une unité. On s'intéressera en particulier à l'analyse de données longitudinales, pour lesquelles on a à disposition de brèves séries chronologiques. La clé pour incorporer la dépendance entre mesures est la modélisation parsimonieuse de la structure de covariance, laquelle nous permettra également de traiter les données groupées et de traiter un cas particulier d'hétéroscédasticité pour ces mêmes données.


```{r dependencyfig, eval = TRUE, echo = FALSE, fig.cap="Dependency graph of covariance models for longitudinal data. Arrows indicate nested models."}
```

## Données longitudinales

Dans les **données longitudinales**, aussi appelées données de panel ou mesures répétées, on a des mesures répétées sur la même variables réponse pour $m$ unités ou individus, typiquement à différents temps. Dans ce cadre, il est logique de supposer que les observations de différents individus sont indépendantes, mais pas les mesures pour un même individu. Voici quelques exemples de données longitudinales:

- Un sondage sur la satisfaction face au service à clientèle est envoyé aux clients d'une compagnie suite à un appel de leur part: la variable réponse est la moyenne d'échelles de Likert.
- On prend trois mesures de circonférence d'arbres à différentes hauteurs pour estimer le volume de bois d'une forêt.
- une étude suit une cohorte pour évaluer l'attitude de jeunes adolescents et leur attitudes envers la consommation de drogues et de stupéfiants, avec un suivi annuel.


Règle générale, on s'intéresse à la modélisation de la moyenne de la réponse en fonction d'autres variables explicatives. Si les mesures sont collectées dans le temps, on peut aussi représenter graphiquement l'évolution temporelle en identifiant la variable représentant le temps et en dessinant un diagramme à ligne brisé par individu: ce type de graphique est appelé **diagramme spaghetti**.

Dans ce qui suit, on s'intéresse plus en détail à deux exemples.

```{example, label ="sixvilles", name = "Étude longitudinale sur l'effet de la pollution atmosphérique sur la mortalité dans six villes américaines."}
L'article *An Association Between Air Pollution And Mortality In Six U.S. Cities* est le fruit du travail de chercheurs de l'Université de Harvard [paru dans la revue *The New England Journal of Medicine*](https://www.nejm.org/doi/full/10.1056/NEJM199312093292401) en 1993. L'étude se penchait sur le lien entre la pollution atmosphérique, en particulier l'effet des particules fines, et la mortalité en milieu urbain en suivant une cohorte de 8111 adultes de six villes américains pendant une période allant de 14 à 16 ans. Cette étude longitudinale avait démarrée en 1979. La base de données `fev1` contient 300 filles parmi les 13 379 enfants nés en 1967 ou ultérieurement dans six villes. La plupart des enfants ont été enrollés dans l'étude en première ou deuxième année primaire, soit vers l'âge de six ou sept ans. À chaque examen annuel, on a mesuré le volume d'air évacué durant la premier seconde d'une expiration, appelé volume expiratoire maximal ou VEM1.
```


```{r diagrammespagh1, echo = FALSE, cache = TRUE, fig.cap="Diagramme spaghetti des données de spirométrie de l'étude *Six Cities Study of Air Pollution and Health*.", eval = TRUE}
data(fev1, package = "hecmodstat")
g1 <- ggplot(data=fev1, aes(x = age, y = exp(logvem1))) +
  geom_line(aes(group = id), alpha = 0.1) +
  ylab("volume expiratoire maximal (VEM1)") +
  xlab("âge (en années)")
g1
```

Outre la variable réponse VEM1, on a également à disposition la taille, l'âge et l'identifiant de l'enfant. La Figure \@ref(fig:diagrammespagh1) montre les courbes de croissance du volume expiratoire maximal en fonction de l'âge de la personne. On peut remarquer que la croissance est presque linéaire, avec une diminution de la pente à partir de 14 ans. Si les profils semblent suivre la même tendance, on voit une forte fluctuation autour de la moyenne. Toutes les courbes ne démarrent ni ne finissent au même âge: une analyse exploratoire des identifiants des enfants nous indique qu'on a entre 1 et 12 mesures par enfant, pour un total de 1994 mesures. Les données sont stockées en format long: chaque ligne correspond à une mesure de VEM1, tandis qu'on recense plusieurs mesures par identifiant (`id`). L'âge de la fille (`age`) représente la variable temps, et sa taille (en mètres) évolue au fil des années. L'âge et la taille initiale lors de la première collecte de données sont fixes et identiques pour toutes les mesures d'une même personne.

Conceptuellement, on pourrait envisager que chaque fille a une courbe de croissance spécifique de sa capacité respiratoire et qui est mesurée avec une erreur à cause de la variabilité de la prise de mesure. On peut également envisager une moyenne théorique due à des facteurs biologique, autour duquelle les courbes de croissance des différentes filles fluctuent.


Notre modèle statistique devra prendre en compte que les mesures sur un même sujet sont corrélées: par exemple, si une courbe de croissance est plus élevée que la moyenne globale, elle aura tendance à rester au dessus de cette moyenne peu importe l'âge de la personne. On pourrait également postuler que les fluctuations sont corrélées dans le temps: une fois la moyenne globale soustraite, les mesures d'une même personnes sont plus semblables quand elles sont rapprochées. En revanche, on peut raisonnablement supposer que les observations de filles différentes sont indépendantes. Ainsi, si on s'intéressait uniquement au comportement à un âge donné, on pourrait faire l'analyse d'une coupe transversale des données en calculant la moyenne empirique.


```{r diagrammespagh2, echo = FALSE, cache = TRUE, fig.cap="Diagramme spaghetti  pour les données de l'étude clinique sur la dépression et du programme comportemental *Beating the Blues*.", eval = TRUE}
data(beattheblues, package = "hecmodstat")
btb_long <- tidyr::pivot_longer(data = beattheblues,
                    cols = tidyselect::starts_with("idb"),
                    names_to = "mois",
                    names_prefix = "idb",
                    values_to = "idb",
                    values_drop_na = TRUE)
levels(btb_long$traitement) <- c("Beating the Blues","usuel")
g2 <- ggplot(data = btb_long,
             aes(x = mois, y=idb, group=id, col = traitement)) +
  geom_line(alpha = 0.4) +
  ylab("inventaire de dépression de Beck (IDB-2)") +
  xlab("mois depuis le début du traitements") +
  theme(legend.position = c(0.8, 0.9)) +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_continuous(limits = c(0,65), expand = c(0,0))
# data(dentaire, package = "hecmodstat")
# dentaire_long <- tidyr::pivon_longer(data = dentaire,
#                     cols = tidyselect::starts_with("dist"),
#                     names_to = "age",
#                     names_prefix = "dist",
#                     values_to = "dist"
#                     )
# g3 <- ggplot(data = dentaire_long,
#              aes(x = age, y=dist, group=id, col = genre)) +
#   geom_line(alpha = 0.4) +
#   ylab("mesure de distance orthodontique (en mm)") +
#   xlab("âge") +
#   theme(legend.position = c(0.8, 0.1)) +
#   scale_x_discrete(expand = c(0,0)) +
#   scale_y_continuous(limits = c(0,35), expand = c(0,0))
# g1 / g2 / g3
g2
```

```{example, label ="beatingtheblues", name = "Programme thérapeutique *Beating the Blues*."}

[*Beating the Blues*](https://www.beatingtheblues.co.uk/) est un programme clinique qui vise à traiter les personnes atteintes d'anxiété et de dépression clinique légère ou modéré. Comparativement aux autres thérapies cognitivo-comportementales, les rencontres avec un(e) thérapeute sont remplacées par des capsules vidéos et une prise en charge en ligne pour amener les patients à prendre en charge leur guérison. Les personnes qui se sont joints à l'étude ont été assignés de manière aléatoire à deux traitements, soit *Beating the Blues* ou le traitement usuel. L'objectif dans ce exemple est de comparer l'efficacité relative des traitements, mais puisque les individus ne sont assignés qu'à l'un ou l'autre des traitements, on ne pourra que comparer les différences moyennes de traitement. Le diagramme spaghetti de la Figure \@ref(fig:diagrammespagh2) illustre le comportement erratique de l'évolution de la santé mentale des individus telle que mesurée à l'aide de l'inventaire de dépression de Beck, un questionnaire de 21 items. Si la décroissance semble en moyenne plus grande pour *Beating the Blues*, c'est loin d'être certain: il y a beaucoup de pertes de suivi et la variabilité inter-individus est énorme puisque le point de départ des individus est différent; on pourrait penser à standardiser la courbe par la mesure initiale pour faciliter la comparaison. On remarque que tous les patients sont suivis à intervalle réguliers de deux mois et que les données sont strictement positives.
```

Les données longitudinales ne sont pas toujours stockées dans un format qui soit convenable pour l'analyse. En effet, il est commun d'enregistrer ces données en **format large**, auquel cas chaque ligne représente un individu différent et les colonnes contiennent à la fois des variables explicatives et les différentes répétitions de la variables réponse $y_1, \ldots, y_{n_i}$.

Une base de données en **format long** contient une seule valeur de la variable réponse par ligne, avec une colonne additionnelle indiquant l'identifiant (temporel) de l'observation. On peut passer de format large en format long en transformant les noms de colonnes en étiquettes de la variables d'identification. De même, on suppposera par la suite que les données sont ordonnées par individu, puis chronologiquement.

En format long, les variables explicatives qui sont fixes au sein d'une unité pour toutes les répétitions seront copiées sur chaque ligne. Il faut faire attention au calcul des statistiques descriptives: ces dernières ne seront pas correctes sauf si on ne conserve qu'une copie pour chaque unité. En particulier, l'estimé de l'écart-type de la variable explicative sera sous-estimé parce qu'on gonfle artificiellement la taille de l'échantillon lors de la duplication.

:::keyidea

Des données longitudinales ou mesures répétées comprennent plusieurs mesures pour une même unité, ce qui engendre de la corrélation entre les réponses d'une même unité.

Un diagramme spaghetti (ligne brisée en fonction du temps pour chaque unité) permet de visualiser l'évolution chronologique de la variable réponse.

Lorsqu'on analyse des données longitudinales, on doit identifier par le biais d'une analyse exploratoire plusieurs facteurs qui impacteront nos analyses. Il faut notamment déterminer

- si les données sont enregistrées en format long;
- si les données sont mesurées à des intervalles réguliers;
- s'il y a autant de mesures répétées pour chaque individu (échantillon balancé);
- lesquelles des variables explicatives varient dans le temps et lesquelles sont fixes.

:::

## Modélisation de la matrice de covariance


Pour un vecteur aléatoire $\boldsymbol{Y}$,
on définit la matrice de covariance comme étant la matrice symétrique $n\times n$
\[
\mathsf{Co}(\boldsymbol{Y})=
 \begin{pmatrix}
   \sigma_{1}^2 & \sigma_{12} & \sigma_{13} & \cdots & \sigma_{1n} \\
    \sigma_{21} & \sigma_{2}^2 & \sigma_{23} & \cdots & \sigma_{2n} \\
     \sigma_{31} & \sigma_{32} & \sigma_{3}^2 & \ddots & \sigma_{3n} \\
   \vdots &  \vdots &  \ddots & \ddots &  \vdots \\
       \sigma_{n1} & \sigma_{n2} & \sigma_{n3} & \cdots & \sigma_{n}^2 \\
 \end{pmatrix}.
\]
Le $i$e élément de la diagonale de  $\mathsf{Co}(\boldsymbol{Y})$ est la variance de $Y_i$ et $\boldsymbol{\Sigma}$ est symmétrique, donc $\sigma_{ij}=\sigma_{ji}$. Une matrice de covariance est positive définie, d'où $\mathbf{v}^\top\boldsymbol{\Sigma}\mathbf{v}$ pour tout $n$ vecteur $\mathbf{v}$ non-nul.


Le point de départ de notre analyse est la dérivation de l'estimateur des moindres carrés ordinaires du modèle de régression linéaire sous l'hypothèse que les valeurs de la variable réponses sont conditionnellement indépendantes, normales et homoscédastiques avec $Y_i \mid \mathbf{x}_i \sim \mathsf{No}(\mathbf{x}_i\boldsymbol{\beta}, \sigma^2)$.

Si les données sont conditionnellement normales, la matrice de covariance $\boldsymbol{\Sigma}$ encode la structure de dépendance entre les observations: les éléments hors-diagonale sont nuls si les observations sont indépendantes, tandis que les éléments de la diagonale encodent la variance de chaque mesure. Si les données sont homoscédastiques, alors tous les éléments de la diagonale sont identiques et de la même mesure tous les éléments hors-diagonale valent zéro si les données sont indépendantes.


Une façon alternative d'écrire la vraisemblance d'un échantillon de taille $n$ du modèle linéaire classique est de considérer qu'il s'agit d'une seule réalisation d'un vecteur aléatoire de dimension $n$, avec $\boldsymbol{Y} \sim \mathsf{No}_n(\mathbf{X}\boldsymbol{\beta}, \sigma^2, \mathbf{I}_n)$ où $\mathbf{I}_n$ est la matrice identité $n \times n$. À partir de là, on peut logiquement déduire qu'il suffit de modifier la matrice de covariance $\boldsymbol{\Sigma}$ pour relaxer les postulats d'indépendance et d'homoscédasticité.

Un problème demeure: sans contrainte aucune, la matrice de covariance $\boldsymbol{\Sigma}$ possèdera $n(n+1)/2$ éléments uniques puisqu'elle doit être symmétrique: c'est davantage que le nombre d'observations à disposition! Cette réalité nous contraint donc à paramétriser $\boldsymbol{\Sigma}$ à l'aide d'un modèle doté de quelques paramètres $\boldsymbol{\psi}$ qui pourront être estimés conjointement avec les paramètres de la moyenne $\boldsymbol{\beta}$ par le biais de la méthode du maximum de vraisemblance (et des variantes de cette dernière).



En résumé, la généralisation du modèle linéaire pour prendre en compte la dépendance entre observation passe par la modélisation de la matrice de covariance $\boldsymbol{\Sigma}$ et nous verrons plusieurs modèles possibles pour cette dernière.

La première supposition qui nous permettra de réduire le nombre de paramètres au sein de la matrice de covariance est l'hypothèse que les observations de différentes unités sont**indépendantes**. La deuxième supposition sera le postulat que la structure de covariance intra-unité sera identique pour toutes les unités.

Plus précisément, si les données sont ordonnées par unité et qu'on a $m$ groupes, alors la matrice de covariance $\boldsymbol{\Sigma}$ sera diagonale en bloc,
\begin{align*}
 \mathsf{Co}(\boldsymbol{Y}) = \begin{pmatrix}
                \boldsymbol{\Sigma}_1 & \mathbf{O} & \cdots & \mathbf{O}\\
                 \mathbf{O} &\boldsymbol{\Sigma}_2 & \cdots & \mathbf{O} \\
                 \vdots & \ddots & \ddots & \vdots \\
                  \mathbf{O} & \mathbf{O} & \cdots & \boldsymbol{\Sigma}_m
               \end{pmatrix}.
\end{align*}
où $\mathbf{O}$ dénote une matrice de zéro. La covariance inter-groupe est nulle parce qu’on suppose que les données d’unités différentes sont indépendantes les unes des autres.

Si on paramétrise les sous-blocs $\boldsymbol{\Sigma}_1, \ldots, \boldsymbol{\Sigma}_m$ qui représentent la covariance au sein d'une unité, on supposera que la structure (et les paramètres) seront identiques, en gardant en tête que ces matrices ne sont pas de la même taille si le nombre de réplications temporelles n'est pas identique pour toutes les unités. En particulier, si $n_i$ dénote le nombre de réplications (temporelles) pour l'unité $i$ avec $\sum_{i=1}^m n_i=n$ et que $T=n_1 = \cdots = n_m$, alors chaque matrice bloc sera identique et $\boldsymbol{\Sigma}_1 = \cdots =\boldsymbol{\Sigma}_m$. Les paramètres $\boldsymbol{\psi}$ de la matrice de covariance seront estimables parce qu'on aura $m$ réplications de la matrice.

Souvent, on ne s'intéresse pas aux paramètres de la matrice de covariance, $\boldsymbol{\psi}$: ces derniers ne servent qu'à guarantir la validité de l'inférence pour les paramètres de la moyenne, $\boldsymbol{\beta}$.

### Covariance non structurée

Le modèle le plus général pour la dépendance intra-individu est une matrice **non structurée** de taille $n_{\max} \times n_{\max}$ où $n_{\max} = \max\{n_1, \ldots, n_m\}$, avec
\begin{align*}
\boldsymbol{\Sigma}_i = \begin{pmatrix}
\sigma_{11} & \sigma_{12} &\cdots \sigma_{1n_{\max}} \\
\sigma_{21} & \ddots & \ddots & \vdots \\
\vdots &  \ddots & \ddots & \vdots \\
\sigma_{n_{\max}1} & \cdots & \cdots & \sigma_{n_{\max}n_{\max}}
\end{pmatrix}.
\end{align*}

Le modèle non structuré présenté prend en compte à la fois la corrélation entre observations d'une même unité et l'hétéroscédasticité, en postulant en revanche que cette dernière est la même pour toutes les unités pour une répétition donnée. On pourrait aisément contraindre le modèle de manière à spécifier une variance égale pour toutes les observations en fixant $\sigma_{11} = \cdots = \sigma_{n_{\max}n_{\max}}$.

 Ce modèle ne sera logique que si les mesures sur chaque unités sont comparables: par exemple, si le temps de réponse à chaque vague d'un sondage est différent, il serait illogique d'associer la réponse d'individus pour des temps différents. Par contre, si on modélise la circonférence d'arbres et que l'on obtient des mesures pour ces derniers à des hauteurs régulières (un mètre, deux mètres et cinq mètres du sol), alors on pourra associer chaque mesure à un paramètre.

Ainsi, il faut savoir si les données sont mesurées à des intervalles réguliers ou sont comparables. Le nombre de paramètres à estimer, $n_{\max} (n_{\max} − 1)/2$, restreint son utilisation aux cas
où le nombre maximum d’observations par unité est petit et le nombre d'unités $m$ est grand. On va considérer deux autres modèles plus simples.


### Modèles de covariance autorégressif

La dépendance entre observations consécutives est plus forte qu'entre deux observations plus distantes. Un modèle simple issu de la litérature des séries chronologiques postule que la corrélation entre deux observations ne dépend pas du temps de la mesure, mais uniquement de la distance $h>0$ entre deux observations. Le modèle de corrélation autorégressive d'ordre 1, pour deux mesures à temps $t$ et $t+h$, est
\begin{align*}
\mathsf{Cov}(Y_{t}, Y_{t+h}) = \sigma^2\rho^h, \qquad  |\rho| < 1.
\end{align*}
Ainsi, on postule que la corrélation entre deux observations à distance $h$ décroît comme une série géométrique: plus la distance est grande entre les observations, plus la corrélation est faible.

Ce modèle n'inclut qu'un seul paramètre de corrélation supplémentaire, $\rho$, en plus de la variance commune des mesures $\sigma^2$. Le modèle  autorégressif d'ordre 1, dénoté $\mathsf{AR}(1)$, est ainsi parcimonieux comparativement au modèle de covariance non structurée. On peut aussi garder la même structure de corrélation en ajoutant une variance différente pour chaque temps donné: le modèle résultant est alors dit hétérogène.

Si les pas de temps sont réguliers et chaque observation est mesurée au temps $t=0,1,2,\ldots$, on peut écrire la matrice de covariance intra-groupe de l'unité $i$, $\boldsymbol{Y}_i$, sous la forme
\begin{align*}
\boldsymbol{\Sigma}_i = \sigma^2   \begin{pmatrix}
   1 & \rho & \rho^2 & \cdots & \rho^{n_i}\\
    \rho & 1 & \rho & \cdots & \rho^{n_i-1}\\
    \rho^2 & \rho & 1 & \cdots & \rho^{n_i-2}\\
  \vdots & \ddots & \ddots & \ddots &  \vdots\\
 \rho^{n_i} &\rho^{n_i-1} & \cdots & \rho & 1
  \end{pmatrix}.
\end{align*}

Sans s'attarder pour l'instant à l'estimation de la structure de covariance, on présente la matrice de corrélation estimée les données *Beating the Blues* avec comme modèle pour la moyenne les variables explicatives `mois` (traitée comme une variable catégorielle), un indicateur binaire pour `medicaments` qui indique si la personne a une prescription et finalement un indicateur pour le traitement.

```{r covmodelsBtB, echo = FALSE, eval = TRUE, cache = TRUE, fig.cap = "Estimés de la matrice de corrélation pour l'individu 6 avec les données *Beating the Blues*, avec modèle de covariance non structurée (gauche) et modèle autorégressif hétérogène d'ordre 1 (droite)."}
mod1 <- nlme::gls(
  idb ~ factor(mois) + medicaments + traitement,
  data = btb_long,
  correlation = nlme::corSymm(form = ~ 1 | id),
  weights = varIdent(form = ~ 1 | mois))
mod2 <- nlme::gls(
  idb ~ factor(mois) + medicaments + traitement,
  data = btb_long,
  correlation = nlme::corAR1(form = ~ 1 | id),
  weights = nlme::varIdent(form = ~ 1 | mois))
cov1 <- getVarCov(mod1, individual = 6)
cov2 <- getVarCov(mod2, individual = 6)
class(cov1) <- class(cov2) <- "matrix"
cor1_df <- tibble::tibble(reshape2::melt(cov2cor(cov1))) %>%
  mutate(Var1 = forcats::fct_reorder(factor(Var1), .x = Var1, .desc = TRUE),
         Var2 = factor(Var2))
cor2_df <- tibble::tibble(reshape2::melt(cov2cor(cov2))) %>%
  mutate(Var1 = forcats::fct_reorder(factor(Var1), .x = Var1, .desc = TRUE),
         Var2 = factor(Var2))
sd1 <- sqrt(diag(cov1))
sd2 <- sqrt(diag(cov2))
g1 <- ggplot(data = cor1_df,
             aes(x = Var2, y = Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "white",
                       high = "#440154FF",
                       limit = c(0,1),
                       name = "corrélation") +
  theme_minimal()+
  coord_fixed() +
  scale_x_discrete(labels = c(2L,4L,6L,8L,10L), position = "top") +
  scale_y_discrete(labels = c(10L,8L,6L,4L,2L)) +
  labs(x = element_blank(), y = "mois", subtitle = "non structuré")
g2 <-  ggplot(data = cor2_df,
              aes(x = Var2, y = Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "white",
                       high = "#440154FF",
                       limit = c(0,1),
                       name="corrélation") +
  theme_minimal()+
  coord_fixed() +
  scale_x_discrete(labels = c(2L,4L,6L,8L,10L), position = "top") +
  scale_y_discrete(labels = c(10L,8L,6L,4L,2L)) +
  labs(x = element_blank(), y = "mois", subtitle = "AR(1)")

g1 + g2  + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```
Les estimés de la corrélation pour **Beating the blues** sont présentés dans la Figure \@ref(fig:covmodelsBtB): on voit que
les estimés de la corrélation  sont très élevés et presque constant pour toutes les observations entre 4 et 10 mois, mais la corrélation est plus faible entre la mesure à 2 mois et les autres estimés. Par construction, la matrice de corrélation autorégressive d'ordre 1 assume que la corrélation est décroissante; or, au vu du panneau de gauche, ce modèle ne semble pas adéquat. Cela n'est pas surprenant dans la mesure où les courbes sont systématiquement au dessus ou en dessous de la courbe moyenne; nous verrons dans le prochain chapitre comment prendre en compte ce décalage individuel de chaque courbe.
Pour le modèle non structuré, les estimés de l'écart type ne sont pas très variables: l'estimé va de 12 au temps $t=6$ mois jusqu'à 9 au temps $t=10$ mois, mais presque constants pour les premiers 8 mois (4 premières mesures).

Quel est l'impact de la modélisation de la covariance sur l'inférence? Reportons-nous à nos données *Beating the Blues*. Ici, on s'intéresse à l'efficacité du traitement par rapport au groupe contrôle (traitement usuel): plus le score pour l'inventaire de dépression de Beck (`idb`) est faible, plus le traitement est efficace. Comme l'évolution chronologique du traitement est très différente, on spécifie un terme différent pour chaque tranche de deux mois en incluant `mois` comme variables catégorielle au modèle pour la moyenne. Outre `mois`, on inclut la variable contrôle `medicaments`, une variable binaire qui indique si la personne prend des médicaments.

Règle générale, les estimés des coefficients ne varient que très peu lorsqu'on met toutes les informations des 100 individus en commun. En revanche, comme l'échantillon est débalancé et que plusieurs personnes arrêtent avant 10 mois, la prise en compte de la corrélation intra-individu a un impact important sur les estimés. Le Tableau \@ref(tab:coefbeattheblues) montre les coefficients estimés avec leurs erreurs-types et les intervalles de confiance asymptotique symmétrique: si on ignore

```{r coefbeattheblues, echo = FALSE, cache = TRUE}
library(modelsummary)
data(beattheblues, package = "hecmodstat")
btb_long <- tidyr::pivot_longer(data = beattheblues,
                                cols = tidyselect::starts_with("idb"),
                                names_to = "mois",
                                names_prefix = "idb",
                                values_to = "idb",
                                values_drop_na = TRUE)
levels(btb_long$traitement) <- c("Beating the Blues","usuel")
mod1a <-  nlme::gls(
  idb ~ factor(mois)*traitement + medicaments,
  data = btb_long)
mod1b <- nlme::gls(
  idb ~ factor(mois)*traitement + medicaments,
  data = btb_long,
  correlation = nlme::corSymm(form = ~ 1 | id),
  weights = nlme::varIdent(form = ~ 1 | mois))
#sandw <- vcovHC(mod1b, type = "HC1")
# lmtest::coeftest(mod1a)
# lmtest::coeftest(mod1b)
library(modelsummary)
gf <- gof_map[21, ,drop=FALSE]
gf$fmt <- 1
modelsummary::modelsummary(
  title = "Coefficients (erreurs-types) et intervalles de confiance à 95% pour le modèle de régression linéaire classique (Modèle 1) et modèles avec corrélation intra-individu avec covariance non structurée (Modèle 2).",
      models = list("Modèle 1" = mod1a, "Modèle 2" = mod1b),
       estimate = "{estimate} ({std.error})",
        statistic ="[{conf.low}, {conf.high}]",
    # coef_omit = "(Intercept)|factor(mois)*",
        gof_omit = "RMSE|Num.Obs.",
        coef_rename = c("(Intercept)"="ordonnée à l'origine",
                        "factor(mois)2" = "2 mois (B)",
                        "factor(mois)4" = "4 mois (B)",
                        "factor(mois)6" = "6 mois (B)",
                        "factor(mois)8" = "8 mois (B)",

                       "medicamentsoui"="médicaments (oui)",
                       "traitementusuel"="traitement (usuel)",
                       "factor(mois)4:traitementusuel" = "4 mois (U)",
                       "factor(mois)2:traitementusuel" = "2 mois (U)",
                       "factor(mois)6:traitementusuel" = "6 mois (U)",
                       "factor(mois)8:traitementusuel" = "8 mois (U)")
                     , output = "kableExtra")
```



Ainsi, la prise en compte de la corrélation entraîne une augmentation des coefficient pour *Beating the blues* (B), possiblement parce que plus de personnes traitées avec *Beating the blues* ont quitté l'étude prématurément et on ignore la structure individuelle. L'information fournie par un individu est partiellement redondante: on a 100 individus, mais 380 inventaires à cause des mesures répétées.


Si notre but est de comparer l'évolution de la maladie, alors cette approche ferait une meilleur utilisation des données que la différence entre la réponse au temps $t=0$ mois et celle au temps $t=k$ mois pour $k \in \{2, \ldots, 8\}$ mois parce qu'on estimerait la trajectoire avec chacune des personnes même lorsque certaines courbes sont incomplètes. Ici, le modèle est paramétrisé en termes de contrastes: concentrons-nous sur l'effet du traitement pour une personne qui ne prend pas de médicaments (la réponse sera la même pour ces individus). La moyenne modélisée comprends une interaction entre l'indicateur de `traitement` et le mois, ce qui revient à spécifier une moyenne pour chacune de ces instances une fois l'effet de médicaments pris en compte. Pour une personne assignée à *Beating the blues*, la différence d'inventaire après 2 mois est $\beta_1$, tandis que celle pour une personne assignée au traitement usuel est $\beta_1 + \beta_6$. L'hypothèse nulle que la différence de différences de traitements entre usuel et *Beating the blues* est nulle revient donc $\mathscr{H}_0: \beta_6=0$ (ou `2 mois (U)` dans le tableau) et notre modèle est paramétrisé de telle sorte que cette information est directement disponible dans la sortie. Comparativement au modèle linéaire classique qui regroupe toutes les mesures de la variable réponse sans égard pour la structure de groupe, nos intervalles de confiance pour l'effet de traitement pour *Beating the blues* sont fortement décalés. Dans ce cas de figure, l'incertitude des estimés qui prennent en compte la corrélation est moindre. Cet exemple illustre que pour les cas de figure où les données sont débalancées (nombre de réponse différent par individu), la conclusion et les estimés sont fortement affectés. Plus généralement, on verra un changement important pour les erreurs-types des coefficients, mais dans une moindre mesure pour les estimés ponctuels de ces derniers.



### Données groupées et modèle d'équicorrélation


Dans certains contextes, il n'y aura pas de structure logique pour les données: dans l'exemple précédent, le modèle AR(1) se base sur le nombre de mois séparant les questionnaires, mais qu'en serait-il si on interrogait les employés de différents départements au sein de l'école sur leur expérience durant la pandémie? Selon la pression de leur responsables d'unités, ils ou elles ont pu avoir une expérience très différente. Quelquefois, la dépendance proviendra non pas de mesures répétées, mais plutôt de regroupements logiques.

Un exemple nous est fourni par le sondage [*Workplace Employee Relations Survey*](https://www.gov.uk/government/collections/workplace-employment-relations-study-wers), une étude du gouvernement britannique qui a été conduite en 1980, 1984, 1990, 1998, 2004 et 2011. L'étude est multi-niveaux: des entrevues détaillées sont conduites le ou la gestionnaire senior en charge des relations de travail, une personne syndiquée et non-syndiquée oeuvrant au sein de l'entreprise ainsi qu'un questionnaire administré à au plus 25 personnes dans chaque lieu de travail. Les milieux de travail sont sélectionnés par le biais d'un échantillon stratifié et certaines sont suivies d'une vague à l'autre; [voir la méthodologie pour plus de détails à ce sujet](https://www.wers2011.info/methodology).

Ainsi, il serait logique de croire que les réponses des membres d'une même entreprise, qui partagent des politiques communes (convention collective, milieu de travail), etc. sont corrélées. Contrairement aux données longitudinales, le regroupement se fera par milieu de travail et il n'y a pas de relation logique entre les individus qui sont échangeables car l'ordre dans lequel ils apparaissent dans la base de données est arbitraire.

Dans ce contexte, une matrice de covariance non structurée ou une structure autorégressive est illogique, puisque chaque individu est différent d'une entreprise à l'autre. Un modèle simple qui peut être adéquat est le modèle d'équicorrélation, dans lequel la corrélation entre deux réponses au sein d'une unité a une corrélation constante de $\rho$.

Le modèle d'équicorrélation est d'ordinaire paramétrisé sous la forme
\begin{align*}
\boldsymbol{\Sigma}_i =
\begin{pmatrix}
  \sigma^2+\tau & \tau &\cdots  & \tau\\
  \tau & \sigma^2+\tau &\ddots& \vdots\\
  \vdots & \ddots & \ddots & \vdots \\
    \tau & \tau & \cdots &\sigma^2+\tau
\end{pmatrix}.
\end{align*}
et la corrélation intra-groupe pour deux observations est $\rho=\tau/(\sigma^2+\tau)$ avec $|\rho| < 1$; ce modèle est valide si et seulement si $1+(n_{\max}-1)\rho > 0$ en dimension $n_{\max}$ et cette inégalité se traduit par la contrainte $\tau > -\sigma^2/n_{\max}$. On recouvre le modèle avec données non corrélées et homoscédastiques si $\tau=0$: ce constat servira lorsque nous considérerons les tests d'hypothèse.



:::keyidea

On peut modéliser la covariance des unités intra-groupes dans le cas de données longitudinales ou groupées si le postulat d'indépendance ne tient pas la route.

- En prenant en compte la corrélation intra-unité, on obtient typiquement des estimés des erreurs-types plus élevés pour les coefficients.

- On suppose que les observations au sein d'une même unité sont corrélées, mais que les observations d'unités différentes sont indépendantes.

On a considéré trois principaux modèles de covariance:

- Le modèle de corrélation $\mathsf{AR}(1)$ suppose que la corrélation décroît exponentiellement dans le temps et ne dépend que de la distance (temporelle) entre deux observations au sein d'une même unité.
- Le modèle d'équicorrélation suppose que les données sont échangeable et que leur corrélation $\rho$ est identique pour toutes les unités de groupe. Cette structure est plus logique pour les données groupées.
- Le modèle non structuré permet modéliser de manière générale la matrice de corrélation intra-unité. Le modèle a un nombre élévé de paramètres et n'est logique ou utile que si nous avons suffisamment de réplications à chaque temps donné pour estimer les paramètres.
- Des versions hétérogènes de ces trois modèles existent et impliquent des variances différents à chaque temps $t$, mais égales pour tous les individus.

:::

## Comparaisons de modèles


## Hétéroscédasticité de groupe
