# Compléments mathématiques


### Population et échantillons

Ce qui différencie la statistique des autres sciences est la prise en compte de l'incertitude et de la notion d'aléatoire. Règle générale, on cherche à estimer une caractéristique d'une population définie à l'aide d'un échantillon (un sous-groupe de la population) de taille restreinte. 

La **population d'intérêt** est une collection d'individus formant la matière première d'une étude statistique. Par exemple, pour l'Enquête sur la population active (EPA) de Statistique Canada, « la population cible comprend la population canadienne civile non institutionnalisée de 15 ans et plus ». Même si on faisait un recensement et qu'on interrogeait tous les membres de la population cible, la caractéristique d'intérêt peut varier selon le moment de la collecte; une personne peut trouver un emploi, quitter le marché du travail ou encore se retrouver au chômage. Cela explique la variabilité intrinsèque.

En général, on se base sur un **échantillon**  pour obtenir de l'information. L'**inférence statistique** vise à tirer des conclusions, pour toute la population, en utilisant seulement l'information contenue dans l'échantillon et en tenant compte des sources de variabilité. Le sondeur George Gallup (traduction libre) a fait cette merveilleuse analogie entre échantillon et population: 

> «Il n'est pas nécessaire de manger un bol complet de soupe pour savoir si elle est trop salé; pour autant qu'elle ait été bien brassée, une cuillère suffit.»

Un **échantillon** est un sous-groupe d'individus tirés de la population à partir duquel on fait une analyse statistiques. La création de plans d'enquête est un sujet complexe et des cours entiers d'échantillonnage y sont consacrés. Même si on ne collectera pas de données, il convient de noter la condition essentielle pour pouvoir tirer des conclusions fiables à partir d'un échantillon: ce dernier doit être représentatif de la population étudiée, en ce sens que sa composition doit être similaire à celle de la population. On doit ainsi éviter les biais de sélection, notamment les échantillons de commodité qui consistent en une sélection d'amis et de connaissances. 

Notre échantillon est **aléatoire**, donc notre mesure d'une caractéristique d'intérêt le sera également et la conclusion de notre procédure de test variera d'un échantillon à l'autre. Plus la taille de notre échantillon sera grande, plus on obtiendra une mesure précise, voire exacte si on fait un recensement, de la quantité d'intérêt. L'exemple suivant illustre pourquoi le choix de l'échantillon est important.

```{example, label="Galluppoll", title="L'élection présidentielle américaine de 1936"}

Désireuse de prédire le résultat de l'élection présidentielle américaine de 1936, la revue *Literary Digest* a sondé 10 millions d'électeurs par la poste, dont 2.4 millions ont répondu au sondage en donnant une nette avance au candidat républicain Alf Landon (57\%) face au président sortant Franklin D. Roosevelt (43\%). Ce dernier a néanmoins remporté l'élection avec 62\% des suffrages, une erreur de prédiction de 19\%.  Le plan d'échantillonnage avait été conçu en utilisant des bottins téléphoniques, des enregistrements d'automobiles et des listes de membres de clubs privés, etc.:  \href{https://www.jstor.org/stable/2749114}{la non-réponse différentielle et un échantillon biaisé vers les classes supérieures sont en grande partie responsable de cette erreur.}

Gallup avait de son côté correctement prédit la victoire de Roosevelt en utilisant un échantillon aléatoire de (seulement) 50 000 électeurs. \href{https://medium.com/@ozanozbey/how-not-to-sample-11579793dac}{L'histoire complète (en anglais).}

```

## Variables aléatoires

Un phénomène d'intérêt varie d'un individu à l'autre (autrement un modèle statistique est rarement adéquat). On cherche à décrire son comportement, soit l'ensemble des valeurs possibles et leur probabilité/fréquence relative au sein de la population décrites par la loi de probabilité de la variable aléatoire. 

On fera la distinction entre deux cas de figure: quand le phénomène prend des valeurs finies, comme par exemple un événement binaire (achat/non-achat d'un produit) ou un continuum de valeurs (par exemple, le prix d'un item). On dénote les variables aléatoires par des lettres majuscules: par exemple, $Y \sim \mathsf{No}(\mu, \sigma^2)$ indique que $Y$ suit une loi normale de paramètres $\mu$ et $\sigma$, qui représentent respectivement l'espérance et la variance de $Y$. 

La fonction de répartition $F(y)$ donne la probabilité cumulative qu'un événement n'excède pas une variable donnée, $F(y) = \mathsf{Pr}(Y \leq y)$. 

Si la variable $Y$ prend des valeurs discrètes, alors on utilise la fonction de masse $f(y)=\mathsf{Pr}(Y=y)$ qui donne la probabilité pour chacune des valeurs de $y$. 
Si la variable $Y$ est continue, aucune valeur numérique de $y$ n'a de probabilité non-nulle; la densité sert à estimer la probabilité que la variable $Y$ appartienne à un ensemble $B$, via $\mathsf{Pr}(Y \in B) = \int_B f(y) \mathrm{d} y$; la fonction de répartition est ainsi $F(y) = \int_{-\infty}^y f(x) \mathrm{d} x$. Plusieurs lois aléatoires décrivent des phénomènes physiques simples et ont donc une justification empirique; on revisite les distributions les plus fréquemment couvertes.

 
```{example, label="loibern", name="Loi de Bernoulli"}
On considère un phénomène binaire, comme le lancer d'une pièce de monnaie (pile/face). De manière générale, on associe les deux possibilités à succès/échec et on suppose que la probabilité de succès est $\pi$. Par convention, on représente les échecs (non) par des zéros et les réussites (oui) par des uns. Donc, si la variable $Y$ vaut $0$ ou $1$, alors  $\mathsf{Pr}(Y=1)=\pi$ et $\mathsf{Pr}(Y=0)=1-\pi$ (complémentaire). La fonction de masse de la [loi Bernoulli](https://fr.wikipedia.org/wiki/Loi_de_Bernoulli) s'écrit de façon plus compacte
\begin{align*}
\mathsf{P}(Y=y) = \pi^y (1-\pi)^{1-y}, \quad y=0, 1.
\end{align*}

Un calcul rapide montre que $\mathsf{E}(Y)=\pi$ et $\mathsf{Va}(Y)=\pi(1-\pi)$.
Voici quelques exemples de questions de recherches comprenant une variable réponse binaire:

- est-ce qu'un client potentiel a répondu favorablement à une offre
promotionnelle?
- est-ce qu'un client est satisfait du service après-vente?
- est-ce qu'une firme va faire faillite au cours des trois prochaines années?
- est-ce qu'un participant à une étude réussit une tâche?
  
```

```{example , label="loibinom", name = "Loi binomiale"}
Si les données représentent la somme d'événements Bernoulli indépendants, la loi du nombre de réussites $Y$ pour un nombre d'essais donné $m$ est dite [binomiale](https://fr.wikipedia.org/wiki/Loi_binomiale), dénotée $\mathsf{Bin}(m, \pi)$; sa fonction de masse est
\begin{align*}
\mathsf{Pr}(Y=y) = \binom{m}{y}\pi^y (1-\pi)^{1-y}, \quad y=0, 1.
\end{align*}
La vraisemblance pour un échantillon de la loi binomiale est (à constante de normalisation près qui ne dépend pas de $\pi$) la même que pour un échantillon aléatoire de $m$ variables Bernoulli indépendantes. L'espérance d'une variable binomiale est $\mathsf{E}(Y)=m\pi$ et la variance $\mathsf{Va}(Y)=m\pi(1-\pi)$. 

On peut ainsi considérer le nombre de personnes qui ont obtenu leur permis de conduire parmi $m$ candidat(e)s ou le nombre de clients sur $m$ qui ont passé une commande de plus de 10\$ dans un magasin.

```


Plus généralement, on peut considérer des variables de dénombrement qui prennent des valeurs entières. Parmi les exemples de questions de recherches comprenant une variable réponse de dénombrement: 

- le nombre de réclamations faites par un client d'une compagnie d'assurance
au cours d'une année.
- le nombre d'achats effectués par un client depuis un mois.
- le nombre de tâches réussies par un participant lors d'une étude.

```{example, label = "loigeom", name = "Loi géométrique"}
La [loi géométrique](https://fr.wikipedia.org/wiki/Loi_g%C3%A9om%C3%A9trique) décrit le comportement du nombre d'essais Bernoulli de probabilité de succès $\pi$ nécessaires avant l'obtention d'un premier succès. La fonction de masse de $Y \sim \mathsf{Geo}(\pi)$ est 
\begin{align*}
\mathsf{Pr}(Y=y) = \pi (1-\pi)^{y-1}, \quad y=1,2, \ldots
\end{align*}

Par exemple, on pourrait modéliser le nombre de visites d'une maison en vente avant une première offre d'achat à l'aide d'une variable géométrique.

```


```{example label = "loipoisson", name = "Loi de Poisson"}
Si la probabilité d'un événement Bernoulli est **rare** dans le sens où $n\pi \to \lambda$ quand le nombre d'essais $n$ augmente, alors le nombre de succès suit une loi de Poisson de fonction de masse
\begin{align*}
\mathsf{Pr}(Y=y) = \frac{\exp(-\lambda)\lambda^y}{\Gamma(y+1)}, \quad y=0, 1, 2, \ldots
\end{align*}
où $\Gamma(\cdot)$ dénote la fonction gamma. Le paramètre $\lambda$ de la loi de Poisson représente à la fois l'espérance et la variance de la variable, c'est-à-dire que $\mathsf{E}(Y)=\mathsf{Va}(Y)=\lambda$.

```


```{example, label = "loibinneg", name = "Loi binomiale négative"}
On considère une série d'essais Bernoulli de probabilité de succès $\pi$ jusqu'à l'obtention de $m$ succès. Soit $Y$, le nombre d'échecs: puisque la dernière réalisation doit forcément être un succès, mais que l'ordre des succès/échecs précédents n'importe pas, la fonction de masse est 
\begin{align*}
\mathsf{Pr}(Y=y)= \binom{m-1+y}{y} \pi^m (1-\pi)^{y}.
\end{align*}

La loi binomiale négative apparaît également si on considère la loi non-conditionnelle du modèle hiérarchique gamma-Poisson, dans lequel on suppose que le paramètre de la moyenne de la loi Poisson est aussi aléatoire, c'est-à-dire $Y \mid \Lambda=\lambda \sim \mathsf{Po}(\lambda)$ et $\Lambda$ suit une loi Gamma de paramètre de forme $r$ et de paramètre d'échelle $\theta$, dont la densité est \[f(x) = \theta^{-r}x^{r-1}\exp(-x/\theta)/\Gamma(r).\] Le nombre d'événements suit alors une loi binomiale négative.

La paramétrisation la plus courante pour la modélisation est légèrement différente: on utilise la fonction de masse est
\begin{align*}
\mathsf{P}(Y=y)=\frac{\Gamma(y+r)}{\Gamma(y+1)\Gamma(r)} \left(\frac{r}{r + \mu} \right)^{r} \left(\frac{\mu}{r+\mu}\right)^y, y=0, 1, \ldots, \mu,r >0,
\end{align*}
où $\Gamma$ dénote la fonction gamma. À noter que le paramètre $r>0$ n'est plus nécessairement entier. La moyenne théorique et la variance sont 
$\mathsf{E}(Y)=\mu$ et $\mathsf{Va}(Y)=\mu+k\mu^2$, où $k=1/r$. La variance d'une variable binomiale négative est *supérieure* à sa moyenne et le modèle est utilisé comme alternative à la loi de Poisson pour modéliser la surdispersion. 
```

## Loi des grands nombres


Un estimateur est dit **convergent** si la valeur obtenue à mesure que la taille de l'échantillon augmente s'approche de la vraie valeur que l'on cherche à estimer. La loi des grands nombres établit que la moyenne empirique de $n$ observations indépendantes de même espérance,  $\overline{Y}_n$, tend vers l'espérance commune des variables $\mu$, où $\overline{Y}_n \rightarrow \mu$. En gros, ce résultat nous dit que l'on réussit à approximer de mieux en mieux la quantité d'intérêt quand la taille de l'échantillon (et donc la quantité d'information disponible sur le paramètre) augmente. La loi des grands nombres est très utiles dans les expériences Monte Carlo: on peut ainsi approximer par simulation la moyenne d'une fonction de variables aléatoires $g(Y)$ en simulant de façon répétée des variables $Y$ indépendantes et identiquement distribuées et en prenant la moyenne empirique $n^{-1} \sum_{i=1}^n g(Y_i)$.

Si la loi des grands nombres nous renseigne sur le comportement limite ponctuel, il ne nous donne aucune information sur la variabilité de notre estimé de la moyenne et la vitesse à laquelle on s'approche de la vraie valeur du paramètre.



## Théorème central limite


Le théorème central limite dit que, pour un échantillon aléatoire de taille $n$ dont les observations sont indépendantes et tirées d'une loi quelconque d'espérance $\mu$ et de variance finie $\sigma^2$, alors la moyenne empirique tend non seulement vers $\mu$, mais à une vitesse précise:

- l'estimateur $\overline{Y}$ sera centré autour de $\mu$,
- l'erreur-type sera de $\sigma/\sqrt{n}$; le taux de convergence est donc de $\sqrt{n}$. Ainsi, pour un échantillon de taille 100, l'erreur-type de la moyenne empirique sera 10 fois moindre que l'écart-type de la variable aléatoire sous-jacente.
- la loi approximative de la moyenne $\overline{Y}$ sera normale.

Mathématiquement, le théorème central limite dicte que $\sqrt{n}(\overline{Y}-\mu) \stackrel{\mathrm{d}}{\rightarrow} \mathsf{No}(0, \sigma^2)$. Si $n$ est grand (typiquement supérieur à $30$, mais cette règle dépend de la loi sous-jacente de $Y$), alors $\overline{Y} \stackrel{\cdot}{\sim} \mathsf{No}(\mu, \sigma^2/n)$.

Comment interpréter ce résultat? On considère comme exemple le temps de trajet moyen de trains à haute vitesse AVE entre Madrid et Barcelone opérés par la Renfe.

```{r renfeclt, echo = FALSE, fig.cap="Distribution empirique des temps de trajet en trains à grande vitesse.", fig.width = 6, fit.height = 5}
url <- "https://lbelzile.bitbucket.io/MATH60604/renfe.sas7bdat"
renfe <- haven::read_sas(url)[,c("type","duree")]
duree <- renfe$duree[renfe$type %in% c("AVE","AVE-TGV")]
ggplot(data = data.frame(duree), aes(x=duree)) + 
  geom_bar() + 
  xlab("durée (en minutes)") +
  ylab("fréquence")
```

Une analyse exploratoire indique que la durée du trajet de la base de données est celle affichée sur le billet (et non le temps réel du parcours). Ainsi, il n'y a ainsi que `r length(sort(unique(duree)))` valeurs possibles. Le temps affiché moyen pour le parcours, estimé sur la base de `r length(duree)` observations, est de `r as.integer(floor(mean(duree)))` minutes et `r as.integer(round(mean(duree)%% 1 * 60,0))` secondes. La Figure (voir \@ref(fig:renfeclt)) montre la distribution empirique des données.


Considérons maintenant des échantillons de taille dix. Dans notre premier échantillon aléatoire, la durée moyenne affichée est `r mean(sample(duree, 10L))` minutes, elle est de `r mean(sample(duree, 10L))` minutes dans le deuxième, de `r mean(sample(duree, 10L))` dans le troisième, et ainsi de suite.


```{r renfemeanCLT, echo = FALSE, fig.cap="Représentation graphique du théorème central limite: échantillon aléatoire de 20 observations avec leur moyenne empirique (trait vertical rouge) (en haut à gauche). Les trois autres panneaux montrent les histogrammes des moyennes empiriques d'échantillons répétés de taille 5 (en haut à droite), 20 (en bas à gauche) et les histogrammes pour $n=5, 20, 100$ (en bas à droite) avec courbe de densité de l'approximation normale fournie par le théorème central limite.", fig.width = 8, fit.height = 10, out.width = '90%'}
set.seed(1234)
moy5 <- data.frame(moy = replicate(n = 10000, expr = mean(sample(duree, size = 5, replace = FALSE))))
moy20 <- data.frame(moy = replicate(n = 10000, expr = mean(sample(duree, size = 20, replace = FALSE))))
moy100 <- data.frame(moy = replicate(n = 20000, expr = mean(sample(duree, size = 100, replace = FALSE))))

p0 <- ggplot(data = (df20 <- data.frame(duree = sample(duree, 20))), aes(x=duree)) + 
  geom_bar() + 
  geom_vline(xintercept = mean(df20$duree), col = "red") + 
  xlab("durée (en minutes)") +
  ylab("fréquence")

p1 <- ggplot(data = moy5) + 
  geom_histogram(bins = 30, aes(x=moy, y=..density..), alpha = 0.2) + 
  stat_function(fun = dnorm, col = "blue", args = list(mean = mean(duree), sd = sqrt(var(duree))/sqrt(5)), n = 1000) + 
  xlab("durée moyenne (en minutes)") +
  ylab("densité")

p2 <- ggplot(data = moy20) + 
  geom_histogram(data = moy20, bins = 30, aes(x=moy, y=..density..), alpha = 0.2) + 
  stat_function(fun = dnorm, col = "blue", args = list(mean = mean(duree), sd = sqrt(var(duree))/sqrt(20)), n = 1000) + 
  xlab("durée moyenne (en minutes)") +
  ylab("densité")

p3 <- ggplot(data = moy100) + 
  geom_histogram(bins = 50, aes(x=moy, y=..density..), alpha = 0.2) + 
  stat_function(fun = dnorm, col = "blue", args = list(mean = mean(duree), sd = sqrt(var(duree))/10), n = 1000)  + 
  geom_histogram(data = moy20, bins = 30, aes(x=moy, y=..density..), alpha = 0.2) + 
  stat_function(fun = dnorm, col = "blue", args = list(mean = mean(duree), sd = sqrt(var(duree))/sqrt(20)), n = 1000) + 
   geom_histogram(bins = 30, aes(x=moy, y=..density..), alpha = 0.2) + 
  stat_function(fun = dnorm, col = "blue", args = list(mean = mean(duree), sd = sqrt(var(duree))/sqrt(5)), n = 1000) + 
  xlim(c(142,198)) + 
  xlab("durée moyenne (en minutes)") +
  ylab("densité")


cowplot::plot_grid(p0, p1, p2, p3)
```

Supposons qu'on tire $B=1000$ échantillons différents, chacun de taille $n=5$, de notre ensemble, et qu'on calcule la moyenne de chacun d'entre eux. Le graphique supérieur droit \@ref(fig:renfemeanCLT) montre un de ces 1000 échantillons aléatoire de taille $n=20$ tiré de notre base de données. Les autres graphiques de la Figure \@ref(fig:renfemeanCLT) illustrent l'effet de l'augmentation de la taille de l'échantillon: si l'approximation normale est approximative avec $n=5$, la distribution des moyennes est virtuellement identique à partir de $n=20$. Plus la moyenne est calculée à partir d'un grand échantillon (c'est-à-dire, plus $n$ augmente), plus la qualité de l'approximation normale est meilleure et plus la courbe se concentre autour de la vraie moyenne; malgré le fait que nos données sont discrètes, la distribution des moyennes est approximativement normale. 

On a considéré un seul échantillon dans l'exemple, mais vous pouvez vous amuser à regarder la taille de l'échantillon nécessaire pour que l'effet du théorème central limite prenne effet en simulant des observations d'une loi quelconque de variance finie. Cette [applette](http://195.134.76.37/applets/AppletCentralLimit/Appl_CentralLimit2.html) permet de faire des simulations en variant la loi des données et la taille de l'échantillon.

Les statistiques de test qui découlent d'une moyenne centrée-réduite (ou d'une quantité équivalente pour laquelle un théorème central limite s'applique) ont souvent une loi nulle standard normale, du moins asymptotiquement (quand $n$ est grand, typiquement $n>30$ est suffisant). C'est ce qui garantie la validité de notre inférence!

