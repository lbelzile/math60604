# Régression linéaire {#regression-lineaire}

```{r}
#| label: setup
#| file: "_common.R"
#| include: true
#| eval: true
#| message: false
#| warning: false
#| echo: false
#| cache: false
```


## Introduction

Le modèle de régression linéaire, ou modèle linéaire, est l'un des outils les plus polyvalents pour l'inférence statistique. La régression linéaire est principalement utilisée pour évaluer les effets des variables explicatives (souvent l'effet d'une manipulation ou d'un traitement dans un cadre expérimental) sur la moyenne d'une variable réponse continue, ou pour la prédiction. 
Un modèle linéaire est un modèle qui décrit la moyenne d'une **variable réponse** continue $Y_i$ d'un échantillon aléatoire de taille $n$ comme **fonction linéaire** des **variables explicatives** (également appelés prédicteurs, régresseurs ou covariables) $X_1, \ldots, X_p$.

Dénotons par $Y_i$ la valeur de $Y$ pour le sujet $i$, et $X_{ij}$ la valeur de la $j$e variable explicative du sujet $i$. 
\begin{align}
\underset{\text{moyenne conditionnelle}}{\mathsf{E}(Y_i \mid \boldsymbol{X}_i=\boldsymbol{x}_i)}=\mu_i=\underset{\substack{\text{combinaison linéaire (somme pondérée)}\\ \text{de variables explicatives}}}{\beta_0 + \beta_1x_{i1} + \cdots + \beta_p x_{ip}}\equiv \mathbf{x}_i\boldsymbol{\beta}.
\end{align}
où $\mathbf{x}_i = (1, x_{i1}, \ldots, x_{ip})$ est un vecteur ligne de taille $(p+1)$ contenant les variables explicatives de l'observation $i$ et $\boldsymbol{\beta} = (\beta_0, \ldots, \beta_p)^\top$ est un vecteur colonne de longueur $p+1$ contenant les coefficients de la moyenne. Le fait que la moyenne est conditionnelle aux valeurs de $\mathbf{X}$  implique simplement que l'on considère les régresseurs comme constant, ou connus à l'avance. Les coefficients $\boldsymbol{\beta}$ sont les mêmes pour toutes les observations, mais le vecteurs de variables explicatives $\mathbf{x}_i$ peut différer d'une observation à l'autre. Le modèle est **linéaire** en $\beta_0, \ldots, \beta_p$, pas nécessairement dans les variables explicatives.



Pour simplifier la notation, nous regroupons les observations dans un vecteur $n$ $\boldsymbol{Y}$ et les explications dans une matrice $n \times (p+1)$ $\mathbf{X}$ en concaténant une colonne de uns et les vecteurs de colonnes $p$ $\boldsymbol{X}_1, \ldots, \boldsymbol{X}_p$, chacun contenant les $n$ observations des explications respectives. La matrice $\mathbf{X}$ est appelée **matrice du modèle** (ou parfois matrice de devis dans un contexte expérimental), et sa $i$ème ligne est $\mathbf{x}_i$.


En supposant que la  variable réponse provient d'une famille de localisation, nous pouvons réécrire le modèle linéaire en termes de la moyenne plus un aléa,
\begin{align*}
\underset{\text{observation}\vphantom{\mu_i}}{Y_i} = \underset{\text{moyenne } \mu_i}{\vphantom{Y_i}\mathbf{x}_i\boldsymbol{\beta}} + \underset{\text{aléa}\vphantom{\mu_i}}{\vphantom{Y_i}\varepsilon_i},
\end{align*}
où $\varepsilon_i$ est le terme spécifique à l'observation $i$. On assume que les aléas $\varepsilon_1, \ldots \varepsilon_n$ sont indépendants et identiquement distribués, avec $\mathsf{E}(\varepsilon_i \mid \mathbf{x}_i) = 0$ et $\mathsf{Var}(\varepsilon_i \mid \mathbf{x}_i) = \sigma^2$. On fixe l'espérance de l'aléa à zéro car on postule qu’il n’y a pas d’erreur systématique. La variance $\sigma^2$ sert à tenir compte du fait qu’aucune relation linéaire exacte ne lie $\mathbf{x}_i$ et $Y_i$, ou que les mesures de $Y_i$ sont variables.


Le modèle linéaire normal ou gaussien spécifie que les réponses suivent une loi normale, avec $Y_i \mid \boldsymbol{X}_i=\boldsymbol{x}_i \sim \mathsf{normale}(\mathbf{x}_i\boldsymbol{\beta}, \sigma^2)$. La loi normale est une famille de localisation, de sorte que $Y \sim \mathsf{normale}(\mu, \sigma^2)$ équivaut à la décomposition additive $\mu + \varepsilon$ pour $\varepsilon \sim \mathsf{normale}(0, \sigma^2)$.

### Exemples

Considérons quelques exemples de jeux de données qui serviront à illustrer les méthodes par la suite. 

:::{#exm-lee-choi1}

##  Cohérence de descriptions de produits

L'étude 1 de @Lee.Choi:2019 (base de données `LC19_S1`, paquet `hecedsm`) considère l'impact sur la perception d'un produit de la divergence entre la description textuelle et l'image. Dans leur première expérience, un paquet de six brosses à dents est vendu, mais l'image montre soit un paquet de six, soit une seule). Les auteurs ont également mesuré la familiarité préalable avec la marque de l'article. Les $n=96$ participants ont été recrutés à l'aide d'un panel en ligne. Nous pourrions ajuster un modèle linéaire pour le score moyen d'évaluation du produit, `prodeval`, en fonction de la familiarité de la marque `familiarity`, un nombre entier allant de 1 à 7, et une variable binaire pour le facteur expérimental `consistency`, codé `0` pour des descriptions d'image/texte cohérentes et `1` si elles sont incohérentes. La matrice du modèle qui en résulte est alors de dimension $96\times 3$. La réponse `prodeval` est fortement discrétisée.


```{r}
#| eval: true
#| echo: true
data(LC19_S1, package = "hecedsm")
modmat <- model.matrix( # Matrice du modèle
     ~ familiarity + consistency,
     data = LC19_S1)
tail(modmat, n = 5L) # Imprimer les premières 5 lignes
dim(modmat) # dimension de la matrice du modèle
```

:::



:::{#exm-teaching-baumann}

## Méthodes d'apprentissage de compréhension de lecture


La base de données `BSJ92` du paquet `hecedsm` contient les résultats d'une expérience de @Baumann:1992 sur l'efficacité de différentes stratégies de lecture sur la compréhension d'enfants.


> Soixante-six élèves de quatrième année ont été assignés au hasard à l'un des trois groupes expérimentaux suivants : (a) un groupe « Think-Aloud » (TA), dans lequel les élèves ont appris diverses stratégies de contrôle de la compréhension pour la lecture d'histoires (par exemple : auto-questionnement, prédiction, relecture) par le biais de la réflexion à haute voix; (b) un groupe lecture dirigée-activité de réflexion (DRTA), dans lequel les élèves ont appris une stratégie de prédiction-vérification pour lire et répondre aux histoires; ou (c) un groupe activité de lecture dirigée (DRA), un groupe contrôle dans lequel les élèves se sont engagés dans une lecture guidée non interactive d'histoires.

Les variables d'intérêt sont `group`, le facteur pour le groupe expérimental, soit `DRTA`, `TA` et `DR` ainsi que les variables numériques `pretest1` et `posttest1`, qui donnent le score (sur 16) sur le test pré-expérience pour la tâche de détection des erreurs.

```{r}
#| eval: true
#| echo: false
data(BSJ92, package = "hecedsm")
# Compute sample correlation between pretest and posttest 1
cor_baum <- with(BSJ92, cor(posttest1, pretest1))
```

Les données sont balancées puisqu'il y a 22 observations dans chacun des trois sous-groupes. Les chercheurs ont appliqué une série de trois évaluations: le test 1 de  détection d'erreurs, le test 2 consistant en un questionnaire de suivi de compréhension, et le test 3 standardisé *Degrees of Reading Power*). Les tests 1 et 2 ont été administrés à la fois avant et après l'intervention: cela nous permet d'établir l'amélioration moyenne de l'élève en ajoutant le résultat du test pré-intervention comme covariable. Les tests 1 étaient sur 16, mais celui administré après l'expérience a été rendu plus difficile pour éviter les cas d'étudiants obtenant des scores presque complets. La corrélation entre le pré-test et le post-test 1 est $(\widehat{\rho}_1=`r round(cor_baum, 2)`)$, beaucoup plus forte que celle du second test $(\widehat{\rho}_2=`r round(cor(BSJ92$posttest2, BSJ92$pretest2), 2)`)$.


:::


:::{#exm-college-salary-discrimination}

# Discrimination salariale dans un collège américain

 On s'intéresse à la discrimination salariale dans un collège américain, au sein duquel une étude a été réalisée pour investiguer s'il existait des inégalités salariales entre hommes et femmes. Le jeu de données `college` contient les variables suivantes:

-   `salaire`: salaire de professeurs pendant l'année académique 2008--2009 (en milliers de dollars USD).
-   `echelon`: échelon académique, soit adjoint (`adjoint`), aggrégé (`aggrege`) ou titulaire (`titulaire`).
-   `domaine`: variable catégorielle indiquant le champ d'expertise du professeur, soit appliqué (`applique`) ou théorique (`theorique`).
-   `sexe`: indicateur binaire pour le sexe, `homme` ou `femme`.
-   `service`: nombre d'années de service.
-   `annees`: nombre d'années depuis l'obtention du doctorat.

:::






:::{#exm-moon-vanepps}

## Suggestion de montants de dons

L'étude 1 de @Moon.VanEpps:2023 (données `MV23_S1`, paquet `hecedsm`) porte sur la proportion de donateurs à un organisme de charité et le montant de leurs dons. Les participants au panel en ligne avaient la possibilité de gagner 25$ et de faire don d'une partie de cette somme à l'organisme de leur choix. Les données fournies incluent uniquement les personnes qui n'ont pas dépassé ce montant et qui ont indiqué avoir fait un don d'un montant non nul.

:::


:::{#exm-sokolova}

## Un emballage en carton supplémentaire est-il considéré comme plus écologique ?

@Sokolova:2023 tient compte des préjugés des consommateurs lorsqu'il s'agit d'évaluer le caractère écologique des emballages. Des produits tels que les céréales sont emballés dans des sacs en plastique, eux-mêmes recouverts d'une boîte. Ils supposent (et constatent) que, paradoxalement, les consommateurs ont tendance à considérer l'emballage comme plus écologique lorsque la quantité de carton ou de carton entourant la boîte est plus importante, ce qui n'est pas le cas. Nous examinons dans la suite les données de l'étude 2A, qui mesure la perception du respect de l'environnement (PEF, variable `pef`) en fonction de la `proportion` d'emballage en carton (soit aucun, soit la moitié de la surface du plastique, soit la même, soit le double).

:::

### Analyse exploratoire des données

L'analyse exploratoire des données est une procédure itérative par laquelle nous interrogeons les données, en utilisant des informations auxiliaires, des statistiques descriptives et des graphiques, afin de mieux informer notre modélisation.

Elle est utile pour mieux comprendre les caractéristiques des données (plan d'échantillonnage, valeurs manquantes, valeurs aberrantes), la nature des observations, qu'il s'agisse de variables réponse ou explicatives et les interrelations entre variables.


Voir le [Chapitre 11 de Alexander (2023)](https://tellingstorieswithdata.com/11-eda.html) pour des exemples. En particulier, il convient de vérifier

- que les variables catégorielles sont adéquatement traitées comme des facteurs (`factor`).
- que les valeurs manquantes sont adéquatement déclarées comme telles (code d’erreur, 999, etc.)
- s’il ne vaudrait mieux pas retirer certaines variables explicatives avec beaucoup de valeurs manquantes.
- s’il ne vaudrait mieux pas fusionner des modalités de variables catégorielles si le nombre d’observation par modalité est trop faible.
- qu’il n’y a pas de variable explicative dérivée de la variable réponse
- que le sous-ensemble des observations employé pour l’analyse statistique est adéquat.
- qu’il n’y a pas d’anomalies ou de valeurs aberrantes (par ex., 999 pour valeurs manquantes) qui viendraient fausser les résultats.




:::{#exm-college-aed}

## Analyse exploratoire des données `college`

Une analyse exploratoire des données est de mise avant d'ébaucher un modèle. Si le salaire augmente au fil des ans, on voit que l'hétérogénéité change en fonction de l'échelon et qu'il y a une relation claire entre ce dernier et le nombre d'années de service (les professeurs n'étant éligibles à des promotions qu'après un certain nombre d'années). Les professeurs adjoints qui ne sont pas promus sont généralement mis à la porte, aussi il y a moins d'occasions pour que les salaires varient sur cette échelle.

```{r}
#| label: fig-edacollege
#| eval: true
#| echo: false
#| fig-cap: "Analyse exploratoire des données `college`: répartition des salaires en fonction de l'échelon et du nombre d'années de service"
data(college, package = "hecmodstat")
p1 <- ggplot(college, aes(y = salaire, x = echelon)) +
  geom_boxplot() +
  xlab("échelon académique") +
  ylab("salaire (en milliers de dollars USD)")
p2 <- ggplot(college, aes(x = service, y = salaire, col = sexe)) +
  geom_point() +
  facet_wrap(~ echelon, scales = "free") +
  xlab("années de service") +
  ylab("salaire (en milliers de dollars USD)") + theme(legend.position = "bottom")
library(patchwork)
p1 + p2 + plot_layout(width = c(1,3))
```

Ainsi, le salaire augmente avec les années, mais la variabilité croît également. Les professeurs adjoints qui ne sont pas promus sont généralement mis à la porte, aussi il y a moins d'occasions pour que les salaires varient sur cette échelle. Il y a peu de femmes dans l'échantillon: moins d'information signifie moins de puissance pour détecter de petites différences de salaire. Si on fait un tableau de contingence de l'échelon et du sexe, on peut calculer la proportion relative homme/femme dans chaque échelon: `r round(100*11/(56+11),0)`\% des profs adjoints, `r round(100*10/(54+10),0)`\% pour les aggrégés, mais seulement `r round(100*18/(248+18),0)`\% des titulaires alors que ces derniers sont mieux payés en moyenne.

```{r tableaucontingence, eval = TRUE, echo = FALSE, fig.align = "center"}
knitr::kable(table(college$sexe, college$echelon),
             caption = "Tableau de contingence donnant le nombre de professeurs du collège par sexe et par échelon académique.",
             booktabs = TRUE)
```

Plusieurs des variables explicatives potentielles des données `college` sont cat/gorielles (`echelon`, `sexe`, `discipline`), les deux dernières étant binaires. Les variables numériques `annees` et `service` sont fortement corrélées, avec une corrélation linéaire de `r with(hecmodstat::college, cor(annees, service))`.

:::


:::{#exm-donnees-manquantes-moon}

## Analyse exploratoire et données manquantes


Il convient de vérifier pour les données de @Moon.VanEpps:2023 que la description de la collecte coïncide avec la structure. Puisque les personnes qui n'ont pas donné ne remplissent pas le champ pour le montant, ce dernier indique une valeur manquante. Tous les montants des dons sont entre 0.25$ et 25$.

```{r}
#| eval: true
#| echo: true
data(MV23_S1, package = "hecedsm")
str(MV23_S1)
summary(MV23_S1)
```


Si nous incluons `amount` comme variable réponse dans un modèle de régression, les 235 observations manquantes seront supprimées par défaut. Cela ne pose pas de problème si nous voulons comparer le montant moyen des personnes qui ont fait un don, mais dans le cas contraire, nous devons transformer les `NA` en zéros. La variable `donate` ne doit pas être incluse comme variable explicative dans le modèle, car elle permet de prédire exactement les personnes qui n'ont pas donné.

:::



### Spécification du modèle pour la moyenne


La première étape d'une analyse consiste à décider quelles variables explicatives doivent être ajoutées à l'équation de la moyenne, et sous quelle forme. Les modèles ne sont que des approximations de la réalité; la section 2.1 de @Venables:2000 affirme que, si nous pensons que la véritable fonction moyenne reliant les variables explicatives $\boldsymbol{X}$ et la réponse $Y$ est de la forme $\mathsf{E}(Y \mid \boldsymbol{X}) = f(\boldsymbol{X})$ pour $f$ suffisamment lisse, alors le modèle linéaire est une approximation du premier ordre. À des fins d'interprétation, il est logique de centrer sur la moyenne toute variable explicative continue, car cela facilite l'interprétation.


Dans un cadre expérimental, où la condition expérimentale est attribué de manière aléatoire, nous pouvons directement comparer les différents traitements et tirer des conclusions causales (puisque toutes les autres choses sont égales en moyenne constantes, toute différence détectable est due en moyenne à notre manipulation). Bien que nous nous abstenions généralement d'inclure d'autres variables explicatives afin de préserver la simplicité du modèle, il peut néanmoins être utile de prendre en compte certaines variables concomitantes qui expliquent une partie de la variabilité afin de filtrer le bruit de fond et d'augmenter la puissance de l'étude. Par exemple, pour les données de @Baumann:1992, l'objectif est de comparer les scores moyens en fonction de la méthode d'enseignement, nous inclurions `group`. Dans cet exemple, il serait également logique d'inclure le résultat `pretest1` en tant qu'élément explicatif pour `posttest1`. De cette façon, nous modéliserons la différence moyenne d'amélioration entre le pré-test et le post-test plutôt que le résultat final.




Dans un contexte observationnel, les participants dans différents groupes ont des caractéristiques différentes et nous devons donc tenir compte de ces différences. Les modèles linéaires utilisés en économie et en finance contiennent souvent des variables de contrôle au modèle pour tenir compte des différences potentielles dues aux variables sociodémographiques (âge, revenu, etc.) qui seraient corrélées à l'appartenance aux groupes.  Tout test de coefficients ne prendrait en compte que la corrélation entre le résultat $Y$ et le facteur explicatif postulé d'intérêt.

## Interprétation des coefficients

La spécification de la moyenne est
\begin{align*}
\mathsf{E}(Y_i \mid \boldsymbol{X}_i = \boldsymbol{x}_i) = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}.
\end{align*}
L'**ordonnée à l'origine** $\beta_0$ est la **valeur moyenne de $Y$** lorsque toutes les variables explicatives du modèles sont nulles, soit $\boldsymbol{x}_i=\boldsymbol{0}_p$.
\begin{align*}
\beta_0 &= \mathsf{E}(Y \mid X_1=0,X_2=0,\ldots,X_p=0) \\
&= \beta_0 + \beta_1 \times 0 + \beta_2 \times 0 + \cdots + \beta_p \times 0
\end{align*}
Bien sur, il se peut que cette interprétation n'ait aucun sens dans le contexte étudié. Centrer les variables explicatives numériques (pour que leurs moyennes soit zéro) permet de rendre l'ordonnée à l'origine plus interprétable.


En régression linéaire, le paramètre $\beta_j$ mesure l'effet de la variable $X_j$ sur la variable $Y$ une fois que l'on tient compte des effets des autres variables explicatives. Pour chaque augmentation d'une unité de $X_j$, la réponse $Y$ augmente en moyenne de $\beta_j$ lorsque les autres variables demeurent inchangées,
\begin{align*}
\beta_j &= \mathsf{E}(Y \mid X_j= x_j+1, \boldsymbol{X}_{-j} = \boldsymbol{x}_{-j})  - \mathsf{E}(Y \mid \boldsymbol{X} = \boldsymbol{x}) \\
&= \sum_{\substack{k=1\\k \neq j}}^p \beta_kx_k + \beta_j(x_j+1) - \sum_{k=1}^p \beta_k x_k
\end{align*}

:::{#def-effet-marginal}

## Effet marginal

On définit l'effet marginal comme la dérivée première de la moyenne conditionnelle par rapport à $X_j$, soit
$$\text{effet marginal de }X_j =  \frac{\partial \mathsf{E}(Y \mid \boldsymbol{X})}{ \partial X_j}.$$
Le coefficient $\beta_j$ est aussi l'*effet marginal* de la variable $X_j$.

:::



Les variables indicatrices, qui prennent typiquement des valeurs de $-1$, $0$ et $1$, servent à indiquer l'appartenance aux différentes modalités d'une variable catégorielle. Par exemple, pour une variable indicatrice binaire, nous pouvons créer une colonne dont les entrées sont $1$ pour le groupe de traitement et $0$ pour le groupe de contrôle.



:::{#exm-moon}

## Modèle linéaire avec une seule variable binaire

Considérons par exemple un modèle linéaire pour les données de @Moon.VanEpps:2023 qui inclut le montant (`amount`) (en dollars, de 0 pour les personnes qui n'ont pas fait de don, jusqu'à 25 dollars).

L'équation du modèle linéaire simple qui inclut la variable binaire `condition` est
\begin{align*}
\mathsf{E}(\texttt{amount} \mid \texttt{condition})&= \beta_0 + \beta_1 \mathbf{1}_{\texttt{condition}=\texttt{quantity}}.
\\&= \begin{cases}
\beta_0, & \texttt{condition}=0, \\
\beta_0 + \beta_1 & \texttt{condition}=1.
\end{cases}
\end{align*}
Soit $\mu_0$ l'espérance du montant pour le groupe contrôle (`open-ended`) et $\mu_1$ celui des participants du groupe de traitement (`quantity`). Un modèle linéaire qui ne contient qu'une variable binaire $X$ comme régresseur revient à spécifier une moyenne différente pour chacun des deux groupes. L'ordonnée à l'origine $\beta_0$ est la moyenne du groupe contrôle. La moyenne du groupe traitement (`quantity`) est $\beta_0 + \beta_1 = \mu_1$ et donc $\beta_1=\mu_1-\mu_0$ est la différence du montant moyen de dons entre le groupe `open-ended`  et le groupe `quantity`. Cette paramétrisation est commode si on veut tester s'il y a une différence moyenne entre les deux groupes, puisque cette hypothèse nulle correspond à $\mathscr{H}_0: \beta_1=0$.

```{r}
#| label: fig-donation-moon
#| eval: true
#| echo: false
#| fig-align: 'center'
#| fig-cap: "Modèle linéaire simple pour les données `MV23_S1` avec `condition` comme variable explicative binaire, avec nuage de points décalés et un diagramme en demi-violin. Les cercles indiquent les moyennes de l'échantillon."
data("MV23_S1", package = "hecedsm")
MV23_S1 <- MV23_S1 |>
  dplyr::mutate(amount2 = ifelse(is.na(amount), 0, amount),
                condbin = ifelse(condition == "quantity",1,0))
coefs <- coef(lm(data = MV23_S1, amount2 ~ condbin))
ggplot(data = MV23_S1,
   aes(x = condbin, y = amount2, group = condition)) +
  see::geom_violindot(aes(col = condition), position_dots = position_jitter(width = 0.05)) +
  MetBrewer::scale_color_met_d("Hiroshige") +
  geom_abline(intercept = coefs[1], slope = coefs[2]) +
   scale_x_continuous(breaks = 0:1,
                     limits = c(-0.1,1.5),
                     labels = 0:1) +
  theme(legend.position = "inside",
        legend.position.inside = c(.95, .95),
        legend.justification = c("right", "top")) +
  labs(x = "condition", y = "montant (en dollars)") +
  stat_summary(fun = mean, aes(col = condition))
```


Même si le modèle linéaire définit une droite, cette dernière ne peut être évaluée qu'à $0$ ou $1$; la @fig-donation-moon montre cette droite avec en plus un nuage de points des montants, décalés horizontalement, et de la densité pour chaque condition. Le point coloré indique la moyenne empirique, qui correspond aux estimations.

Même s'il est clair que les données sont fortement discrétisées avec beaucoup de doublons et de zéros, l'échantillon a une taille de `r nrow(MV23_S1)` observations, donc les conclusions quant aux moyennes de groupe seront fiables.

:::



Considérons des variables catégorielles avec $K > 2$ niveaux, qui dans **R** sont de la classe `factor`. La paramétrisation par défaut des facteurs se fait en termes de contraste de traitement: le niveau de référence du facteur (par défaut, la première valeur dans l'ordre alphanumérique) sera traité comme la catégorie de référence et assimilé à l'ordonnée à l'origine. Le logiciel créera alors un ensemble de $K-1$ variables indicatrices pour un facteur à $K$ niveaux, chacune d'entre elles ayant un pour la catégorie représentée et zéro dans le cas contraire.

:::{#exm-baumann-dummies}

# Codage binaire pour les variables catégorielles

Considérons l'étude de @Baumann:1992 et la seule variable `group`. Les données sont classées par groupe : les 22 premières observations concernent le groupe `DR`, les 22 suivantes le groupe `DRTA` et les 22 dernières le groupe `TA`. Si nous ajustons un modèle avec `groupe` comme variable catégorielle

```{r}
#| eval: true
#| echo: true
data(BSJ92, package = "hecedsm")
class(BSJ92$group) # Vérifier que group est un facteur
levels(BSJ92$group) # première valeur est la catégorie de référence
# Imprimer trois lignes de la matrice du modèle
# (trois enfants de groupes différents)
model.matrix(~ group, data = BSJ92)[c(1,23,47),]
# Comparer avec les niveaux des facteurs
BSJ92$group[c(1,23,47)]
```

Si nous ajustons un modèle avec `groupe` comme variable catégorielle, la spécification de la moyenne du modèle est $$\mathsf{E}(Y \mid \texttt{group})= \beta_0 + \beta_1\mathbf{1}_{\texttt{group}=\texttt{DRTA}} + \beta_2\mathbf{1}_{\texttt{group}=\texttt{TA}}.$$
Puisque la variable `group` est catégorielle avec $K=3$ niveaux, il nous faut mettre $K-1 = 2$ variables indicatrices.

Avec la paramétrisation en termes de **traitements** (option par défaut), on obtient

-  $\mathbf{1}_{\texttt{group}=\texttt{DRTA}}=1$  si `group=DRTA` et zéro sinon,
-  $\mathbf{1}_{\texttt{group}=\texttt{TA}}=1$ si `group=TA` et zéro sinon.

Étant donné que le modèle comprend une ordonnée à l'origine et que le modèle décrit en fin de compte trois moyennes de groupe, nous n'avons besoin que de deux variables supplémentaires. Avec la paramétrisation en termes de **traitements**, la moyenne du groupe de référence est l'ordonnée à l'origine. Si `group`=`DR` (référence), les deux variables indicatrices binaires `groupDRTA` et `groupTA` sont nulles. La moyenne de chaque groupe est

- $\mu_{\texttt{DR}} = \beta_0$,
- $\mu_{\texttt{DRTA}}=\beta_0 + \beta_1$ et
- $\mu_{\texttt{TA}} = \beta_0 + \beta_2$.

Ainsi, $\beta_1$ est la différence de moyenne entre les groupes `DRTA` et`DR`, et de la même façon $\beta_2=\mu_{\texttt{TA}}- \mu_{\texttt{DR}}$.

:::

:::{#rem-sumtozero}

## Contrainte de somme nulle

La paramétrisation discutée ci-dessus, qui est la valeur par défaut de la fonction `lm`, n'est pas la seule disponible. Plutôt que de comparer la moyenne de chaque groupe avec celle d'une catégorie de référence, la paramétrisation par défaut pour les modèles d'analyse de la variance est en termes de contraintes de somme nulle pour les coefficients, où l'ordonnée à l'origine est la moyenne équi-pondérée de chaque groupe, et les paramètres $\beta_1, \ldots, \beta_{K-1}$ sont des différences par rapport à cette moyenne.

```{r}
#| eval: false
#| echo: true
model.matrix(
    ~ group,
    data = BSJ92,
    contrasts.arg = list(group = "contr.sum"))
```

```{r}
#| eval: true
#| echo: false
#| label: tbl-sum2zero
#| tbl-cap: "Paramétrisation des variables indicatrices pour la contrainte de somme nulle pour une variable catégorielle."
modmat_sum2zero <- model.matrix(
    ~ group,
    data = BSJ92,
    contrasts.arg = list(group = "contr.sum"))[c(1,23,47),]
rownames(modmat_sum2zero) <- BSJ92$group[c(1,23,47)]
kable(modmat_sum2zero, booktabs = TRUE, row.names = TRUE)
```

Dans la contrainte de somme nulle, nous obtenons à nouveau deux variables indicatrices, `group1` et `group2`, ainsi que l'ordonnée à l'origine. La valeur de `group1` est $1$ si `group=DR`, $0$ si `group=DRTA` et $-1$ si `group=TA`. ous trouvons $\mu_{\texttt{DR}} = \beta_0 + \beta_1$, $\mu_{\texttt{DRTA}}=\beta_0 + \beta_2$ et $\mu_{\texttt{TA}} = \beta_0 - \beta_1 - \beta_2$. Quelques manipulations algébriques révèlent que
$\beta_0 = (\mu_{\texttt{DR}} +\mu_{\texttt{DRTA}}+\mu_{\texttt{TA}})/3$, l'espérance équipondérée des différents niveaux. De manière générale, l'ordonnée à l'origine moins la somme de tous les autres coefficients liés aux facteurs. 


En supprimant l'ordonnée à l'origine, on pourrait inclure trois variables indicatrices pour chaque niveau d'un facteur et chaque paramètre correspondrait alors à la moyenne. Ce n'est pas recommandé dans **R** car le logiciel traite différemment les modèles sans ordonnée à l'origine et certains résultats seront absurdes (par exemple, le coefficient de détermination sera erroné).

:::


:::{#exm-college-coeff}

## Interprétation des coefficients

On considère un modèle de régression pour les données `college` qui inclut le sexe, l'échelon académique, le nombre d'années de service et le domaine d'expertise (appliquée ou théorique).


Le modèle linéaire postulé s'écrit

\begin{align*}
\texttt{salaire} &= \beta_0 + \beta_1 \mathbf{1}_{\texttt{sexe}=\texttt{femme}} +\beta_2 \mathbf{1}_{\texttt{domaine}=\texttt{theorique}} \\&\quad +\beta_3 \mathbf{1}_{\texttt{echelon}=\texttt{aggrege}}
+\beta_4 \mathbf{1}_{\texttt{echelon}=\texttt{titulaire}} \\&\quad+\beta_5 \texttt{service} + \varepsilon.
\end{align*}

```{r}
#| label: tbl-collegecoefs
#| tbl-cap: "Estimations des coefficients du modèle linéaire pour les données $\\texttt{college}$ (en dollars USD, arrondis à l'unité)."
#| eval: true
#| echo: false
college$sexe <- relevel(x = college$sexe, ref = "homme")
college_lm <- lm(salaire ~  sexe + domaine + echelon + service , data = college)
coefs_college_lm <- round(coef(college_lm)*1000,0)
names(coefs_college_lm) <- paste0("$\\widehat{\\beta}_",0:5,"$")
knitr::kable(t(coefs_college_lm),
     booktabs = TRUE, escape = FALSE)
```


L'interprétation des coefficients est la suivante:

- L'ordonnée à l'origine $\beta_0$ correspond au salaire moyen d'un professeur adjoint (un homme) qui vient de compléter ses études et qui travaille dans un domaine appliqué: on estime ce salaire à $\widehat{\beta}_0=`r coefs_college_lm[1]`$ dollars.
- toutes choses étant égales par ailleurs (même domaine, échelon et années depuis le dernier diplôme), l'écart de salaire entre un homme et un femme est estimé à  $\widehat{\beta}_1=`r coefs_college_lm[2]`$ dollars.
- *ceteris paribus*, un(e) professeur(e) qui oeuvre dans un domaine théorique gagne $\beta_2$ dollars de plus qu'une personne du même sexe dans un domaine appliqué; on estime cette différence à $`r coefs_college_lm[3]`$ dollars.
- *ceteris paribus*, la différence moyenne de salaire entre professeurs adjoints et aggrégés est estimée à  $\widehat{\beta}_3=`r coefs_college_lm[4]`$ dollars.
- *ceteris paribus*, la différence moyenne de salaire entre professeurs adjoints et titulaires est de $\widehat{\beta}_4=`r coefs_college_lm[5]`$ dollars.
- au sein d'un même échelon, chaque année supplémentaire de service mène à une augmentation de salaire annuelle moyenne de $\widehat{\beta}_5=`r coefs_college_lm[6]`$ dollars.

:::

:::{#rem-polynomes}

## Polynômes

Il n'est pas toujours possible de fixer la valeur des autres colonnes de $\mathbf{X}$ si plusieurs colonnes contiennent des transformations ou des fonctions d'une même variable explicative. Par exemple, on pourrait par exemple considérer un polynôme d'ordre $k$ (ordinairement, on va prendre $k\leq 3$),
\begin{align*}
\mathsf{E}(Y \mid X=x)=\beta_0+ \beta_1 x+ \beta_2 x^2 + \cdots +\beta_k x^k.
\end{align*}
Si l'on inclut un terme d'ordre $k$, $x^k$, il faut **toujours** inclure les termes d'ordre inférieur $1, x, \ldots, x^{k-1}$ pour l'interprétabilité du modèle résultant (autrement, cela revient à choisir un polynôme en imposant que certains coefficients soient zéros). L'interprétation des effets des covariables nonlinéaires (même polynomiaux) est complexe parce qu'on ne peut pas « fixer la valeur des autres variables »: l'effet d'une augmentation d'une unité de $x$ *dépend de la valeur de cette dernière*. L'effet marginal  de $x$ est $\beta_1 + \sum_{j=1}^{k-1}j \beta_{j+1}x^j$.

L'utilisation de polynôme, plus flexibles, n'est généralement pas recommendée car ces derniers se généralisent mal hors de l'étendue observée des données. L'utilisation de splines avec une pénalité sur les coefficients, avec des modèles additifs, offre plus de flexibilité.

:::


:::{#exm-quadmod}

## Modèle quadratique pour les données automobile

Considérons un modèle de régression linéaire pour l'autonomie d'essence en fonction de la puissance du moteur pour différentes voitures dont les caractéristiques sont données dans le jeu de données `automobiles`. Le modèle postulé incluant un terme quadratique est
\begin{align*}
\texttt{autonomie}_i = \beta_0 + \beta_1 \texttt{puissance}_i + \beta_2 \texttt{puissance}_i^2 + \varepsilon_i
\end{align*}
Afin de comparer l'ajustement du modèle quadratique, on peut inclure également la droite ajustée du modèle de régression simple qui n'inclut que puissance.

```{r}
#| label: fig-autoquad2d
#| echo: false
#| fig-cap: Modèle de régression avec terme quadratique pour la puissance (gris), versus spline cubique pénalisée (ligne traitillée).
data(automobile, package = "hecmodstat")
mod <- lm(autonomie ~ puissance + I(puissance^2),  data = automobile)
ggplot(data = automobile, aes(x = puissance, y = autonomie)) +
  geom_point()+
      geom_line(data = data.frame(puissance = automobile$puissance, fitted = mod$fitted),
                aes(x = puissance, y = fitted), col = "grey", show.legend = FALSE) +
    geom_smooth(method = mgcv::gam, formula =  y ~ s(x, bs = "cs"), se = FALSE, col = "black", linetype = "dashed") +
  labs(#title = "Caractéristiques d'automobiles, circa 1983",
       x = "puissance du moteur (en chevaux-vapeurs)",
             y = "autonomie d'essence\n (en miles/US gallon)")
```

À vue d'oeil, l'ajustement quadratique est bon: nous verrons plus tard à l'aide de test si une simple droite aurait été suffisante.
On voit aussi dans la @fig-autoquad2d que l'autonomie d'essence décroît rapidement quand la puissance croît entre $0$ et $189.35$, mais semble remonter légèrement par la suite pour les voitures qui un moteur de plus de 200 chevaux-vapeurs, ce que le modèle quadratique capture.  Prenez garde en revanche à l'extrapolation là où vous n'avez pas de données (comme l'illustre remarquablement bien [le modèle cubique de Hassett pour le nombre de cas quotidiens de coronavirus](https://web.archive.org/web/20210315050023/https://livefreeordichotomize.com/2020/05/05/model-detective/)).

La représentation graphique du modèle polynomial de degré 2 présenté dans la @fig-autoquad2d peut sembler contre-intuitive, mais c'est une projection en 2D d'un plan 3D de coordonnées $\beta_0 + \beta_1x-y +\beta_2z =0$, où $x=\texttt{puissance}$, $z=\texttt{puissance}^2$ et $y=\texttt{autonomie}$. La physique et le bon-sens imposent la contrainte $z = x^2$, et donc les valeurs ajustées vivent sur une courbe dans un sous-espace du plan ajusté, représenté en gris dans la @fig-hyperplan.



```{r}
#| label: fig-hyperplan
#| echo: false
#| warning: false
#| message: false
#| fig-cap: Représentation graphique 3D du modèle de régression linéaire pour les données
#|   $\texttt{automobile}$.
out_type <- knitr::opts_knit$get("rmarkdown.pandoc.to")
if(out_type == 'html'){
  data(automobile, package = "hecmodstat")
  automobile$puissance2 <- with(automobile, I(puissance^2))
  mod <- lm(autonomie ~ puissance + puissance2,  data = automobile)
  to_plot_x <- with(automobile, range(puissance))
  to_plot_y <- with(automobile, range(puissance2))
  df <- data.frame(puissance = rep(to_plot_x, 2),
                   puissance2 = rep(to_plot_y, each = 2))
  df["pred"] <- predict.lm(mod, df, se.fit = FALSE)
  surf <- reshape2::acast(df, puissance2 ~ puissance)
  color <- rep(0, length(df))
  automobile$pred <- predict(mod)
  df2 <- data.frame(x = (0:250),
                    y = (0:250)^2,
                    z = predict(mod, data.frame(puissance = 0:250, puissance2 = (0:250)^2))
  )
  scene <- list(
    xaxis = list(title = "puissance (chevaux vapeurs)"),
    yaxis = list(title = "puissance carré"),
    zaxis = list(title = "autonomie d'essence (miles au gallon)"))
  automobile %>%
    plotly::plot_ly(colors = "grey") %>%
    plotly::add_markers(x = ~puissance, y = ~puissance2, z = ~autonomie,
                name = "data",
                opacity = .8,
                marker=list(color = 'black', size = 4, hoverinfo="skip", opacity = 0.8)) %>%
     plotly::add_surface(x = to_plot_x, y = to_plot_y, z = ~surf,
                         inherit = FALSE,
                         name = "Relation entre puissance et autonomie",
                         opacity = .75, cauto = FALSE, surfacecolor = color) %>%
    plotly::add_trace(data = df2,
                      x=~x, y = ~y, z = ~z,
                      type = 'scatter3d', mode = 'lines', color = "grey") %>%
    plotly::layout(scene = scene) %>%
    plotly::hide_guides()


#library(rgl)
#plot3d(y = automobile$autonomie, x = automobile$puissance, z = I(automobile$puissance^2),
#          xlab = expression("puissance"),
#          ylab = expression("autonomie"),
#          zlab = expression(paste("puissance"^{2})),
#          axis.col = rep("black", 3))
#ols <- coef(mod)
#ran <- range(automobile$puissance)
#hor_seq <- seq(from = ran[1], to = ran[2], length = 1000)
#hor2_seq <- hor_seq^2
#mpg_seq <- ols[1] + ols[2]*hor_seq + ols[3]*hor2_seq

#points3d(x = hor_seq, z = hor2_seq, y = mpg_seq, col = hecblue)
#planes3d(a = ols[2], c = ols[3], b = -1, d = ols[1], alpha = 0.1)
#rglwidget()
} else{
  knitr::include_graphics('images/hyperplan_auto.png')
}
```


:::



## Estimation des paramètres

Considérons un échantillon de $n$ observations. On n'observe ni les aléas $\boldsymbol{\varepsilon}$, ni les paramètres $\boldsymbol{\beta}$: il est donc impossible de recouvrer les (vrais) coefficients du modèle. Effectivement, le système d'équation spécifié par le modèle linéaire inclut $n+p+1$ inconnues, mais uniquement $n$ observations. Si on se concentre sur les $p+1$ paramètres de moyenne et sur la variance $\sigma^2$, nous pourrons estimer les paramètres généralement si $n> p+2$, mais cela dépend de la spécification. Une infinité de plans pourraient passer dans le nuage de points; il faut donc choisir la meilleure droite (selon un critère donné). La section aborde le choix de ce critère et l'estimation des paramètres de la moyenne.

### Moindres carrés ordinaires

Soit une matrice de modèle $\mathbf{X}$ et une formulation pour la moyenne avec $\mathsf{E}(Y_i) = \mathbf{x}_i\boldsymbol{\beta}$. Les estimateurs des moindres carrés ordinaires $\widehat{\boldsymbol{\beta}}=(\widehat{\beta}_0, \ldots, \widehat{\beta}_p)$ sont les paramètres qui minimisent simultanément la distance euclidienne entre les observations $y_i$ et les **valeurs ajustées** $\widehat{y}_i=\mathbf{x}_i\widehat{\boldsymbol{\beta}}$. 

```{r}
#| eval: true
#| echo: false
#| label: fig-vertdist
#| fig-cap: Résidus ordinaires $e_i$ (vecteurs verticaux) ajoutés à la droit de régression dans l'espace $(x, y)$ (gauche) et l'ajustement de la variable réponse $y_i$ en fonction des valeurs ajustées $\widehat{y}_i$. 
set.seed(1234)
n <- 100L
x <- rexp(n = n, rate = 1/100)
y <- 100*rt(n, df = 10) + 40 + 2*x
ols <- lm(y ~ x)
res <- resid(ols)
yhat <- fitted(ols)
df <- data.frame(x = x, y = y, res = res, fitted = yhat)
vlines <- data.frame(x1 = x, 
    y1 = yhat, 
    y2 = yhat + res)
vlines2 <- data.frame(x1 = yhat, 
    y1 = y, 
    y2 = y - res)
g1 <- ggplot(data = df, 
       aes(x = x, y = y)) +
        geom_point() +
  geom_smooth(method = "lm", 
              formula = y ~ x,
              se = FALSE, 
              col ="black") +
   labs(x = "variable explicative",
       y = "variable réponse") +
        geom_segment(aes(x = x1, y = y1, xend = x1, yend = y2), arrow= arrow(length = unit(0.2,"cm")), color = "grey",
                     data = vlines, show.legend = FALSE)
g2 <- ggplot(data = df, aes(x = yhat, y = y)) +
        geom_point() +
geom_abline(intercept = 0, slope = 1) +
   labs(x = "valeurs ajustées",
       y = "variable réponse") +
scale_x_continuous(limits = range(c(yhat, y)),
                   expand = expansion()) + scale_y_continuous(limits = range(c(yhat, y)),
                   expand = expansion()) +
geom_segment(aes(x = x1, y = y1, xend = x1, yend = y2), 
arrow= arrow(length = unit(0.2,"cm")), color = "grey",
                     data = vlines2, show.legend = FALSE)
g1 + g2
```





En d'autres mots, les estimateurs des moindres carrés sont la solution du problème d'optimization convexe
\begin{align*}
\widehat{\boldsymbol{\beta}} &=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}\sum_{i=1}^n (Y_i-\widehat{Y}_i)^2= \min_{\boldsymbol{\beta}} \|\boldsymbol{Y}-\mathbf{X}\boldsymbol{\beta}\|^2
\end{align*}
Ce système d'équation a une solution  explicite qui est plus facilement exprimée en notation matricielle. Soit les matrices et vecteurs
\begin{align*}
\boldsymbol{Y} =
 \begin{pmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
 \end{pmatrix} ,
 \;
\mathbf{X} = \begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix} , \;
\boldsymbol{\beta} =
 \begin{pmatrix}
  \beta_1 \\
  \beta_2 \\
  \vdots \\
  \beta_p
 \end{pmatrix}
\end{align*}


:::{#prp-ols-mle}

## Moindres carrés ordinaires

L'estimateur des moindres carrés ordinaires résoud le problème d'optimisation non-contraint
\begin{align*}
\widehat{\boldsymbol{\beta}}=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}
On peut calculer la dérivée première par rapport à $\boldsymbol{\beta}$, égaler à zéro et isoler le maximum pour obtenir une formule explicite pour $\widehat{\boldsymbol{\beta}}$,
\begin{align*}
\mathbf{0}_n&=\frac{\partial}{\partial\boldsymbol{\beta}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\\
\\&=\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}\\
 \\&=\mathbf{X}^\top (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\end{align*}
en utilisant la [règle de dérivation en chaîne](http://www.stat.rice.edu/~dobelman/notes_papers/math/Matrix.Calculus.AppD.pdf); on peut ainsi distribuer les termes pour obtenir l'*équation normale*
\begin{align*}
 \mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}&=\mathbf{X}^\top \boldsymbol{y}.
\end{align*}
Si $\mathbf{X}$ est une matrice de rang $p$, alors la forme quadratique $\mathbf{X}^\top \mathbf{X}$ est inversible et l'unique solution du problème d'optimisation est
\begin{align*}
\widehat{\boldsymbol{\beta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \boldsymbol{Y}.
\end{align*}
Si le rang de la matrice $\mathbf{X}$ est dimension $n \times (p+1)$ est de rang $p+1$, l'unique solution du problème d'optimisation est
$$
\widehat{\boldsymbol{\beta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \boldsymbol{Y}.
$$ {#eq-ols}
Cet estimateur dit des **moindres carrés ordinaires** (MCO) est explicite; il n'est donc pas nécessaire de procéder à l'optimisation à l'aide d'algorithmes numériques.

:::

### Maximum de vraisemblance


Nous pourrions également envisager l'estimation du maximum de vraisemblance. @prp-mle-normal-linmod montre que, en supposant la normalité des aléas, les estimateurs des moindres carrés de $\boldsymbol{\beta}$ coïncident avec ceux du maximum de vraisemblance.


:::{#prp-mle-normal-linmod}

## Estimation du maximum de vraisemblance du modèle linéaire normal


Le modèle de régression linéaire spécifie que les observations $Y_i \sim \mathsf{normale}(\mathbf{x}_i\boldsymbol{\beta}, \sigma^2)$ sont indépendantes. 
Le modèle linéaire a $p+2$ paramètres ($\boldsymbol{\beta}$ et $\sigma^2$) et la log-vraisemblance est, abstraction faite des termes constants,
\begin{align*}
\ell(\boldsymbol{\beta}, \sigma)&\propto-\frac{n}{2} \ln (\sigma^2) -\frac{1}{2\sigma^2}\left\{(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\right\}^2.
\end{align*}
Maximiser la log-vraisemblance par rapport à $\boldsymbol{\beta}$ revient à minimiser la somme du carré des erreurs $\sum_{i=1}^n (y_i - \mathbf{x}_i\boldsymbol{\beta})^2$, quelle que soit la valeur de  $\sigma$, et on recouvre $\widehat{\boldsymbol{\beta}}$. L'estimateur du maximum de vraisemblance de la variance  $\widehat{\sigma}^2$ est
\begin{align*}
\widehat{\sigma}^2=\mathrm{arg max}_{\sigma^2} \ell(\widehat{\boldsymbol{\beta}}, \sigma^2).
\end{align*}
La log-vraisemblance profilée de $\sigma^2$, abstraction faite des constantes, est
\begin{align*}
\ell_{\mathrm{p}}(\sigma^2)
&\propto-\frac{1}{2}\left\{n\ln\sigma^2+\frac{1}{\sigma^2}(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{\beta}})^\top(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{\beta}})\right\}.
\end{align*}
En différenciant chaque terme par rapport à $\sigma^2$ et en fixant le gradient à zéro, on obtient 
\begin{align*}
\frac{\partial \ell_{\mathrm{p}}(\sigma^2)}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{\beta}})^\top(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{\beta}})}{2\sigma^4} = 0
\end{align*}

On déduit que l'estimateur du maximum de vraisemblance est la moyenne des carrés des résidus,
\begin{align*}
\widehat{\sigma}^2&=\frac{1}{n}(\boldsymbol{Y}-\mathbf{X}\hat{\boldsymbol{\beta}})^\top(\boldsymbol{Y}-\mathbf{X}\hat{\boldsymbol{\beta}})\\&= \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{x}_i\widehat{\boldsymbol{\beta}})^2= \frac{\mathsf{SC}_e}{n};
\end{align*}
L'estimateur sans biais habituel de $\sigma^2$ calculé par le logiciel est
$$S^2=\mathsf{SC}_e/(n-p-1),
$$ où le dénominateur est la taille de l'échantillon $n$ moins le nombre de paramètres de la moyenne $\boldsymbol{\beta}$, soit $p+1$.

:::


:::{#prp-info-normal}

## Matrices d'information pour modèles linéaires normaux.

Les entrées de la matrice d'information observée du modèle linéaire normal sont les suivantes
\begin{align*}
-\frac{\partial^2 \ell(\boldsymbol{\beta}, \sigma^2)}{\partial \boldsymbol{\beta}\partial \boldsymbol{\beta}^\top} &= \frac{1}{\sigma^2} \frac{\partial \mathbf{X}^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}^\top} =  \frac{\mathbf{X}^\top\mathbf{X}}{\sigma^2}\\
-\frac{\partial^2 \ell(\boldsymbol{\beta}, \sigma^2)}{\partial \boldsymbol{\beta}\partial \sigma^2} &=- \frac{\mathbf{X}^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\sigma^4}\\
-\frac{\partial^2 \ell(\boldsymbol{\beta}, \sigma^2)}{\partial (\sigma^2)^2} &= -\frac{n}{2\sigma^4} + \frac{(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\sigma^6}.
\end{align*}
Si on évalue l'information observée aux EMV, on obtient
\begin{align*}
j(\widehat{\boldsymbol{\beta}}, \widehat{\sigma^2}) = 
\begin{pmatrix}
\frac{\mathbf{X}^\top\mathbf{X}}{\widehat{\sigma^2}} & \boldsymbol{0}_{p+1} \\  \boldsymbol{0}_{p+1}^\top & \frac{n}{2\widehat{\sigma^4}}
\end{pmatrix}
\end{align*}
puisque $\widehat{\sigma}^2=\mathsf{SC}_e/n$ et que les résidus sont orthogonaux à la matrice du modèle. Sachant que  $\mathsf{E}(Y \mid \mathbf{X})=\mathbf{X}\boldsymbol{\beta}$, la matrice d'information de Fisher est
\begin{align*}
i(\boldsymbol{\beta}, \sigma^2) = 
\begin{pmatrix}
\frac{\mathbf{X}^\top\mathbf{X}}{\sigma^2} & \boldsymbol{0}_{p+1} \\  \boldsymbol{0}_{p+1}^\top & \frac{n}{2\sigma^4}
\end{pmatrix}
\end{align*}
Puisque la loi asymptotique de l'estimateur est normale, les EMV de $\sigma^2$ et $\boldsymbol{\beta}$ sont asymptotiquement indépendants car leur corrélation asymptotique est nulle.Pourvu que la matrice carrée $(p+1)$, $\mathbf{X}^\top\mathbf{X}$ soit inversible, la variance asymptotique des estimateurs est $\mathsf{Var}(\widehat{\boldsymbol{\beta}})=\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}$ et $\mathsf{Var}(\widehat{\sigma}^2) = 2\sigma^4/n$.

:::


:::{#rem-independance}

Si on suppose que les observations sont normales, alors on peut montrer que $\mathsf{SC}_e/\sigma^2 \sim \chi^2_{n-p-1}$ et $\widehat{\boldsymbol{\beta}} \sim \mathsf{normale}\{\boldsymbol{\beta}, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\}$ sont indépendants et leurs lois sont connues. Cela nous permettra de construire des tests d'hypothèse.

:::

### Ajustement des modèles linéaires à l'aide d'un logiciel

Bien que nous puissions construire la matrice du modèle nous-mêmes et utiliser la formule des moindres carrés de l'@eq-ols, les routines numériques implémentées dans les logiciels sont préférables car plus stables. La fonction `lm` dans **R** ajuste **les modèles linéaires**, tout comme `glm` avec les arguments par défaut. Les objets de la classe `lm` ont plusieurs méthodes qui vous permettent d'extraire des objets spécifiques des objets `lm`. Par exemple, les fonctions `coef`, `resid`, `fitted`, `model.matrix` renvoient les estimations des coefficients $\widehat{\boldsymbol{\beta}},$ les résidus ordinaires $\boldsymbol{e},$ les valeurs ajustées $\widehat{\boldsymbol{y}}$ et la matrice du modèle $\mathbf{X}$.


```{r fitlm}
#| eval: false
#| echo: true
data(BSJ92, package = "hecedsm") # charger les données
str(BSJ92) # vérifier que les variables catégorielles sont "factor"
# Ajustement de la régression linéaire
linmod <- lm(posttest1 ~ pretest1 + group, 
             data = BSJ92)
est_beta <- coef(linmod) # coefficients (betas)
vcov_beta <- vcov(linmod) # matrice de covariance des betas
summary(linmod) # tableau résumé
beta_ic <- confint(linmod) # IC de Wald pour betas
y_adj <- fitted(linmod) # valeurs ajustées
e <- resid(linmod) # résidus ordinaires

# Vérifier la formule des moindres carrés ordinaires
X <- model.matrix(linmod) # matrice du modèle
y <- college$salary
isTRUE(all.equal(
  c(solve(t(X) %*% X) %*% t(X) %*% y),
  as.numeric(coef(linmod))
))
```


La méthode `summary` est sans doute la plus utile: elle affiche les estimations des paramètres de la moyenne ainsi que leurs erreurs type, les valeurs $t$ pour le test de Wald de l'hypothèse $\mathscr{H}_0 : \beta_i=0$ et les valeurs-$p$ associées. D'autres statistiques descriptives, portant sur la taille de l'échantillon, les degrés de liberté, etc. sont données au bas du tableau. Notez que la fonction `lm` utilise l'estimateur sans biais de la variance $\sigma^2$.


## Prédictions {#sec-predictions-lm}

Une fois les estimations des coefficients obtenues, on peut calculer les valeurs ajustées $\widehat{\boldsymbol{y}}$ avec $\mathbf{X}\widehat{\boldsymbol{\beta}}$, où $\mathbf{X}$ dénote la matrice du modèle $n \times (p+1)$. On peut aussi généraliser cette approche et obtenir une estimation de la moyenne pour n'importe quel vecteur lignes de covariables $\mathbf{x}^* = (1, x^*_1, \ldots, x^*_p)$, sachant que $\mathsf{E}(Y \mid \mathbf{x}^*)=\mathbf{x}^*\boldsymbol{\beta}$, en remplaçant les coefficients inconnus $\boldsymbol{\beta}$ par leurs estimations $\widehat{\boldsymbol{\beta}}$. Pour le modèle postulé, c'est le meilleur prédicteur linéaire non-biaisé de la moyenne.

Si l'on veut prédire la valeur d'une nouvelle observation, disons $Y^*$, dont le vecteur de variables explicatives $\mathbf{x}^*$ sont connues, la prédiction sera donc $\widehat{y}^* = \mathbf{x}^*\widehat{\boldsymbol{\beta}}$ parce que
\begin{align*}
\mathsf{E}(\widehat{Y}^* \mid \mathbf{X}, \mathbf{x}^*) = \mathsf{E}(\mathbf{x}^*\widehat{\boldsymbol{\beta}}\mid \mathbf{X}, \mathbf{x}^*) = \mathbf{x}^*\boldsymbol{\beta}.
\end{align*}
Cependant, les observations individuelles varient davantage que les moyennes (qui sont elles-mêmes basées sur plusieurs observations). Intuitivement, cela est dû à l'incertitude supplémentaire du terme d'erreur apparaissant dans l'équation du modèle: la variabilité des prédictions est la somme de l'incertitude due aux estimateurs (basés sur des données aléatoires) et de la variance intrinsèque des observations en supposant que la nouvelle observation est indépendante de celles utilisées pour estimer les coefficients,
\begin{align*}
\mathsf{Va}(Y^*-\widehat{Y}^* \mid \mathbf{X}, \mathbf{x}^*) &= \mathsf{Va}(Y^*  - \mathbf{x}^*\widehat{\boldsymbol{\beta}} \mid \mathbf{X}, \mathbf{x}^*)
\\&=\mathsf{Va}(Y^* \mid \mathbf{X}, \mathbf{x}^*) + \mathsf{Va}(\mathbf{x}^*\widehat{\boldsymbol{\beta}} \mid \mathbf{X}, \mathbf{x}^*)
\\& = \sigma^2 + \sigma^2\mathbf{x}^{*\vphantom{\top}}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}^{*\top}.
\end{align*}
On peut baser les intervalles de prédictions sur la loi Student-$t$, à l'aide du pivot
\begin{align*}
\frac{Y^*-\mathrm{x}^*\widehat{\boldsymbol{\beta}}}{\sqrt{S^2\{1+\mathrm{x}^*(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}^{*\top}\}}}\sim \mathsf{Student}(n-p-1).
\end{align*}
On obtient l'**intervalle de prédiction** de niveau $1-\alpha$ pour $Y^*$ en inversant la statistique de test
\begin{align*}
\mathrm{x}^*\widehat{\boldsymbol{\beta}}\pm \mathfrak{t}_{n-p-1}(\alpha/2)\sqrt{S^2\{1+\mathrm{x}^*(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}^{*\top}\}}.
\end{align*}
Des calculs similaires pour les **intervalles de confiance** ponctuels pour la moyenne $\mathrm{x}^*\boldsymbol{\beta}$ \donnent
\begin{align*}
\mathrm{x}^*\widehat{\boldsymbol{\beta}}\pm \mathfrak{t}_{n-p-1}(\alpha/2)\sqrt{S^2\mathrm{x}^*(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}^{*\top}}.
\end{align*}
Les deux formules diffèrent uniquement au niveau de la variabilité.



:::{#exm-sokolova-pred}

## Prédiction pour une régression linéaire simple

Considérons les données de l'@exm-sokolova. On ajuste un modèle de régression linéaire simple avec $\texttt{pef} = \beta_0 + \beta_1 \texttt{proportion} + \varepsilon$, où $\varepsilon \sim \mathsf{normal}(0,\sigma^2)$ et on suppose les observations indépendantes.


La @fig-predinterval  montre les bandes d'incertitude ponctuelles pour une simple régression linéaire des données de @Sokolova:2023 en fonction de la `proportion` de carton par rapport au plastique, les valeurs les plus élevées indiquant un emballage avec plus de carton superflu. Le modèle ne tient pas compte du fait que notre réponse provient d'une distribution discrète limitée avec des valeurs entières allant de 1 à 7, et que les ratios testés dans l'expérience sont 0 (pas de carton), 0.5, 1 et 2 uniquement. La droite centrale donne la prédiction des individus lorsque nous faisons varier la proportion carton/plastique. En examinant les formules des intervalles de confiance et de prédiction, il est clair que les bandes ne sont pas linéaires (nous considérons la racine carrée d'une fonction qui implique les prédicteurs), mais il n'est pas évident visuellement que l'incertitude augmente au fur et à mesure que l'on s'éloigne de la moyenne des prédicteurs.


Il est plus facile de s'en rendre compte en reproduisant les courbes potentielles qui auraient pu se produire avec des données différentes: la @fig-predinterval montre les nouvelles pentes potentielles générées à partir de la distribution normale asymptotique des estimateurs $\widehat{\boldsymbol{\beta}}$. La forme hyperbolique n'est pas surprenante: nous pivotons essentiellement les courbes à partir de la `pef`/`proportion` moyenne, et leur potentiel de déviation est d'autant plus élevé que nous nous éloignons de la moyenne dans chaque direction. Les intervalles de prédiction (gris pâle) sont très larges et couvrent essentiellement l'ensemble des valeurs potentielles de l'échelle de Likert sur la perception du respect de l'environnement, à l'exception de quelques observations. En revanche, les intervalles de confiance pour la moyenne sont assez étroits, en raison de la taille importante de l'échantillon. On constate également que les courbes s'en écartent peu.

```{r}
#| label: fig-predinterval
#| cache: true
#| echo: false
#| message: false
#| warning: false
#| fig-cap: Prédictions avec intervalles de prédiction (à gauche) et intervalles de confiance pour la moyenne (à droite) pour la régression linéaire simple de la perception du respect de l'environnement (`pef`) en fonction de la `proportion` de carton par rapport au plastique, avec des observations décalées horizontalement. Le graphique montre les prédictions ainsi que les intervalles de confiance ponctuels à 95 % de la moyenne et des prédictions individuelles. L'axe des ordonnées a été tronqué.
#| fig-format: png
library(gganimate)
data(SKD23_S2A, package = "hecedsm") # load data
lm_simple <- lm(pef ~ proportion, data = SKD23_S2A) # fit simple linear regression
# <- lm(intention ~ fixation, data = intention)
predci <- data.frame(cbind(proportion = seq(0, 2, by = 0.1),
                predict(lm_simple, newdata = data.frame(proportion = seq(0, 2, by = 0.1)),
                        interval = "c")[,-1]))
predpi <- data.frame(cbind(proportion = seq(0, 2, by = 0.1),
                predict(lm_simple, newdata = data.frame(proportion = seq(0, 2, by = 0.1)),
                        interval = "p")[,-1]))
nsim <- 25L
boot_dat <- data.frame(id = 1:nsim,
                     mvtnorm::rmvnorm(n = nsim, mean = coef(lm_simple), sigma = vcov(lm_simple)))
colnames(boot_dat) <- c("id","intercept", "slope")
out_type <- knitr::opts_knit$get("rmarkdown.pandoc.to")
if(out_type == 'html'){
   g1 <- ggplot(data = boot_dat) +
    geom_point(data = SKD23_S2A, aes(x=proportion, y=pef), col = "grey", alpha = 0.8,
               position = position_jitter(width = 0.1, height = 0)) +
    geom_ribbon(data = predpi, aes(x = proportion, ymin = lwr, ymax = upr), fill = "grey70", alpha = 0.2) +
    geom_ribbon(data = predci, aes(x = proportion, ymin = lwr, ymax = upr), fill = "grey70", alpha = 0.5) +
    geom_abline(slope = coef(lm_simple)[2], intercept = coef(lm_simple)[1]) +
    scale_y_continuous(limits = c(1,7), oob = scales::squish, breaks = 1:7, labels = 1:7) +
    scale_x_continuous(limits = c(0,2), oob = scales::squish) +
    labs(x = "proportion de carton/plastique", subtitle = "perception du respect de l'environnement",
         y = "") +
    geom_abline(mapping = aes(slope = slope, intercept = intercept), col = "gray10",
                linetype = "dashed", alpha = 0.4) + transition_manual(frames = id, cumulative = TRUE)
  g1
  g1
} else {
  ggplot(data = boot_dat) +
    geom_point(data = SKD23_S2A, aes(x=proportion, y=pef), col = "grey", alpha = 0.8,
               position = position_jitter(width = 0.1, height = 0)) +
    geom_ribbon(data = predpi, aes(x = proportion, ymin = lwr, ymax = upr), fill = "grey70", alpha = 0.2) +
    geom_ribbon(data = predci, aes(x = proportion, ymin = lwr, ymax = upr), fill = "grey70", alpha = 0.5) +
    geom_abline(slope = coef(lm_simple)[2], intercept = coef(lm_simple)[1]) +
    scale_y_continuous(limits = c(1,7), oob = scales::squish, breaks = 1:7, labels = 1:7) +
    scale_x_continuous(limits = c(0,2), oob = scales::squish) +
 labs(x = "proportion de carton/plastique", subtitle = "perception du respect de l'environnement",
         y = "") +
    geom_abline(mapping = aes(slope = slope, intercept = intercept), col = "gray10",
                linetype = "dashed", alpha = 0.4)
}
```
Dans **R**, la fonction générique `predict` prend comme arguments un modèle et une nouvelle base de données `newdata` contenant un tableau avec la même structure que les données qui ont servi à l'ajustement du modèle (à minima, les colonnes de variables explicatives utilisées dans le modèle).


```{r}
#| eval: false
#| echo: true
data(SKD23_S2A, package = "hecedsm") # charger les données
lm_simple <- lm(pef ~ proportion, data = SKD23_S2A) # régression linéaire simple
predict(lm_simple,
        newdata = data.frame(proportion = c(0, 0.5, 1, 2)),
        interval = "prediction") # intervalles de prédiction
predict(lm_simple,
        newdata = data.frame(proportion = c(0, 0.5, 1, 2)),
        interval = "confidence") # IC de confiance pour la moyenne
```

```{r}
#| label: tbl-predints-soko
#| eval: true
#| echo: false
#| layout-ncol: 2
#| tbl-cap: "Prédictions avec intervalles de prédiction (gauche) et intervalles de confiance pour la moyenne (droite)."
data(SKD23_S2A, package = "hecedsm") # load data
lm_simple <- lm(pef ~ proportion, data = SKD23_S2A) # fit simple linear regression
tab1 <- predict(lm_simple,
        newdata = data.frame(proportion = c(0, 0.5, 1, 2)),
        interval = "prediction") # prediction intervals
tab2 <- predict(lm_simple,
        newdata = data.frame(proportion = c(0, 0.5, 1, 2)),
        interval = "confidence") # confidence for mean
knitr::kable(cbind(proportion = c(0, 0.5, 1, 2), tab1), align = c("cccc"),
             col.names = c("`proportion`", "prédiction", "borne inf.","borne sup."),
             booktabs = TRUE, caption = "Intervalles de prédiction")
knitr::kable(tab2,
             col.names = c("moyenne", "borne inf.","borne sup."),
             booktabs = TRUE, align = c("ccc"),
             caption = "Intervalles de confiance pour la moyenne")
```

:::




## Tests d'hypothèses

Les tests d'hypothèses dans les modèles linéaires et d'analyse de la variance suivent la procédure usuelle: nous comparons deux modèles emboîtés, dont l'un (le modèle nul) est une simplification d'un modèle plus complexe (modèle alternatif) obtenu en imposant des restrictions sur les coefficients de la moyenne.


Les tests de restrictions pour les composantes de $\boldsymbol{\beta}$ sont particulièrement intéressants. Les propriétés de l'estimateur du maximum de vraisemblance pour les grands échantillons impliquent que
\begin{align*}
\widehat{\boldsymbol{\beta}} \stackrel{\cdot}{\sim}\mathsf{normale}_{p+1}\left\{\boldsymbol{\beta}, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\right\}
\end{align*}
pour une taille d'échantillon suffisamment grande, et ce résultat est exact si les observations sont normales. On peut aisément obtenir les erreurs-type des coefficients en remplaçant  $\sigma^2$ par un estimé; avec des données normales, on peut montrer que la somme du carré des erreurs $\mathsf{SC}_e \sim \sigma^2\chi^2_{n-p-1}$ et $\mathsf{SC}_e$ est indépendante de $\widehat{\boldsymbol{\beta}}$.


Dans un contexte inférentiel, il est souvent important de tester si l'effet d'une variable explicative est significatif : si $x_j$ est binaire ou continu, le test pour $\mathscr{H}_0 : \beta_j=0$ correspond à un effet marginal nul pour $x_j$. Le modèle nul est une régression linéaire dans laquelle nous supprimons la $(j+1)$ème colonne de $\mathbf{X}$.

:::{#prp-wald}

## Tests de Wald en régression linéaire

Rappelons que la statistique du test de Wald pour l'hypothèse $\mathscr{H}_0: \beta_j=b$ est
$$W = \frac{\widehat{\beta}_j - b}{\mathsf{se}(\widehat{\beta}_j)}.$$
La statistique du test de Wald est rapportée par la plupart des logiciels pour l'hypothèse  $b=0$. Puisque $\mathsf{Var}(\widehat{\beta}_j) = \sigma^2 [(\mathbf{X}^\top\mathbf{X})^{-1}]_{j,j}$, nous pouvons estimer l'erreur type à partir de $S^2$ et en déduire que la distribution de $W$ sous l'hypothèse nulle est  $\mathsf{Student}(n-p-1)$.  Cela explique la terminologie « $t$ values » dans le tableau `summary`. Outre les estimations des coefficients, il est possible d'obtenir des intervalles de confiance basés sur Wald pour $\beta_j$, qui comme à l'accoutumée sont de la forme $\widehat{\beta}_j \pm \mathfrak{t}_{n-p-1,\alpha/2} \mathsf{se}(\widehat{\beta}_j)$, avec $\mathfrak{t}_{n-p-1,\alpha/2}$ le quantile de niveau $1-\alpha/2$ d'une loi $\mathsf{Student}({n-p-1})$.

:::


:::{#exm-sokolova-simple-ttest}

Considérons les données de @exm-sokolova. Si nous ajustons le modèle de régression linéaire simple, nous pouvons extraire les valeurs -$p$ pour les tests de Wald ou tests-$t$. Le test pour l'ordonnée à l'origine est sans intérêt puisque les données sont mesurées sur une échelle de 1 à 7, de sorte que la réponse moyenne lorsque `proportion=0` ne peut être nulle. Le coefficient de `proportion` suggère une tendance de 0.5 point par unité de ratio, et il est significativement différent de zéro, ce qui indique que le score `pef` change avec le ratio carton/plastique.

```{r}
#| eval: true
#| echo: true
# tests-t (Wald) pour beta=0 avec valeurs-p
summary(lm_simple)$coefficients
confint(lm_simple) # intervalles de confiance pour betas
```
:::

Pour les variables catégorielles à plus de deux niveaux, tester si $\beta_j=0$ n'est généralement pas intéressant car le coefficient représente la différence entre la catégorie $x_j$ et la ligne de base avec la paramétrisation du modèle en terme de contrastes (traitements): ces deux catégories peuvent avoir une faible différence, mais la variable catégorielle dans son ensemble peut toujours être un prédicteur utile compte tenu des autres explications. L'hypothèse d'un contraste nul est spécifique car elle implique un modèle nul dans lequel les catégories sélectionnées sont fusionnées, ce qui dépend de la référence. Nous souhaitons plutôt comparer un modèle dans lequel toutes les variables sont présentes avec un modèle dans lequel la variable explicative catégorielle est omise.


:::{#prp-ftest}

## Tests-*F* pour comparaison de modèles emboîtés

Considérons le modèle linéaire *complet* qui contient $p$ variables explicatives,
\begin{align*}
\mathbb{M}_1: Y=\beta_0+\beta_1 x_1 + \cdots + \beta_p x_p + \varepsilon.
\end{align*}
Supposons sans perte de généralité que nous voulions tester $\mathscr{H}_0 : \beta_{k+1}=\beta_{k+2}=\cdots=\beta_p=0$ pour $k < p$ (on pourrait permuter les colonnes de la matrice du modèle pour obtenir cette configuration).
L'hypothèse globale spécifie que $(p-k)$ des paramètres $\beta$ sont nuls. Le *modèle restreint* correspondant à l'hypothèse nulle ne contient que les covariables pour lesquelles $\beta_j \neq 0$,
\begin{align*}
\mathbb{M}_0: Y=\beta_0+\beta_1 x_1 + \cdots + \beta_k x_k + \varepsilon.
\end{align*}
Soit $\mathsf{SC}_e(\mathbb{M}_1)$ la somme du carré des résidus du modèle complet $\mathbb{M}_1$,
\begin{align*}
\mathsf{SC}_e(\mathbb{M}_1)=\sum_{i=1}^n (Y_i-\widehat{Y}_i^{\mathbb{M}_1})^2,
\end{align*}
où $\hat{Y}_i^{\mathbb{M}_1}$ est la  $i$e valeur ajustée du  modèle $\mathbb{M}_1$. On définit de la même façon la somme du carré des résidus,  $\mathsf{SC}_e(\mathbb{M}_0)$, pour le modèle $\mathbb{M}_0$. Logiquement, $\mathsf{SC}_e(\mathbb{M}_0) \geq \mathsf{SC}_e(\mathbb{M}_1)$.


La statistique $F$ est
\begin{align*}
F=\frac{\{\mathsf{SC}_e(\mathbb{M}_0)-\mathsf{SC}_e(\mathbb{M}_1)\}/(p-k)}{\mathsf{SC}_e(\mathbb{M}_1)/(n-p-1)}.
\end{align*}
Sous $\mathscr{H}_0$, la statistique $F$  suit une loi de Fisher  (@def-loiF) avec $(p-k)$ et $(n-p-1)$ degrés de liberté, $\mathsf{Fisher}(p-k, n-p-1)$.
Les degrés de libertés du numérateur, $p-k$, indiquent le nombre de restrictions ou la différence du nombre de paramètres, tandis que celle du dénominateur, $n-p-1$ est la taille de l'échantillons moins le nombre de paramères pour la moyenne du modèle $\mathbb{M}_1$.

:::

Quand la $j$e variable explicative est continue ou binaire, le test $F$ est  équivalent au test $t$ pour $\beta_j=0$. En effet, la statistique $F$ est le carré de la statistique de Wald, et ils mènent à la même inférence --- les valeurs-$p$ sont identiques. Bien qu'il soit rapporté dans les tableaux, le test pour $\beta_0=0$ n'est pas intéressant; nous conservons l'ordonnée à l'origine uniquement pour centrer les résidus.

:::{#rem-lrtvsF}

## Tests *F* versus test du rapport de vraisemblance

Pour la régression linéaire normale, le test du rapport de vraisemblance pour comparer les modèles $\mathbb{M}_1$ et $\mathbb{M}_0$ est une fonction de la somme des carrés des résidus: la formule habituelle se simplifie à
\begin{align*}
R &= 2( \ell_{\mathbb{M}_1} - \ell_{\mathbb{M}_0}) \\&= n\ln\{\mathsf{SC}_e(\mathbb{M}_0)/\mathsf{SC}_e(\mathbb{M}_1)\}\\
&= n \ln \left( 1+ \frac{p-k}{n-p-1}F\right)
\end{align*}
Le test du rapport de vraisemblance et les tests $F$ sont liés par une transformation monotone, et nous pouvons utiliser la distribution $\mathsf{Fisher}$ à des fins de comparaison, plutôt que l'approximation $\chi^2$ pour grand échantillon. Les tests $t$ et $F$ présentés ci-dessus pourraient donc tous deux être considérés comme des cas particuliers de [tests de rapport de vraisemblance](@sec-testsvrais), mais en utilisant Student-$t$ contre la distribution normale lorsque $p-k=1$, et $\mathsf{Fisher}$ contre $\chi^2$ lorsque $p-k \ge 1$. Lorsque $n$ est grand, les résultats sont à peu près les mêmes.

:::

<!-- ### Analysis of variance tables -->

<!-- In **R**, the generic `anova` can be used to compare two nested models by passing as arguments the simpler model and the complete model (in this order). Alternatively, we can call `anova` on the output of a linear regression model to produce an **analysis of variance** table, in which we compare the sum of square of the model with all variables, and the difference in sum of square when adding terms sequentially in the order in which they appear in the formula. The latter is typically not of interest, and we must be careful in models with interactions. -->

### Contrastes


Supposons que nous effectuions une analyse de la variance et que le test $F$ pour l'hypothèse nulle (globale) selon laquelle les moyennes de tous les groupes sont égales soit très élevé: nous rejetons l'hypothèse nulle en faveur de l'alternative, qui stipule qu'au moins une des moyennes du groupe est différente. La question suivante sera de savoir où se situent ces différences. En effet, dans un contexte expérimental, cela implique qu'une ou plusieurs manipulations ont un effet différent des autres sur la réponse moyenne. Souvent, cela n'est pas intéressant en soi: nous pourrions être intéressés par la comparaison de différentes options par rapport à un groupe de contrôle ou déterminer si des combinaisons spécifiques fonctionnent mieux que séparément, ou trouver le meilleur traitement en comparant toutes les paires.


La question scientifique qui a justifié l'expérience peut conduire à un ensemble spécifique d'hypothèses, qui peuvent être formulées par les chercheurs comme des comparaisons entre les moyennes de différents sous-groupes. Nous pouvons normalement les exprimer sous la forme de **contrastes**. Si le test global $F$ pour l'égalité des moyennes est équivalent à une pièce faiblement éclairée, les contrastes sont comparables à des projecteurs qui permettent de mettre l'accent sur des aspects particuliers des différences entre les traitements. Formellement, un contraste est une combinaison linéaire de moyennes: en clair, cela signifie que nous attribuons un poids à chaque moyenne de groupe et que nous les additionnons, puis que nous comparons ce résumé à une valeur postulée $a$, généralement zéro.


Les contrastes encodent la question de recherche : si $c_i$ représente le poids de la moyenne du groupe $\mu_i$ $(i=1, \ldots, K)$, alors nous pouvons écrire le contraste comme $C = c_1 \mu_1 + \cdots + c_K \mu_K$ avec l'hypothèse nulle $\mathscr{H}_0 : C=a$ pour une alternative bilatérale. L'estimation du contraste linéaire est obtenue en remplaçant la moyenne inconnue de la population $\mu_i$ par la moyenne de l'échantillon de ce groupe, $\widehat{\mu}_i = \overline{y}_{i}$. Nous pouvons facilement obtenir l'erreur type de la combinaison linéaire $C$. La formule, l'erreur type, en supposant une taille de sous-échantillon de $n_1, \ldots, n_K$ et une variance commune $\sigma^2$, est la racine carrée de
\begin{align*}
\mathsf{Va}(\widehat{C}) = \widehat{\sigma}^2\left(\frac{c_1^2}{n_1} + \cdots + \frac{c_K^2}{n_K}\right).
\end{align*}
Nous pouvons alors construire une statistique $t$ comme d'habitude en examinant la différence entre notre valeur postulée et la moyenne pondérée observée, convenablement normalisée. Si le test global $F$ conduit au rejet de la valeur nulle, il existe au moins un contraste significatif au même niveau.
Lorsque les vecteurs de contraste sont orthogonaux, les tests ne sont pas corrélés. Mathématiquement, si nous laissons $c_{i}$ et $c^{*}_{i}$ désigner les poids attachés à la moyenne du groupe $i$ comprenant $n_i$ observations, les contrastes sont orthogonaux si $c_{1}c^{*}_{1}/n_1 + \cdots + c_{K}c^{*}_K/n_K = 0$ ; si l'échantillon est équilibré avec le même nombre d'observations dans chaque groupe, $n/K = n_1 =\cdots = n_K$, nous pouvons considérer le produit scalaire des deux vecteurs de contrastes et négliger la taille des sous-échantillons.

Si nous avons $K$ groupes, il y a $K-1$ contrastes pour les différences deux à deux, le dernier étant capturé par la moyenne de l'échantillon pour l'effet global^[La contrainte $c_1 + \cdots + c_K=0$ garantit que les contrastes linéaires sont orthogonaux à la moyenne, qui a un poids $c_i=n_i/n$ et pour les échantillons équilibrés $c_i =1/n$.]. Si nous nous intéressons uniquement à la différence entre groupes (par opposition à l'effet global de tous les traitements), nous imposons une contrainte de somme à zéro sur les poids, de sorte que $c_1 + \cdots + c_K=0$.

### Exemples de tests


:::{#exm-moonvaepps-test}

## Test du montant des dons

Considérons l'@exm-moon, dans lequel nous testons les différences entre les montants libres (`open-ended`) et les montants suggérés (`quantity`). Le test qui nous intéresse est $\mathscr{H}_0 : \beta_1=0$, où $\beta_1=\mu_{\texttt{oe}} - \mu_{\texttt{qty}}$ est la différence moyenne entre les groupes. Outre le fait que la différence est statistiquement significative au niveau de 5 %, nous voulons également rapporter les **moyennes marginales**, qui, lorsque nous avons une seule variable explicative catégorielle dans le modèle linéaire, est la moyenne empirique de chaque sous-groupe.

```{r}
#| eval: true
#| echo: true
data("MV23_S1", package = "hecedsm")
MV23_S1 <- MV23_S1 |>
    dplyr::mutate(amount2 = ifelse(is.na(amount), 0, amount))
linmod_MV23 <- lm(amount2 ~ condition, data = MV23_S1)
# Test Wald avec coefficients
summary(linmod_MV23)
# ANOVA avec tests F
anova(linmod_MV23)
# Moyennes marginales
(emm <- emmeans::emmeans(linmod_MV23, spec = "condition"))
emm |> emmeans::contrast(method = "pairwise") # vecteur de contraste (1,-1)
```

:::

::: {#exm-teachingtoread}


## Tests et contrastes pour les méthodes de compréhension de la lecture

Nous examinons maintenant les tests pour l'@exm-teaching-baumann et l'@exm-baumann-dummies, avec une covariable en plus. L'objectif de @Baumann:1992 était de faire une comparaison particulière entre des groupes de traitement. Selon le résumé de l'article:

> Les analyses quantitatives principales comportaient deux contrastes orthogonaux planifiés: l'effet de l'enseignement (TA + DRTA vs. 2 x DR) et l'intensité de l'enseignement (TA vs. DRTA).

Avec un modèle pré-post, nous allons comparer les moyennes pour une valeur commune de `pretest1`, ci-dessous la moyenne globale du score `pretest1`.

```{r}
#| label: pairwise-baumann
#| echo: true
#| eval: true
library(emmeans) # moyennes marginales
data(BSJ92, package = "hecedsm")
mod_post <- lm(posttest1 ~ group + pretest1,
               data = BSJ92)
mod_post0 <- lm(posttest1 ~ pretest1,
               data = BSJ92)
anova(mod_post0, mod_post) # tests F
emmeans_post <- emmeans(object = mod_post,
                        specs = "group")
```



Le résultat du tableau d'analyse de la variance montre qu'il y a bien des différences entre les groupes. On peut donc s'intéresser aux moyennes marginales estimées, qui sont la moyenne de chaque groupe.

```{r}
#| label: tbl-print-pairwise-baumann
#| echo: false
#| eval: true
#| tbl-cap: "Moyennes estimées des groupes avec erreurs-types et intervalles de confiance à 95 % pour le post-test 1 pour un score moyen au pré-test 1."
mod_post <- lm(posttest1 ~ group + pretest1, data = BSJ92)
emmeans_post <- emmeans(object = mod_post,
                        specs = "group") # which variable to keep
# the `pretest1` covariate is fixed to it's overall mean
knitr::kable(emmeans_post,
      digits = c(2,2,2, 0,2,2),
      booktabs = TRUE,
      col.names = c("termes",
                    "moyennes",
                    "erreur-type",
                     "ddl",
                    "borne inf.",
                    "borne sup."))
```


Les deux hypothèses et contrastes de @Baumann:1992 sont $\mathscr{H}_0: \mu_{\mathrm{TA}} + \mu_{\mathrm{DRTA}} = 2 \mu_{\mathrm{DRA}}$
ou
\begin{align*}
\mathscr{H}_0: - 2 \mu_{\mathrm{DR}} + \mu_{\mathrm{DRTA}} + \mu_{\mathrm{TA}} = 0.
\end{align*}
avec poids $c_1=(-2, 1, 1)$; l'ordre des niveaux de traitement est
($\mathrm{DRA}$, $\mathrm{DRTA}$, $\mathrm{TA}$) et ce dernier doit correspond à celui des poids pour les contrastes. Ces derniers donnent les mêmes tests à multiple non-nul près, donc $ac_1$, $a \neq 0$ donne un résultat équivalent, par exemple $(2, -1, -1)$ ou $(1, -1/2, -1/2)$ fonctionnent. Si les estimations changent, les erreurs-types sont ajustées d'autant. Un vecteur de contrastes pour $\mathscr{H}_0:  \mu_{\mathrm{TA}} = \mu_{\mathrm{DRTA}}$
est ($0$, $-1$, $1$): le zéro apparaît parce que la première composante, $\mathrm{DRA}$ n'apparaît pas. Les deux contrastes sont orthogonaux puisque
$(-2 \times 0) + (1 \times -1) + (1 \times 1) = 0$.

```{r}
#| label: contrasts
#| echo: true
#| eval: true
# Identifier l'ordre de niveau du facteur
with(BSJ92, levels(group))
# DR, DRTA, TA (alphabetical)
contrastes_list <- list(
  # Contrastes: combo linéaire de moyennes,
  # la somme des coefficients doit être nulle
  "C1: moy(DRTA+TA) vs DR" = c(-1, 0.5, 0.5),
  "C2: DRTA vs TA" = c(0, 1, -1)
)
contrastes_post <-
  contrast(object = emmeans_post,
           method = contrastes_list)
contrastes_summary_post <- summary(contrastes_post)
```

```{r}
#| label: tbl-print-contrasts
#| echo: false
#| eval: true
#| tbl-cap: "Contrastes estimés pour le post-test 1."
kable(contrastes_post,
      booktabs = TRUE,
      digits = c(2,2,2,0,2,2),
      col.names = c("contraste",
                    "estimation",
                    "erreur-type",
                    "ddl",
                    "stat",
                    "valeur-p")) |>
  kableExtra::kable_styling()
```

Nous pouvons examiner ces différences: puisque `DRTA` contre `TA` est une différence par paire, nous aurions pu obtenir la statistique $t$ directement à partir des contrastes deux à deux en utilisant `pairs(emmeans_post)`.

Quelle est la conclusion de notre analyse des contrastes? Il semble que les méthodes impliquant la réflexion à haute voix aient un impact important sur la compréhension de la lecture par rapport à la seule lecture dirigée. Les preuves ne sont pas aussi solides lorsque nous comparons la méthode qui combine la lecture dirigée, l'activité de réflexion et la réflexion à haute voix, mais la différence est néanmoins significative à niveau 5%.


```{r}
#| echo: true
#| eval: true
# Extraire les coefficients et les erreurs-type
beta_pre <- coefficients(mod_post)['pretest1']
se_pre <- sqrt(c(vcov(mod_post)['pretest1', 'pretest1']))
wald <- (beta_pre - 1)/se_pre # test de Wald directionnel
# Valeur-p basée sur la référence nulle Student-t avec n-p-1 ddl
pval <- 2*pt(abs(wald), df = mod_post$df.residual, lower.tail = FALSE)
# Comparaison de modèles emboîtés avec appel à 'anova'
mod0 <- lm(posttest1 ~ offset(pretest1) + group, data = BSJ92)
# Le décalage (`offset`) fixe le terme, ce qui équivaut à un coefficient de 1.
aov_tab <- anova(mod0, mod_post)
```

Une autre hypothèse potentielle intéressante consiste à tester si le coefficient de `pretest1` est égal à l'unité. Cela équivaut à l'hypothèse $b=1$ pour le test de Wald,  $w = (\widehat{\beta}_{\texttt{pretest1}}-1)/\mathsf{se}(\widehat{\beta}_{\texttt{pretest1}})= `r wald`$, ou bien une comparaison de modèles avec le test $F$ via `anova`, qui donne une statistique de test de $F=`r aov_tab$F[2]`.$ On peut montrer que si $Z \sim \mathsf{Student}(\nu)$, alors $Z^2 \sim \mathsf{Fisher}(1, \nu)$, il s'ensuit que les deux tests sont équivalents et que les valeurs-$p$ sont exactement les mêmes.

:::


::: {#exm-paperorplastic}

## Tests et contrastes pour l'effet de l'emballage carton sur la perception

 Soit $\mu_{0}, \mu_{0.5}, \mu_{1}, \mu_2$ la vraie moyenne du score PEF en fonction de la proportion de carton pour les données de @exm-sokolova. Plusieurs tests pourraient être intéressants ici, mais nous nous concentrons sur les contrastes effectués par les auteurs et sur un test d'hypothèse de linéarité en fonction de la proportion de plastique. Pour ce dernier, nous pouvons comparer le modèle de régression linéaire (dans lequel le score PEF augmente linéairement avec la proportion de carton par rapport au plastique),
  \begin{align*}
 \mathsf{E}(\texttt{pef} \mid \texttt{proportion}) = \beta_0 + \beta_1\texttt{proportion},
 \end{align*}
 au modèle d'analyse de variance qui permet à chacun des quatre groupes d'avoir des moyennes différentes.
\begin{align*}
 &\mathsf{E}(\texttt{pef} \mid \texttt{proportion}) = \alpha_0 + \alpha_1 \mathbf{1}_{\texttt{proportion}=0.5} \\&\quad + \alpha_2 \mathbf{1}_{\texttt{proportion}=1} + \alpha_3\mathbf{1}_{\texttt{proportion}=2}.
\end{align*}
Si on veut obtenir l'hypothèse nulle en terme de contraintes sur les paramètres $\boldsymbol{\alpha}$, on trouve
\begin{align*}
\mu_0 &= \beta_0=\alpha_0 \\
\mu_{0.5} &= \beta_0 + 0.5 \beta_1 = \alpha_0 + \alpha_1\\
\mu_1 &= \beta_0 + \beta_1 = \alpha_0 + \alpha_2 \\
\mu_2 &= \beta_0 + 2 \beta_1= \alpha_0 + \alpha_3.
\end{align*}
Le test comparant la régression linéaire simple à l'analyse de la variance impose deux restrictions simultanées, avec $\mathscr{H}_0 : \alpha_3 = 2\alpha_2= 4\alpha_1$, de sorte que la distribution nulle est $\mathsf{Fisher}(2, 798)$ ou approximativement $\chi^2_2$.


```{r}
#| eval: true
#| echo: true
data(SKD23_S2A, package = "hecedsm")
linmod <- lm(pef ~ proportion, data = SKD23_S2A)
coef(linmod) # extraire coefficients
# ANOVA à un facteur
anovamod <- lm(pef ~ factor(proportion),
               data = SKD23_S2A)
# Comparer les deux modèles emboîtés
anova(linmod, anovamod) # est-ce que l'effet est linéaire?
# Test avec code alternatif (poids pour chaque coefficient)
car::linearHypothesis(model = anovamod,
   hypothesis = rbind(c(0, -2, 1, 0),
                      c(0, 0, -2, 1)))
```
Le résultat montre que les tests $F$ et les valeurs-$p$ sont identiques, que l'on impose les contraintes manuellement ou que l'on soumette simplement les deux modèles imbriqués à la méthode `anova`.

Les auteurs souhaitaient comparer zéro carton avec d'autres choix: nous nous intéressons aux différences par paire, mais uniquement par rapport à la référence $\mu_{0}$:
\begin{align*}
\mu_0 = \mu_{0.5}  & \iff 1\mu_0 - 1\mu_{0.5} + 0\mu_{1} + 0 \mu_{2} = 0\\
\mu_0 = \mu_{1} & \iff 1\mu_0 + 0\mu_{0.5} -1\mu_{1} + 0 \mu_{2} = 0\\
\mu_0 = \mu_{2} & \iff 1\mu_0 + 0\mu_{0.5} + 0\mu_{1} -1 \mu_{2} = 0.
\end{align*}
Les vecteurs de poids pour les contrastes linéaires sont $(1, -1, 0, 0)$, $(1, 0, -1, 0)$ et $(1, 0, 0, -1)$ pour les moyennes marginales.

```{r}
#| eval: true
#| echo: true
moymarg <- anovamod |>
  emmeans::emmeans(specs = "proportion") # moyennes de groupes
contrastlist <- list( # liste de vecteurs de contrastes
   refvsdemi = c(1, -1, 0, 0),
   refvsun =  c(1, 0, -1, 0),
   refvsdeux =  c(1, 0, 0, -1))
# calculer différences relativement à la référence
moymarg |> emmeans::contrast(method = contrastlist)
```



```{r}
#| label: SKD23S2A-save
#| eval: true
#| echo: false
data(SKD23_S2A, package = "hecedsm") # load data
linmod <- lm(pef ~ proportion, data = SKD23_S2A) # fit simple linear regression
anovamod <- lm(pef ~ factor(proportion), data = SKD23_S2A) # one-way ANOVA
# anova(linmod, anovamod) # compare models (two restrictions)
margmean <- anovamod |>  emmeans::emmeans(specs = "proportion") # group means
contrastlist <- list( # specify contrast vectors
   refvshalf = c(1, -1, 0, 0),
   refvsone =  c(1, 0, -1, 0),
   refvstwo =  c(1, 0, 0, -1))
contrast <- margmean |> emmeans::contrast( # compute contrasts relative to reference
 method = contrastlist)
```

Les moyennes des groupes rapportées dans le @tbl-print-groupmeans-PEF correspondent à celles indiquées par les auteurs dans l'article. Elles suggèrent que la perception du respect de l'environnement augmente avec la quantité de carton utilisée dans l'emballage. Nous avons pu ajuster un modèle de régression simple pour évaluer le changement moyen, en traitant la proportion comme une variable explicative continue. La pente estimée pour le changement du score PEF, qui va de 1 à 7 par incréments de 0.25, est `r round(coef(linmod)[2],2)` point par rapport au carton/plastique. Il y a cependant de fortes indications, compte tenu des données, que le changement n'est pas tout à fait linéaire, puisque l'ajustement du modèle de régression linéaire est significativement plus mauvais que le modèle linéaire correspondant.


```{r}
#| label: tbl-print-groupmeans-PEF
#| echo: false
#| eval: true
#| tbl-cap: "Moyennes estimées du DEP par proportion pour les groupes, avec erreurs-types"
knitr::kable(margmean,
      digits = c(2,2,3,0,2,4),
      booktabs = TRUE,
      col.names = c("proportion",
                    "moyenne",
                    "erreur-type",
                    "ddl",
                    "borne inf.",
                    "borne sup.")) |>
    kableExtra::kable_styling()
```

```{r}
#| label: tbl-print-contrast-PEF
#| echo: false
#| eval: true
#| tbl-cap: "Estimations des contrastes pour les différences de PEF relativement à plastique seulement."
kable(contrast,
      booktabs = TRUE,
      digits = c(2,2,2,0,2,2),
      col.names = c("contraste",
                    "estimation",
                    "erreur-type",
                    "ddl",
                    "stat",
                    "valeur-p")) |>
  kableExtra::kable_styling()
```

Toutes les différences dans le @tbl-print-contrast-PEF sont significatives et positives, conformément à l'hypothèse des chercheurs.

:::


:::{#exm-tests-college}

## Tester la discrimination salariale dans l'enseignement supérieur

Considérons l'exemple des données `college` et le modèle linéaire associé avec `echelon`, `sexe`, années de `service` et `domaine` comme variables explicatives.

```{r}
#| eval: true
#| echo: true
data(college, package = "hecmodstat")
mod1_college <- lm(salaire ~ sexe + domaine + echelon + service, data = college)
mod0_college <- lm(salaire ~ domaine + echelon + service, data = college)
# F-test avec "anova" comparant les modèles emboîtés
aov_tab_college <- anova(mod0_college, mod1_college)
# Test t de Wald
wald_college <- summary(mod1_college)$coefficients[2,]
# Test du rapport de vraisemblance avec approx khi-deux
pval_lrt <- pchisq(q = as.numeric(2*(logLik(mod1_college) - logLik(mod0_college))),
       df = 1, lower.tail = FALSE)
```

Le seul test qui nous intéresse ici est $\mathscr{H}_0 : \beta_{\texttt{sexe}} = 0$ contre l'alternative bilatérale $\mathscr{H}_a : \beta_{\texttt{sexe}} \neq 0$. La statistique du test de Wald est $`r wald_college[3]`$, avec une valeur-$p$ de $`r wald_college[4]`$ basée sur une distribution Student-$t$ avec $`r mod1_college$df.residual`$ degrés de liberté. La valeur-$p$ dans la sortie du test $F$ est la même, et celle obtenue par le test du rapport de vraisemblance est la même jusqu'à la deuxième décimale.

```{r }
#| label: summarytestslmcollege
#| echo: false
options(knitr.kable.NA = '')
library(pixiedust)
names(college_lm$coefficients)[2:5] <- c("sexe [femme]","domaine [théorique]","échelon [agrégé]", "échelon [titulaire]")
dust(college_lm) %>%
  sprinkle(col = 2:3, round = 3) %>%
  sprinkle(col = 4, round = 2) %>%
  sprinkle(col = 5, fn = quote(pvalString(value))) %>%
  sprinkle_colnames(estimate = "estimation",
                     std.error = "erreur-type",
                     statistic = "stat de Wald",
                     p.value = "valeur-p") %>%
  sprinkle_bookdown() %>%
  knitr::kable(align = "lrrrr", booktabs = TRUE, caption = "Tableau des estimations des coefficients de régression linéaire avec les erreurs-type associées, les tests de Wald et les valeurs $p$ basées sur la distribution Student-$t$.")
```

:::


## Plans factoriels et interactions


Le modèle additif pour la moyenne spécifie que l'effet marginal d'une variable ( y compris pour les variables catégorielles) est indépendant des autres. Nous pouvons souhaiter assouplir cette hypothèse en incluant des **termes d'interaction**. 


:::{#def-interaction}

## Interaction

On parle d'**interaction** lorsque des combinaisons de variables explicatives affectent la variable réponse différemment que lorsqu'elles sont considérées individuellement. Si $X_j$ et $X_k$ interagissent, l'effet marginal de $\mathsf{E}(Y \mid \boldsymbol{X})$ par rapport à $X_j$ est une fonction de $X_k$, et vice-versa.



:::

On s'attarde au cas où au moins une des variables est catégorielle (facteur).

:::{#exm-assurance-interaction}

## Primes d'assurances et interactions

On considère la relation entre `fumeur` et l'indice de masse corporel pour la détermination de primes d'assurance.
Les fumeurs dont l'indice de masse corporelle (IMC) est égal ou supérieur à 30 paient une prime élevée, mais il semble que le montant de la prime augmente de façon linéaire en fonction de l'IMC. Cette tarification ne semble pas s'appliquer aux non-fumeurs. 


```{r}
#| label: fig-insuranceinter1
#| echo: false
#| fig-align: 'center'
#| fig-cap: "Nuage de points des données `assurance` avec les frais en fonction de l'`imc`, selon le status `fumeur`."
data(assurance, package = "hecmodstat")
assurance <- assurance |>
  dplyr::mutate(obesite = factor(imc >= 30, labels = c("normal","obese"))) |>
  dplyr::mutate(fumobese = droplevels(factor(interaction(obesite, fumeur),
                          levels = c("normal.non","obese.non","normal.oui","obese.oui"),
                          labels = c("non-fumeur","non-fumeur","fumeur non-obèse","fumeur obèse"))))

ggplot(data = assurance,
             aes(x = imc, y = frais, col = fumobese)) +
  geom_point() +
  geom_vline(xintercept = 30) +
  MetBrewer::scale_color_met_d("Hiroshige") +
  labs(x = "indice de masse corporelle",
       y = "", subtitle = "frais médicaux d'assurance (en dollars américains)", col = "") +
  scale_y_continuous(labels = scales::unit_format(unit = "K", scale = 1e-3),
                     expand = c(0, 0), limits = c(0, max(assurance$frais)+100)) 
```


:::


:::{#exm-intention}

## Intention d'achat


On considère un exemple avec des données bidons `interaction`. Le modèle additif (sans interaction) a pour  moyenne
\begin{align*}
\mathsf{E}(\texttt{intention} \mid \cdot)=\beta_0 + \beta_1 \texttt{sexe} + \beta_2 \texttt{fixation},
\end{align*}
où $\texttt{sexe=1}$ pour les femmes et $\texttt{sexe=0}$ pour les hommes

L'effet de la variable continue `fixation` est identique pour les deux sexes. De même, l'effet de la variable binaire est supposé être le même pour toutes les valeurs possibles de la variable continue. Nous pouvons le voir sur le graphique, car la différence entre les lignes représente l'effet de $\texttt{sexe}$, est le même pour toutes les valeurs de $\texttt{fixation}$; les lignes sont *parallèles* : voir le panneau gauche de @fig-interaction-slope.



```{r}
#| eval: true
#| echo: false
#| label: fig-interaction-slope
#| fig-cap: "Nuages de points et droites ajustées pour un modèle sans interaction (gauche) et avec interaction (droite)." 
data(interaction, package = "hecmodstat")
interaction <- interaction |> 
  dplyr::mutate(sexe = factor(sexe, levels = c(0,1),
                             labels = c("homme","femme")))
mod <- lm(intention ~ fixation + sexe, data = interaction)
predmod <- predict(mod)
g1 <- ggplot(data = interaction,   
       mapping = aes(
  x = fixation, 
  y = intention,
  color = sexe)) +
 geom_point() +
 geom_line(aes(y = predmod), linewidth = 1,
           show.legend = FALSE) +
 MetBrewer::scale_color_met_d(name = "Hiroshige") + 
 labs(color = "sexe",
     x = "temps de fixation (en secondes)", 
     y = "intention d'achat")
g2 <- ggplot(data = interaction,   
       mapping = aes(x = fixation, 
                     color = sexe, 
                     y = intention)) +
 geom_point() +
 geom_smooth(formula = y ~ x, se = FALSE, method = "lm", linewidth = 1,
             show.legend = FALSE) +
 MetBrewer::scale_color_met_d(name = "Hiroshige") + 
 labs(color = "sexe",
     x = "temps de fixation (en secondes)", 
     y = "intention d'achat")
g1 + g2 + plot_layout(guides = 'collect') & theme(legend.position = "bottom") 
```


Pour ajuster une pente différente par sexe, on crée une nouvelle variable égale au produit $\texttt{fixation}\times\texttt{sexe}$ et on l'ajoute à notre modèle,
\begin{align*}
\mathsf{E}(\texttt{intention} \mid \cdot)= \beta_0 + \beta_1 \texttt{sexe} + \beta_2\texttt{fixation}  + \beta_3 \texttt{fixation}\cdot \texttt{sexe}.
\end{align*} 

Selon la valeur de $\texttt{sexe}$, on obtient
\begin{align*}
\mathsf{E}(\texttt{intention} \mid \cdot) = 
\begin{cases}
(\beta_0 + \beta_1) + (\beta_2 + \beta_3)\texttt{fixation}, & \texttt{sexe}=1 \text{ (femme)},\\
  \beta_0 + \beta_2 \texttt{fixation}, & \texttt{sexe}=0 \text{ (homme)}.
\end{cases}
\end{align*} 
L'interprétation des coefficients du modèle se fait comme d'habitude avec la paramétrisation (traitement):

- $\beta_0$ est l'intention d'achat moyenne lorsque le temps de fixation est nul pour les hommes,
- $\beta_1$ est la différence d'ordonnée à l'origine entre les femmes et les hommes (différence d'intention d'achat moyenne entre femmes et hommes quand le temps de fixation est nul),
- $\beta_2$ est l'augmentation unitaire de l'intention d'achat par seconde de fixation pour les hommes,
- $\beta_3$ est la différence de pente entre les femmes et les hommes (différence d'intention d'achat moyenne femmes vs hommes pour une augmentation d'une seconde de fixation).


Tester la significativité de l'interaction revient à vérifier si $\mathscr{H}_0: \beta_3=0$.


```{r}
#| eval: true
#| echo: true
data(interaction, package = "hecmodstat")
# Pour spécifier une interaction, utiliser :
mod <- lm(intention ~ sexe + fixation +  sexe:fixation, 
          data = interaction)
# Un raccourci est sexe*fixation, qui donne la même chose
summary(mod)$coefficients
```
Le modèle avec interaction est significativement meilleur, ce qui signifie que l'effet du temps de fixation sur l'intention d'achat varie en fonction du sexe.

:::


:::{#rem-marginalite}

## Principe de marginalité


Tous les termes d'ordre inférieurs devraient être inclus si l'interaction est présente.

Par exemple, on ne retirera pas $\texttt{fixation}$ tout en conservant le terme d'interaction $\texttt{fixation*sexe}$, même si on ne rejette pas $\mathscr{H}_0:\beta_2=0$, puisqu'autrement
\begin{align*}
&\mathsf{E}(\texttt{intention} \mid \cdot) =
\begin{cases}
(\beta_0 + \beta_1) + \beta_3\texttt{fixation}, & \texttt{sexe}=1 \text{ (femme)},\\
  \beta_0, &\texttt{sexe}=0 \text{ (homme)};                 
\end{cases}
\end{align*} 
cela implique que l'intention d'achat est constante pour les hommes, quel que soit le temps de fixation.

Comme le choix de catégorie de référence est arbitraire, changer la variable indicatrice \texttt{sexe} pour $\texttt{0}$ pour les femmes, $\texttt{1}$ pour les hommes, donnerait un autre modèle et potentiellement des inférences différentes. De ce fait, on ne considère jamais le retrait d'un effet principal si la variable est incluse dans une interaction. Le principe de **marginalité** suppose que tous les termes d'ordre inférieurs devraient être inclus.

:::


<!--

:::{#exm-visualising-interaction}

## Insurance data and complex interactions

A good example of interaction is provided by the `insurance` dataset. An exploratory data analysis suggested that premiums depended on age, smoker status and body mass index (BMI). @fig-insuranceinter1   shows that the insurance premium depends on smoking: smokers who have a BMI of 30 and above pay a hefty premium, but there is also seemingly a linear increase in the amount charged with BMI. We see no such behaviour for non-smokers. 

```{r}
#| label: fig-insuranceinter1
#| echo: false
#| fig-cap: "Graph of insurance charges against body mass index, colored by smoking status."
data(insurance, package = "hecstatmod")
insurance <- insurance |>
  mutate(obesity = factor(bmi >= 30, labels = c("normal","obese"))) |>
  mutate(smobese = droplevels(factor(interaction(obesity, smoker),
                          levels = c("normal.no","obese.no","normal.yes","obese.yes"),
                          labels = c("non-smoker","non-smoker","smoker non-obese","smoker obese"))))

ggplot(data = insurance,
             aes(x = bmi, y = charges, col = smobese)) +
  geom_point() +
  geom_vline(xintercept = 30) +
  MetBrewer::scale_color_met_d("Hiroshige") +
  labs(color = "",
       x = "body mass index",
       y = "charges (in USD)") +
  scale_y_continuous(expand = c(0, 0), limits = c(0, max(insurance$charges)+100))
```



From there, we could create an indicator variable $\texttt{obese}=\mathsf{1}_{\texttt{bmi} \geq 30}$ and add an interaction term between smoker/obese (categorical) and age (continuous) in our mean model. We take non-smoker as baseline category. To make interpretation more meaningful, we rescale age so that $\texttt{age}=0$ corresponds to 18 years old.


```{r}
#| label: coefintercharges
#| echo: false
lm_interaction <- lm(charges ~ I(age-18)*obesity*smoker, data = insurance)

names(lm_interaction$coefficients)[2:8] <- c("age","obesity [obese]","smoker [yes]","age * obesity [obese]", "age * smoker [yes]","obesity [obese] * smoker [yes]","age * obesity [obese] * smoker [yes]")
dust(lm_interaction) %>%
  sprinkle(col = 2:3, round = 1) %>%
  sprinkle(col = 4, round = 2) %>%
  sprinkle(col = 5, fn = quote(pvalString(value))) %>%
  sprinkle_colnames(estimate = "estimate",
                     std.error = "std. error",
                     statistic = "Wald stat.",
                     p.value = "p-value") %>%
  sprinkle_bookdown() %>%
  knitr::kable(align = "lrrrr", booktabs = TRUE, caption = "Table of regression coefficients for the insurance data with interaction terms between age, obesity and smoker status.")
```

The linear regression model has eight parameters, which could be mapped to four intercepts and four different slopes for `age`; however, the model is parametrized in terms of contrasts, which facilitates testing restrictions.
```{r}
#| label: equation
#| echo: false
#| eval: false
# equatiomatic::extract_eq(lm_interaction,
#                          intercept = "beta",
#                          wrap = TRUE,
#                          align_env = "align*")
```

\begin{align*}
\texttt{charges} &= \beta_{0} + \beta_{1}\texttt{age} + \beta_{2}\texttt{obese} + \beta_{3}\texttt{smoker} +
\beta_{4}\texttt{age} \cdot \texttt{obese} \\&\quad + \beta_{5}\texttt{age} \cdot \texttt{smoker} + \beta_{6}\texttt{obese} \cdot \texttt{smoker} + \beta_{7}\texttt{age} \cdot \texttt{obese} \cdot \texttt{smoker} + \varepsilon
\end{align*}

Because of the three-way interaction, it is not possible to recover individual parameters by changing the value of the corresponding covariate and keeping everything else constant: changing the smoker status likely impacts multiple regressors simultaneously. To retrieve the interpretation of the different coefficients, we will need to change one parameter at the time, write the mean equation and then isolate the coefficients. Throughout, $\texttt{obese}$ is a dummy variable equal to one if the person has a body mass index greater than 30 and likewise $\texttt{smoker}$ if the person is a smoker.

\begin{align*}
\texttt{charges}  =
\begin{cases}
\beta_{0} + \beta_{1}\texttt{age}  & (\mathrm{g}_1)\, \texttt{non-obese}, \texttt{non-smoker} \\
(\beta_{0} + \beta_{2}) + (\beta_{1} + \beta_{4})\texttt{age},  & (\mathrm{g}_2)\,  \texttt{obese}, \texttt{non-smoker} \\
(\beta_{0} + \beta_{3}) + (\beta_{1} + \beta_{5})\texttt{age},  & (\mathrm{g}_3)\,  \texttt{non-obese}, \texttt{smoker} \\
(\beta_{0} + \beta_{2} + \beta_{3}+ \beta_{6}) + (\beta_{1} + \beta_{4} +\beta_{5} + \beta_7)\texttt{age},  & (\mathrm{g}_4)\,  \texttt{obese}, \texttt{smoker}
\end{cases}
\end{align*}

- The intercept $\beta_0$ is the average at 18 years old of non-smokers who are not obese.
- The slope $\beta_1$ is the average annual increase in charges for non-smokers who are not obese.
- The parameter $\beta_2$ is a contrast, the difference between the average charges of 18 years old non-smokers who are obese and those who are not.
- The parameter $\beta_3$ is a contrast, the difference between the average premium for non-obese 18 years old who smoke and those who don't.
- The parameter $\beta_4$ is a contrast, the difference in average annual increase for non-smokers between obese and non-obese adults.
- The parameter $\beta_5$ is a contrast, the difference in average annual increase for non-obese between smoker and non-smoker adults.

The other two coefficients, $\beta_6$ and $\beta_7$ represent differences of average between groups $\mathrm{g}_1 + \mathrm{g}_4 - \mathrm{g}_2 - \mathrm{g}_3$ for both intercepts and slopes.

The only $F$-test that we should consider in the analysis of variance table is the test for the three-way interaction, which corresponds to $\mathscr{H}_0: \beta_7=0$. The $p$-value against the two-sided alternative is $0.6704$, suggesting no difference in slope. The reason why we do not consider the other tests is that they correspond to irrelevant hypotheses. For example, the test for the two-way interaction term $\mathscr{H}_0: \beta_4=0$ associated to $\texttt{age} \cdot \texttt{obese}$ would correspond to merging the intercepts of group $\mathrm{g}_1$ and $\mathrm{g}_2$. Changing the baseline category would imply that a different difference in intercept is forced to be zero.


Sometimes, however, specific hypothesis could be of interest because of the problem setting. We could perform bespoke test to check here that $\mathscr{H}_0: \beta_2=\beta_4=0$, which consists in merging the two non-smoker categories, or even $\mathscr{H}_0: \beta_2=\beta_4= \beta_5=\beta_7=0$, which amounts to merging non-smokers and the imposing a common slope for $\texttt{age}$. Such tests are not directly available in the output, but we can implement them manually by fitting two models and then plug-in in the values of residual sum of squares of both model in the formula of the $F$ statistic. Using the $F$ null distribution, we get a $p$-value of `r pf((sum(resid(lm(charges ~ smobese + age, data = insurance))^2)- sum(resid(lm_interaction)^2))/4 / summary(lm_interaction)$sigma^2, df1 = 4, df2 = summary(lm_interaction)$df[2], lower.tail = FALSE)`. This suggests both non-smoker groups are indistinguishable and likewise that there is no evidence that slopes for $\texttt{age}$ are not equal.

Most interactions include functions of categorical variables together with either other categorical variables (different intercepts / conditional means per subgroups) or else other. Keep in mind that the validity of our tests above depend on the model being correctly specified: there is however evidence of difference in heterogeneity between groups, with unexplained non-smoker records. Plotting the residuals from the model that includes four different intercepts and slopes for $\texttt{age}$ for each combination of smoker/obesity status misses other features that one would capture in diagnostic plots. in particular, except for some notable outliers, there is evidence that the premiums of smokers also increase with body mass index; as evidenced by @fig-resod. If we include `bmi` as additional covariate in the model in addition of `obese`, the interpretation of changes in obesity will depend on the value of `bmi` and vice-versa.

```{r }
#| label: fig-resod
#| echo: false
#| fig-cap: Residuals from the interaction model for `charges` with `age`,
#|   `smoker` and `obesity`. There is a notably outlier for a male
#|   smoker whose `bmi` is exactly 30 and other points above. There is indication
#|   of a linear trend for both smoker sub-groups as their body mass index increase,
#|   which was not apparent previously because of the effect of age.
insurance$res <- resid(lm_interaction)
ggplot(data = insurance, aes(x = bmi, y = res, col = smobese)) +
    geom_vline(xintercept = 30) +
  geom_point() +
  xlab("body mass index") +
  ylab("residuals") +
  MetBrewer::scale_color_met_d("Hiroshige") +
  labs(col = "smoking/obesity status") +
     theme(legend.position = "bottom")
```

:::

-->

The concept of interactions readily extends to categorical variables with $k$ levels/categories. In this case, we need to use the global $F$-test to check if the interaction is statistically significant.


:::{#def-anova}

## Analysis of variance

An analysis of variance is a linear model in which the mean is a function of categorical explanatory variables. If we have data for all different combinations of factors, the factors are **crossed** and we can consider inclusion of their interactions. 

:::

Consider a two-way analysis of variance model. This is a linear model with two factors, $A$ and $B$, with respectively $n_a$ and $n_b$ levels. The response $Y_{ijk}$ of the $k$th measurement in group $(a_i, b_j)$ is
$$
\underset{\text{response}\vphantom{b}}{Y_{ijk}} = \underset{\text{subgroup mean}}{\mu_{ij}} + \underset{\text{error term}}{\varepsilon_{ijk}}
$$ {#eq-twowayasoneway} 
where 

- $Y_{ijk}$ is the $k$th replicate for $i$th level of factor $A$ and $j$th level of factor $B$
- $\mu_{ij}$ is the average response of measurements in group $(a_i, b_j)$
- $\varepsilon_{ijk}$ are independent error terms with mean zero and standard deviation $\sigma$.


In a full factorial design with interactions, we can write the mean response as $\mathsf{E}(Y \mid A=a_i, B=b_j) = \mu_{ij}$. This model can be reduced to a single one-way ANOVA with a single factor having $n_an_b$ levels. This may be useful to specify contrast weights, or when there is an additional control group in an experimental setting. However, preserving the structure helps setting up hypotheses of interest.

```{r}
#| eval: false
#| echo: false
tab <- rbind(c("$\\mu_{11}$", "$\\mu_{12}$"), 
      c("$\\mu_{21}$", "$\\mu_{22}$"))
rownames(tab) <- c("$\\boldsymbol{a_1}$","$\\boldsymbol{a_2}$")
colnames(tab) <- c("$\\boldsymbol{b_1}$", "$\\boldsymbol{b_2}$")
knitr::kable(tab, booktabs = TRUE)
```


We can equivalently express this in terms of an intercept, main effects of either variables, and interaction terms. The additive model, with no interaction, has average for cell $(i,j)$ of

\begin{align*}
\mathsf{E}(Y_{ij} \mid A = a_i, B=b_j) = \mu + \alpha_i + \beta_j.
\end{align*}
<!--
- $\mu$ is the average of all subgroup averages, termed overall mean.
- $\alpha_i = \mu_{i.} - \mu$ is the mean of level $A_i$ minus the overall mean.
- $\beta_j  = \mu_{.j} - \mu$ is the mean of level $B_j$ minus the overall mean.
- $(\alpha\beta)_{ij} = \mu_{ij} - \mu_{i.} - \mu_{.j} + \mu$ is the interaction term for $A_i$ and $B_j$ which encodes the effect of both variable not already captured by the main effects.
-->


<!--
The model formulation in terms of difference from the global average or main effect ensures that we can test for main effects for factor $A$ by setting $\mathscr{H}_0: \alpha_1 = \cdots = \alpha_{n_a-1}=0$. The $1 +  n_a + n_b$ **sum to zero** constraints,
$$\sum_{i=1}^{n_a} \alpha_i=0, \quad \sum_{j=1}^{n_b} \beta_j=0, \quad  \sum_{j=1}^{n_b} (\alpha\beta)_{ij}=0, \quad \sum_{i=1}^{n_a} (\alpha\beta)_{ij}=0.$$
-->

We can consider model simplifications from bottom up. Removing the interaction leads to a model with $1 + (n_a-1) + (n_b-1)$ parameters, relative to $n_a\times n_b$ for the model with the interaction. We can use an $F$-test to check for the significance of the latter. If the factors don't interact, the mean in the cell is given by the sum of the main effects. Only once we have a removed this term can we consider if all row means  or column means are the same.

:::

While formal testing is needed to check for interactions, the concept can be better understood by looking at graphs (at least in a setting where the means are known with little to no uncertainty).

:::{#def-interactionplot}


## Interaction plot


We can try to detect interactions visually by plotting the (mean) response as a function of one of the covariates, using a so-called **interaction plot**. When there are more than two categorical variables, we can use colors, symbols or panels to represent the categories. Lack of interaction in those plots implies parallel lines, but one must account for the uncertainty.



```{r}
#| label: fig-2by2
#| fig-cap: "Interaction plots (line graphs) for example patterns for means for each of the possible kinds of general outcomes in a 2 by 2 design. Illustration adapted from Figure 10.2 of @Crump.Navarro.Suzuki:2019 by Matthew Crump (CC BY-SA 4.0 license)."
#| out-width: "100%"
#| fig-width: 10
#| fig-height: 8
p1 <- data.frame(
  factorA = c("a1", "a1", "a2", "a2"),
  factorB = c("b1", "b2", "b1", "b2"),
  means = c(5, 5, 5, 5)
)
p2 <- data.frame(
  factorA = c("a1", "a1", "a2", "a2"),
  factorB = c("b1", "b2", "b1", "b2"),
  means = c(10, 10, 5, 5)
)
p3 <- data.frame(
  factorA = c("a1", "a1", "a2", "a2"),
  factorB = c("b1", "b2", "b1", "b2"),
  means = c(5, 10, 5, 10)
)
p4 <- data.frame(
  factorA = c("a1", "a1", "a2", "a2"),
  factorB = c("b1", "b2", "b1", "b2"),
  means = c(5, 10, 10, 15)
)
p5 <- data.frame(
  factorA = c("a1", "a1", "a2", "a2"),
  factorB = c("b1", "b2", "b1", "b2"),
  means = c(5, 10, 10, 5)
)
p6 <- data.frame(
  factorA = c("a1", "a1", "a2", "a2"),
  factorB = c("b1", "b2", "b1", "b2"),
  means = c(10, 13, 5, 2)
)
p7 <- data.frame(
  factorA = c("a1", "a1", "a2", "a2"),
  factorB = c("b1", "b2", "b1", "b2"),
  means = c(2, 12, 5, 9)
)
p8 <- data.frame(
  factorA = c("a1", "a1", "a2", "a2"),
  factorB = c("b1", "b2", "b1", "b2"),
  means = c(10, 18, 5, 7)
)

all_22s <- rbind(p1, p2, p3, p4, p5, p6, p7, p8)
type <- factor(rep(1:8, each = 4), 
               labels = c("no effect",
                          "main effect of A only",
                          "main effect of B only",
                          "both main effects",
                          "interaction only",
                          "main effect of A and interaction",
                           "main effect of B and interaction",
                          
                          "both main effects and interaction" ))
all_22s <- cbind(all_22s, type)
options(ggplot2.discrete.colour= MetBrewer::met.brewer(name = "Hiroshige", 2))
ggplot(all_22s, 
       mapping = aes(x = factorA, 
                     y = means, 
                     type = type,
                     group = factorB, 
                     color = factorB))+
  geom_point() +
  geom_line() +
  labs(x = "factor A",
       subtitle = "mean response",
       y = "",
       color = "factor B") +
  facet_wrap(~type, nrow = 2) +
  theme_classic() +
  theme(legend.position = "bottom")
```

:::

:::{#def-simple}

## Simple effects and main effects

When interactions do not exist, it makes sense to abstract from one or more variable and consider **marginal effects**, obtained by pooling data from the omitted factors and averaging out. Suppose without loss of generality that we are interested in comparing levels of $A$. When interactions between $A$ and $B$ are not significant, we can consider lower order terms and report **estimated marginal means** and contrasts between means of $A$. If the interaction with $B$ has an impact, we can rather compute the subcell average of $A \mid B=b_j$, and similarly for contrasts. We thus distinguish between the following:


- **simple effects**: difference between levels of one in a fixed combination of others. Simple effects are comparing cell averages within a given row or column.
- **main effects**: differences relative to average for each condition of a factor. Main effects are row/column averages.

:::

:::{#exm-STC21-twoway}

## Psychological ownership of borrowed money

Supplemental Study 5 from @Sharma.Tully.Cryder:2021 checks the psychological perception of borrowing money depending on the label. The authors conducted a 2 by 2 between-subject comparison (two-way ANOVA) varying the type of debt (whether the money was advertised as `credit` or `loan`) and the type of purchase the latter would be used for (`discretionary` spending or `need` for necessary purchases). The response is the average of the likelihood and interest in the product, both measured using a 9 point Likert scale from 1 to 9.

The mean model with an interaction can be written using the treatment contrast parametrization as
\begin{align*}
\texttt{likelihood} &= \beta_0 + \beta_1\mathbf{1}_{\texttt{purchase=need}} + \beta_2\mathbf{1}_{\texttt{debttype=loan}} \\&\quad+ \beta_3\mathbf{1}_{\texttt{purchase=need}}\mathbf{1}_{\texttt{debttype=loan}} + \varepsilon
\end{align*}

@Sharma.Tully.Cryder:2021 fitted a model with two factors, each with two levels, and their interaction. Since there are one global average and two main effect (additional difference in average for both factors `debttype` and `purchase`), the interaction involves one degree of freedom since we go from a model with three parameters describing the mean to one that has a different average for each of the four subgroups.

The reason why this is first test to carry out is that if the effect of one factor depends on the level of the other, as shown in @fig-2by2, then we need to compare the label of debt type separately for each type of purchase and vice-versa using simple effects. If the interaction on the contrary isn't significant, then we could pool observations instead and average across either of the two factors, resulting in the marginal comparisons with the main effects.

Fitting the model including the interaction between factors ensures that we keep the additivity assumption and that our conclusions aren't misleading: the price to pay is additional mean parameters to be estimated, which isn't an issue if you collect enough data, but can be critical when data collection is extremely costly and only a few runs are allowed.

In **R**, we include both factors in a formula as 
`response ~ factorA * factorB`, the `*` symbol indicating that both are allowed to interact, as a shorthand for `factorA + factorB + factorA:factorB`; in the main effect model, we would use instead `+` to reflect that the effects of both factors add up.

```{r}
#| eval: true
#| echo: true
# Analysing Supplementary Study 5
# of Sharma, Tully, and Cryder (2021)
data(STC21_SS5, package = "hecedsm")
mod <- aov(likelihood ~ purchase*debttype, 
           data = STC21_SS5)
# Compute means of rows/columns/cells
model.tables(mod, type = "means")
# Analysis of variance reveals 
# non-significant interaction
# of purchase and type
car::Anova(mod, type = 2)
```

Since the interaction is not significant, we can only interpret the main effect of fixation. These conditional mean difference are termed marginal effect, because they are obtained by averaging out the other explanatory. The model however estimates the variance based on residuals from the full interaction model with four cell means, so differs from that obtained by (incorrectly) running a model with only `purchase` as explanatory.

In the analysis of variance table, we focus exclusively on the last line with the sum of squares for `purchase:debttype`. The $F$ statistic is `r round(car::Anova(mod, type = 3)[4,3],2)`; using the $\mathsf{F}$ (`r car::Anova(mod, type = 3)[4,2]`, `r car::Anova(mod, type = 3)[5,2]`) distribution as benchmark, we obtain a $p$-value of `r format.pval(car::Anova(mod, type = 3)[4,4], digits=2)` so there is no evidence the effect of purchase depends on debt type.

We can thus pool data and look at the effect of debt type (`loan` or `credit`) overall by combining the results for all purchase types, one of the planned comparison reported in the Supplementary material. To do this in **R** with the `emmeans` package, we use the `emmeans` function and we quote the factor of interest (i.e., the one we want to keep) in `specs`. By default, this will compute the estimate marginal means: the `contr = "pairwise"` indicates that we want the difference between the two, which gives us the contrasts.

```{r}
#| eval: true
#| echo: true
#| label: fig-interaction-ST
#| fig-cap: "Interaction plots for the @Sharma.Tully.Cryder:2021 data."
# Pairwise comparisons within levels of purchase
# Simple effect
emmeans::emmeans(mod, 
                 specs = "purchase",
                 contr = "pairwise")
# Interaction plot
emmeans::emmip(mod, 
               purchase ~ debttype, 
               CIs = TRUE) +
  theme_minimal()
```

:::


:::{#rem-sumofsquare}

## Sum of square decomposition

There are different sum of square decompositions (type I, II and III) for the comparison of nested models in analysis of variance tables produced by `anova`. These test different models using $F$ statistics, with the same denominator based on $S_2$ from the model output, and the numerator is the difference in sum of squares. All of the decompositions agree when the sample size is **balanced**, meaning each cell has the same number of replications $n_r$, so that the overall number of observations is $n = n_an_bn_r$.

```{r}
#| label: tbl-ssdecompo
#| eval: true
#| echo: false
#| tbl-cap: "Sum of square decompositions in ANOVA tables. Comparison of sum of squares between null, versus alternative model."
tabSS <- cbind("type 1" = c("intercept vs $A$", "$A$ vs $(A,B)$","$(A,B)$ vs $(A,B,AB)$"),
 "type II" = c("$B$ vs $(A,B)$","$A$ vs $(A,B)$","$(A,B)$ vs $(A,B,AB)$"),
"type III" = c("$(B, AB)$ vs $(A,B, AB)$","$(A, AB)$ vs $(A,B,AB)$","$(A,B)$ vs $(A,B,AB)$"))
rownames(tabSS) <- c("$\\boldsymbol{A}$","$\\boldsymbol{B}$","$\\boldsymbol{AB}$")
knitr::kable(tabSS, booktabs = TRUE)
```

@tbl-ssdecompo shows the different sum of squared errors of the models, with the terms in parenthesis indicating which terms are included ($AB$ denotes the interaction).

Type I, the default with the generic `anova`, uses the order in which terms enter, say $A$, $B$, $AB$, so compares in the first line the improvement in the mean-only model with $A$, then in the second line the test for $B$ compares the model with both main effects $A$ and $B$ with only $A$. Since the order in which the factors is specified is arbitrary, this decomposition is arbitrary and not relevant.

The type II decomposition considers terms of the same level in the hierarchy, so the tests for the main effects are $A + B$ vs $A$, $A+B$ vs $B$ and that of the interaction is $A\times B$ vs $A, B$. This should be the default option if we wish to consider main effects when the interaction isn't significant. 

The type III decomposition, popularized by SAS and often taken as the default, considers all other terms, so would test main effects as $A + B + A\times B$ vs $B + A\times B$. This does not respect the marginality principle, so should be avoided. The tests for $A$ or $B$ should not be used.

All three methods agree for the last level with the interaction.

:::


All of the discussion for a two-way ANOVA extends to higher-dimensional designs for $K$ factors. However, the curse of dimensionality makes it more difficult to collect observations in each cell. Any multiway ANOVA with two or more factor can be collapsed into a single one-way ANOVA: this is notably useful when there is a control group which is not related to the factor levels, as no manipulation takes place. The use of contrasts becomes critical since we can write any test for main effects, interactions using the latter through weighting.

:::{#exm-LKUK24}

## Perceptions of cultural appropriation by ideology

We consider a three-way ANOVA from @Lin.Kim.Uduehi.Keinan:2024. Their Study 4 focused on cultural appropriation for soul food recipe cookbook from Chef Dax, who was either black (or not), manipulating the description of the way he obtained the recipes (by peeking without permission in kitchens, by asking permission or no mention for control). Authors postulated that the perception of appropriation would vary by political ideology (liberal or conservative). The study results in a 3 by 2 by 2 three-way ANOVA.

```{r}
data(LKUK24_S4, package = "hecedsm")
# Check repartition of observations in subgroups
xtabs(~politideo + chefdax + brandaction,
          data = LKUK24_S4)
# Factors are crossed, and there are replications
# We fit the three-way ANOVA model (all interactions)
mod <- lm(appropriation ~ politideo * chefdax * brandaction,
          data = LKUK24_S4)
# Compute estimated marginal means for each of the 12 subgroups
emm <- emmeans(mod, 
        specs = c("chefdax", "brandaction", "politideo"))
# Create an interaction plot
emmip(object = emm, 
        formula = brandaction ~ chefdax | politideo, 
        CIs = TRUE)
anova_tab <- car::Anova(mod, type = 2)
```

For the $K$-way ANOVA, we always start with estimating the full model with all $K$-way interaction (provided there are enough data to estimate the latter, which implies there are repetitions). If the latter is significant, we can fix one or more factor levels and compare the others.

```{r}
#| eval: true
#| echo: false
#| label: tbl-anova-LKUK24
#| tbl-cap: "Analysis of variance table (type II decomposition) for the data from Study 4 of Lin et al. (2024)." 
options(contrasts = c("contr.sum", "contr.poly"))
data(LKUK24_S4, package = "hecedsm")
mod <- lm(appropriation ~ politideo * chefdax * brandaction,
   data = LKUK24_S4)
options(knitr.kable.NA = '')
broom::tidy(car::Anova(mod, type = 2)) |>
  dplyr::mutate(p.value =ifelse(is.na(p.value), NA, format.pval(p.value, digits = 2, eps = 1e-3))) |>
  knitr::kable(digits = 2, 
               col.names = c("term", "sum of squares","df","stat","p-value"),
               booktabs = TRUE, 
               linesep = "")
```


If we consider @tbl-anova-LKUK24, we find that there is no three-way interaction and, omitting the latter and focusing on lower-level, a single two-way interaction between political ideology and the race of Chef Dax. We cannot interpret the $p$-value for the main effect of `brandaction`, but we could look at the marginal means.

Based on the data, we will collapse data to a one-way ANOVA comparing the three levels of `brandaction` and a 2 by 2 two-way ANOVA for the other two factors. The results are obtained by averaging over the missing factor, but estimating the standard deviation $\sigma^2$ from the full model.

We are interested in comparing the perception between the race of Chef Dax (black or not, as Southern Soul food cooking is more likely to be associated with cultural appropriation if Chef Dax is not black. We proceed with `emmeans` by computing the marginal means separately for each of the four subcategories, but compare the race of Chef Dax separately for liberals and conservatives due to the presence of the interaction.

```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false
data(LKUK24_S4, package = "hecedsm")
library(emmeans)
mod <- lm(appropriation ~ politideo * chefdax * brandaction,
   data = LKUK24_S4)
# Marginal means for political ideology/Chef Dax
# Compute simple effects, by political ideology
emmeans(mod, 
         specs = "chefdax", 
         by = "politideo",
         contrast = "pairwise")
```
We see that the liberals are much more likely to view Chef Dax cookbook as an instance of cultural appropriation if he is not black; there is limited evidence of any difference between conservatives and liberal when Chef Dax is black. Both differences are statistically significative, but the differences (and thus evidence of an effect) is much stronger for left-leaning respondents.

We can look next at the brand action: we expect participants will view peeking less favorably than if Chef Dax asked for permission to publish the recipes. It's tricky to know the effect of the control, as we are not bringing the point to the attention of participants in this instance.

```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false
# Marginal mean for brandaction
emm_brand <- emmeans(mod, specs = c("brandaction")) 
emm_brand
# Joint F test for the main effect of brandaction
emm_brand |> pairs() |> joint_tests()
```

A joint $F$-test, obtained by collapsing everything to a one-way ANOVA, shows that there are indeed differences. However, note that the averages of the three actions are much smaller than for race.

:::

<!--
Interactions
Two-way ANOVA - one mean per cell
Crossed factors

Interactions (categorical vs categorical)
Interactions (categorical vs continuous)
Tests for interactions
Remark: what can you estimate + is estimation reliable?
Remark: marginality principle.
Interactions and interaction plots
Simple and marginal effects
Confounding variables



Linearity and interpretation of effects - added variable plots

Colinearity and identifiability
Model assumptions
Residual diagnostics
Plots and remedies/ a primer on other models
-->


## Geometry of least squares


:::{#rem-geometry}

## Geometry

The vector of fitted values $\widehat{\boldsymbol{y}} =\mathbf{X} \widehat{\boldsymbol{\beta}} = \mathbf{H}_{\mathbf{X}}\boldsymbol{y}$ is the projection of the response vector $\boldsymbol{y}$ on the linear span generated by the columns of $\mathbf{X}$. The matrix $\mathbf{H}_{\mathbf{X}} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top$, often called hat matrix, is an orthogonal projection matrix, so $\mathbf{H}_{\mathbf{X}}=\mathbf{H}_{\mathbf{X}}^\top$ and $\mathbf{H}_{\mathbf{X}}\mathbf{H}_{\mathbf{X}} = \mathbf{H}_{\mathbf{X}}$ and $\mathbf{H}_{\mathbf{X}}\mathbf{X} = \mathbf{X}$. Since the vector of residuals $\boldsymbol{e} = (e_1, \ldots, e_n)^\top$, which appear in the sum of squared errors, is defined as $\boldsymbol{y} - \widehat{\boldsymbol{y}}$ and $\widehat{\boldsymbol{y}}=\mathbf{X}\boldsymbol{\beta}$, simple algebraic manipulations show that the inner product between ordinary residuals and fitted values is zero, since
\begin{align*}
\widehat{\boldsymbol{y}}^\top\boldsymbol{e} &= \widehat{\boldsymbol{\beta}}^\top \mathbf{X}^\top (\boldsymbol{y}- \mathbf{X} \widehat{\boldsymbol{\beta}})
\\&= \boldsymbol{y}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top(\boldsymbol{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y})\\&=\boldsymbol{y}^\top\mathbf{H}_{\mathbf{X}}\boldsymbol{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y}
\\&= 0
\end{align*}
where we use the definition of $\widehat{\boldsymbol{y}}$ and $\boldsymbol{e} = \boldsymbol{y} - \widehat{\boldsymbol{y}}$ on the first line, then substitute the OLS estimator  $\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{y}$ and distribute terms.
Similarly, $\mathbf{X}^\top\boldsymbol{e}=\boldsymbol{0}_{p+1}$.
The ordinary residuals are thus orthogonal to both the model matrix $\mathbf{X}$ and to the fitted values.


:::{#cor-cor}

## Orthogonality of residuals and fitted values

A direct consequence of this fact is that the sample linear correlation between $\boldsymbol{e}$ and $\widehat{\boldsymbol{y}}$ is zero, so a simple linear regression of $\boldsymbol{e}$ as a function of $\widehat{\boldsymbol{y}}$ has zero intercept and zero slope. The same is true for any plot of $\boldsymbol{e}$ against a column of $\mathbf{X}$. Any additional pattern visible must come from omitted dependence.

```{r}
#| label: fig-zerocor
#| eval: true
#| echo: false
#| fig-cap: "Plot of residuals against fitted values (left), and against the explanatory variable `service` (right) for the linear regression of the `college` data. The intercept and the slope of the simple linear regressions are zero."
mod <- lm(salary ~ sex + field + rank + service, data = college)
g1 <- ggplot(data = data.frame(yhat = fitted(mod), 
                         e = resid(mod)),
       mapping = aes(x = yhat, y = e)) +
  geom_point() +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE, col = "grey") +
  labs(x = "fitted values", y = "residuals")
g2 <- ggplot(data = data.frame(service = college$service, 
                         e = resid(mod)),
       mapping = aes(x = service, y = e)) +
  geom_point() +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE, col = "grey") +
  labs(x = "years of service", y = "residuals")
g1 + g2
```

:::

:::{#cor-zeromean}

## Mean of residuals

Since the inner product between the model matrix $\mathbf{X}$ and the residuals $\boldsymbol{e}$ is zero, the sample mean of $\boldsymbol{e}$ must be zero provided that $\mathbf{1}_n$ is in the linear span of $\mathbf{X}$, which is the case as soon as we include an intercept.

```{r}
#| eval: true
#| echo: true
mod <- lm(salary ~ sex + field + rank + service, data = college)
# Zero correlations
with(college, cor(resid(mod), service))
cor(resid(mod), fitted(mod))
# Mean zero errors
mean(resid(mod))
```

:::

:::{#rem-invariance}

## Invariance

One direct consequence of the definition of the estimator in terms of projection matrices is that the fitted values $\widehat{y}_i$ for two model matrices $\mathbf{X}_a$ and $\mathbf{X}_b$, are the same if they generate the same linear span, as in @exm-baumann-dummies. The interpretation of the coefficients will however change. If we include an intercept term, then we get the same output if the columns of explanatory are mean-centered.

:::

The value of $\boldsymbol{\beta}$ is such that it will maximize the correlation between $Y$ and $\widehat{Y}$. In the case of a single categorical variable, we will obtain fitted values $\widehat{y}$ that correspond to the sample mean of each group.

:::

### Residuals

Residuals are predictions of the errors $\varepsilon$ and represent the difference between the observed value $Y_i$ and the estimated value on the line.
The ordinary residuals are
\begin{align*}
e_i=Y_i-\widehat{Y}_i, \qquad i =1, \ldots, n.
\end{align*}
The sum of the ordinary residuals is always zero by construction if the model includes an intercept, meaning $\overline{e} = 0$.

Not all observations contribute equally to the adjustment of the fitted hyperplane. The geometry of least squares shows that the residuals are orthogonal to the fitted values, and $\boldsymbol{e} = (\mathbf{I}_n-\mathbf{H}_{\mathbf{X}})\boldsymbol{Y}$, where $\mathbf{H}_{\mathbf{X}}=\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top$ is an $n \times n$ projection matrix that spans the $p$-dimensional linear combination of the columns of $\mathbf{X}$, $\mathscr{S}(\mathbf{X})$. If $\mathsf{Va}(\boldsymbol{Y}) = \sigma^2\mathbf{I}_n$, it follows that $\mathsf{Va}(\boldsymbol{e})=\sigma^2(\mathbf{I}_n-\mathbf{H}_{\mathbf{X}})$ because $(\mathbf{I}_n-\mathbf{H}_{\mathbf{X}})$ is a projection matrix, therefore idempotent and symmetric. Because the matrix has rank $n-p$, the ordinary residuals cannot be independent from one another.

If the errors are independent and homoscedastic, the ordinary residual $e_i$ has variance $\sigma^2(1-h_{i})$, where the leverage term $h_i =(\mathbf{H}_{\mathbf{X}})_{ii} = \mathbf{x}_i (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}_i$ is the $i$th diagonal entry of the projection matrix $(\mathbf{H}_{\mathbf{X}})$ and $\mathbf{x}_i$ is the $i$th row of the model matrix corresponding to observation $i$.

We thus conclude that ordinary residuals do not all have the same standard deviation and they are not independent. This is problematic, as we cannot make meaningful comparisons: points with low leverage are bound to deviate more from the fitted model than others. To palliate to this, we can standardize the residuals so each has the same variance under the null of independent homoscedastic errors --- the leverage terms $h_i$ are readily calculated from the model matrix $\mathbf{X}$.
The only remaining question is how to estimate the variance. If we use the $i$th observation to estimate both the residual and the variance, we introduce additional dependence. A better way is remove  the $i$th observation and refit the model with the $n-1$ remaining observations to get of $s^2_{(-i)}$ (there are tricks and closed-form expressions for these, so one doesn't need to fit $n$ different linear models). The jacknife studentized residual $r_i = e_i/\{s_{(-i)}(1-h_i)\}$, also termed externally studentized residuals, are not independent, but they are identically distributed and follow a Student distribution with $n-p-2$ degrees of freedom.
These can be obtained in **R** with the command `rstudent`.

When to use which residuals? By construction, the vector of ordinary residuals $\boldsymbol{e}$ is orthogonal to the fitted values $\widehat{\boldsymbol{y}}$ and also to each column of the model matrix $\mathbf{X}$: this means a simple linear regression of $\boldsymbol{e}$ with any of these as covariate gives zero intercept and zero slope. However, residual patterns due to forgotten interactions, nonlinear terms, etc. could be picked up from pair plots of ordinary residuals against the explanatories.

While the jackknife studentized residuals $r_i$ are not orthogonal, they are not very different. Use jackknife residuals $\boldsymbol{r}$ to check for equality of variance and distributional assumptions (e.g., using quantile-quantile plots).

One thus typically uses ordinary residuals $\boldsymbol{e}$ for plots of fitted values/explanatories against residuals and otherwise jackknife studentized residuals for any other graphical diagnostic plot.


:::{#def-r2}

## Coefficient of determination

When we specify a model, the error term $\boldsymbol{\varepsilon}$ accounts for the fact no perfect linear relationship characterizes the data (if it did, we wouldn't need statistic to begin with). Once we have fitted a model, we estimate the variance $\sigma^2$; one may then wonder which share of the total variance in the sample is explained by the model.

The total sum of squares, defined as the sum of squared residuals from the intercept-only model, serves as comparison --- the simplest model we could come up with would involving every observation by the sample mean of the response and so this gives (up to scale) the variance of the response, $\mathsf{SS}_c = \sum_{i=1}^n (y_i - \overline{y})^2$. We can then compare the variance of the original data with that of the residuals from the model with model matrix $\mathbf{X}$, defined as $\mathsf{SS}_e =\sum_{i=1}^n e_i^2$ with $e_i = y_i - \widehat{\beta}_0 - \sum_{j=1}^p \widehat{\beta}_jX_j$.
We define the coefficient of determination, or squared multiple correlation coefficient of the model, $R^2$, as
\begin{align*}
R^2 &=1- \frac{\mathsf{SS}_e}{\mathsf{SS}_c} = 1 - \frac{sum_{i=1}^n (y_i - \widehat{y}_i)^2}{\sum_{i=1}^n (y_i - \overline{y})^2}.
\end{align*}
An alternative decomposition shows that $R^2 = \mathsf{cor}^2(\boldsymbol{y}, \widehat{\boldsymbol{y}})$, i.e., the coefficient of determination can be interpreted as the square of [Pearson's linear correlation](#def-correlation-Pearson) between the response $\boldsymbol{y}$ and the fitted values $\widehat{\boldsymbol{y}}$.

:::

Its important to note that $R^2$ is not a goodness-of-fit criterion, like the log likelihood: some phenomena are inherently noisy and even a good model will fail to account for much of the response's variability. Moreover, one can inflate the value of $R^2$ by including more explanatory variables and making the model more complex, thereby improving the likelihood and $R^2$. Indeed, the coefficient is non-decreasing in the dimension of $\mathbf{X}$, so a model with $p+1$ covariate will necessarily have a higher $R^2$ values than only $p$ of the explanatories. For model comparisons, it is better to employ information criteria or else rely on the predictive performance if this is the purpose of the regression. Lastly, a model with a high $R^2$ may imply high correlation, but [the relation may be spurious](http://www.tylervigen.com/spurious-correlations): linear regression does not yield causal models!


### Collinearity

The linearity assumption can be interpreted broadly to mean that all relevant covariates have been included and that their effect is correctly specified in the equation of the mean. Adding superfluous covariates to a model has limited impact: if the (partial) correlation between a column vector $\mathbf{X}_k$ and the response variable $\boldsymbol{Y}$ is zero, then $\beta_k=0$ and the estimated coefficient $\widehat{\beta}_k \approx 0$ because the least square estimators are unbiased. If we include many useless variables, say $k$, the lack of parsimony can however make interpretation more difficult. The price to pay for including the $k$ additional covariates is an increase in the variance of the estimators $\widehat{\boldsymbol{\beta}}$.

It is nevertheless preferable to include more variables than to forget key predictors: if we omit an important predictor, their effect may be picked up by other regressors (termed **confounders**) in the model with are correlated with the omitted variable. The interpretation of the other effects can be severely affected by confounders. For example, the simple linear model (or two-sample $t$-test) for salary as a function of sex for the `college` data is invalid because sex is a confounder for rank. Since there are more men than women full professor, the mean salary difference between men and women is higher than it truly is. One way to account for this is to include control variables (such as rank), whose effect we need not be interested in, but that are necessary for the model to be adequate. We could also have used stratification, i.e., tested for wage discrimination within each academic rank. This is the reason why sociodemographic variables (sex, age, education level, etc.) are collected as part of studies.



A linear model is not a [causal model](https://xkcd.com/552/): all it does is capture the linear correlation between an explanatory variable and the response. When there are more than one explanatory, the effect of $x_j$ given what has not already been explained by $\boldsymbol{X}_{-j}$. Thus, if we fail to reject $\mathscr{H}_0:\beta_j=0$ in favor of the alternative $\mathscr{H}_1: \beta_j \neq 0$, we can only say that there is no significant *linear* association between $x_j$ and $Y$ once the effect of other variables included in the model has been accounted for. There are thus two scenarios: either the response is uncorrelated with $x_j$ (uninteresting case, but easy to pick up by plotting both or computing linear correlation), or else there is a strong correlation between $x_j$ and both the response $Y$ as well as (some) of the other explanatory variables $x_1, \ldots, x_p$. This problem is termed (multi)collinearity.

One potential harm of collinearity is a decrease in the precision of parameter estimators. With collinear explanatories, many linear combinations of the covariates represent the response nearly as well. Due to the (near) lack of identifiability, the estimated coefficients become numerically unstable and this causes an increase of the standard errors of the parameters. The predicted or fitted values are unaffected. Generally, collinearity leads to high estimated standard errors and the regression coefficients can change drastically when new observations are included in the model, or when we include or remove explanatories. The individual $\beta$ coefficients may not be statistically significant, but the global $F$-test will indicate that some covariates are relevant for explaining the response. This however would also be the case if there are predictors with strong signal, so neither is likely to be useful to detect issues.

The added-variable plot shows the relation between the response $Y$ and an explanatory $x_j$ after accounting for other variables: the slope $\widehat{\beta}_j$ of the simple linear regression is the same of the full model. A similar idea can be used to see how much of $x_j$ is already explained by the other variables. For a given explanatory variable $x_j$, we define its **variance inflation factor** as $\mathsf{VIF}(j)=(1-R^2(j))^{-1}$, where $R^2(j)$ is the coefficient of determination of the model obtained by regressing $x_j$ on all the other explanatory variables, i.e.,
\begin{align*}
x_j = \beta^{\star}_0 + \beta^{\star}_1 x_1 + \cdots + \beta^{\star}_{j-1} x_{j-1} + \beta^{\star}_{j+1} x_{j+1} + \cdots + \beta^{\star}_px_p + \varepsilon^{\star}
\end{align*}
By definition, $R^2(j)$ represents the proportion of the variance of $x_j$ that is explained by all the other predictor variables. Large variance inflation factors are indicative of problems (typically covariates with $\mathsf{VIF}>10$ require scrutinity, and values in the hundreds or more indicate serious problems).

Added-variable plots can also serve as diagnostics, by means of comparison of the partial residuals with a scatterplot of the pair $(Y, x_j)$; if the latter shows very strong linear relation, but the slope is nearly zero in the added-variable plot, this hints that collinearity is an issue.

What can one do about collinearity? If the goal of the study is to develop a predictive model and we're not interested in the parameters themselves, then we don't need to do anything. Collinearity is not a problem for the overall model: it's only a problem for the individual effects of the variables. Their joint effect is still present in the model, regardless of how the individual effects are combined.

If we are interested in individual parameter estimates, for example,
to see how (and to what extent) the predictor variables explain the behaviour of $Y$, then things get more complicated. Collinearity only affects the variables that are strongly correlated with one another, so we only care if it affects one or more of the variables of interest. There sadly is no good solution to the problem. One could

- try to obtain more data, so as to reduce the effects of collinearity appearing in specific samples or that are due to small sample size.
- create a composite score by somehow combining the variables showing collinearity.
- remove one or more of the collinear variables. You need to be careful when doing this not to end up with a misspecified model.
- use penalized regression. If $\mathbf{X}^\top\mathbf{X}$ is (nearly) not invertible, this may restore the uniqueness of the solution. Penalties introduce bias, but can reduce the variance of the estimators $\boldsymbol{\beta}$. Popular choices include ridge regression (with an $l_2$ penalty), lasso ($l_1$ penalty), but these require adjustment in order to get valid inference.

Whatever the method, it's important to understand that it can be very difficult (and sometimes impossible) to isolate the individual effect of a predictor variable strongly correlated with other predictors.

:::{#exm-collegedatcollinear}

## Collinearity in the `college` data
We consider the `college` data analysis and include all the covariates in the database, including `years`, the number of years since PhD. One can suspect that, unless a professor started his or her career elsewhere before moving to the college, they will have nearly the same years of service. In fact, the correlation between the two variables, `service` and `years` is `r cor(college$service, college$years)`. The variance inflation factor for the five covariates


For categorical variables, the variance inflation factor definition would normally yield for each level a different value; an alternative is the generalized variance inflation factor [@Fox:1992]. Here, we are interested in gender disparities, so the fact that both service and field are
strongly correlated is not problematic, since the $\mathsf{VIF}$ for $\texttt{sex}$ is not high and the other variables are there to act as control and avoid confounders.


```{r}
#| echo: false
vifs <- car::vif(lm(salary ~ service + years + rank + sex+ field, data = college))[,1]
knitr::kable(t(vifs), caption = "(Generalized) variance inflation factor for the $\\texttt{college}$ data.", booktabs = TRUE, escape = FALSE)
```
:::

### Leverage and outliers

The leverage $h_i$ of observation $i$ measures its impact on the least square fit, since we can write $h_i = \partial \widehat{y}_i/\partial y_i$. Leverage values tell us how much each point impacts the fit: they are strictly positive, are bounded below by $1/n$ and above by $1$. The sum of the leverage values is $\sum_{i=1}^n h_i=p+1$: in a good design, each point has approximately the same contribution, with average weight $(p+1)/n$.

Points with high leverage are those that have unusual combinations of explanatories. An influential observation ($h_i\approx 1$) pulls the fitted hyperplane towards itself so that $\hat{y}_i \approx y_i$. As a rule of thumb, points with $h_i> 2(p+1)/n$ should be scrutinized.

It is important to distinguish betwen **influential** observations (which have unusual $\mathbf{x}$ value, i.e., far from the overall mean) and **outliers** (unusual value of the response $y$).
If an observation is both an outlier and has a high leverage, it is problematic.

```{r}
#| label: fig-outliers
#| echo: false
#| fig-cap: Outlier and influential observation. The left panel shows an outlier, whereas
#|   the right panel shows an influential variable (rightmost $x$ value).
set.seed(1)
x <- c(rgamma(99, shape = 5), 20)
y1 <- 0.4*x+3+rnorm(100, sd = 1)
y2 <- 0.5*x[-100]+-6+rnorm(99, sd = 0.2)
dat <- data.frame(x=c(c(x[-100], 5),x), y=c(c(y2, 0), y1), group = factor(rep(1:2, each = 100L)))
ggplot(data = dat, aes(x=x, y=y), colour=group) +
  geom_point()+
  geom_smooth(method = "lm", se = FALSE, col=hecblue) +
  facet_wrap(~group, ncol = 2, scales = "free") +
  theme(legend.position = "none", strip.text = element_blank(), )
```

If influential observations can be detected by inspecting the leverage of each observation, outliers are more difficult to diagnose.

An outlier stands out from the rest of the observations, either because it has an usual response value, or because it falls far from the regression surface.
Loosely speaking, an outlier is an unusual values of $Y$ for a given combination of $\mathbf{X}$ that stands out from the rest.
Outliers can be detected during the exploratory data analysis or picked-up in residual plots (large values of $|e_i|$ in plots of fitted versus residuals) or added-variable plots. One could potentially test whether an jackknife studentized residual is an outlier (adjusting for the fact we would consider only largest values). One can also consider Cook's distance, $C_j$, a statistic giving the scaled distance between the fitted values $\hat{\boldsymbol{y}}$ and the fitted values for the model with all but the $j$th observation, $\hat{\boldsymbol{y}}^{(-j)}$,
\begin{align*}
C_j = \frac{1}{(p+1)S^2} \sum_{i=1}^n \left\{\hat{y}_i - \hat{y}_{i}^{(-j)}\right\}^2
\end{align*}
Large values of $C_j$ indicate that its residual $e_j$ is large relative to other observations or else its  leverage $h_j$ is high. A rule of thumb is to consider points for which $C_j > 4/(n-p-1)$. In practice, if two observations are outlying and lie in the same region, their Cook distance will be halved.

Outliers and influential observations should not be disregarded because they don't comply with the model, but require further investigation. They may motivate further modelling for features not accounted for. It is also useful to check for registration errors in the data (which can be safely discarded).


Except in obvious scenarios, unusual observations should not be discarded. In very large samples, the impact of a single outlier is hopefully limited. Transformations of the response may help reduce outlyingness. Otherwise, alternative objective functions (as those employed in robust regression) can be used; these downweight extreme observations, at the cost of efficiency.


## Model assumptions and diagnostics

So far, we have fit models and tested significance of the parameters without checking the model assumptions. The correctness of statements about the $p$-values and confidence intervals depend on the (approximate) validity of the model assumptions, which all stem from the distributional assumption for the error, assumed to be independent and identically distributed with $\varepsilon_i \stackrel{\cdot}{\sim} \mathsf{normal}(0, \sigma^2)$. This compact mathematical description can be broken down into four assumptions.

- linearity: the mean of $Y$ is $\beta_0 + \beta_1x_1 + \cdots + \beta_p x_p$.
- homoscedasticity: the error variance is constant
- independence of the errors/observations conditional on covariates.
- normality of the errors

This section reviews the assumptions made in order to allow statistical inference using the linear model and different residuals that serve as building blocks for graphical diagnostics. We investigate the consequences of violation of these assumptions and outline potential mitigation strategies, many of which are undertaken in other chapters.

When we perform an hypothesis test, we merely fail to reject the null hypothesis, either because the latter is true or else due to lack of evidence. The same goes for checking the validity of model assumptions: scientific reasoning dictates that we cannot know for certain whether these hold true. Our strategy is therefore to use implications of the linear model assumptions to create graphical diagnostic tools, so as to ensure that there is no gross violation of these hypothesis. However, it is important to beware of over-interpreting diagnostic plots: the human eye is very good at finding spurious patterns.

We review the assumptions in turn and discuss what happens when the assumptions fail to hold.

### Independence assumption

Usually, the independence of the observations follows directly from the type of sampling used  --- this assumption is implicitly true if the observations were taken from a *random sample* from the population. This is generally not the case for longitudinal data, which contains repeated measures from the same individuals across time. Likewise, time series are bound not to have independent observations. If we want to include all the time points in the analysis, we must take into account the possible dependence (correlation) between observations. If we ignore correlation, the estimated standard errors are too small relative to the truth, so the effective sample size is smaller than number of observations.



What is the impact of dependence between measurements? Heuristically, correlated measurements carry less information than independent ones. In the most extreme case, there is no additional information and measurements are identical, but adding them multiple times unduly inflates the statistic and leads to more frequent rejections.

```{r}
#| label: sizetestCorrelation
#| echo: false
#| eval: true
#| cache: true
# if(!"matRes.RData" %in% list.files(path = "./figures")){
# set.seed(1234)
# n <- 25
# ng <- c(2,3,5,10)
# correlation <- seq(0, 0.5, by = 0.05)
# matRes <- array(0, dim = c(1e4,
#                            length(correlation),
#                            length(ng)))
# for(i in seq_along(correlation)){
#   Sigma <- diag(1-correlation[i], nrow = n, ncol = n) +
#     matrix(correlation[i], nrow = n, ncol = n)
# for(j in seq_along(ng)){
#   for(k in seq_len(dim(matRes)[1])){
#     samp <- mev::mvrnorm(n = ng[j],
#                          mu = rep(0, n),
#                          Sigma = Sigma)
#     matRes[k,i,j] <-
#       oneway.test(data = tibble(
#         resp = c(t(samp)),
#         group = factor(x = rep(1:ng[j], each = n))),
#            formula = resp ~ group,
#            var.equal = TRUE)$p.value
#   }
#  }
# }
# save(list = c("matRes", "correlation", "ng"), file = "figures/matRes.RData")
# } else{
#   load(file = "figures/matRes.RData")
# }


```

```{r }
#| label: fig-plotLevelIndep
#| echo: false
#| eval: true
#| fig-cap: Percentage of rejection of the null hypothesis for the $F$-test of equality
#|   of means for the one way ANOVA with data generated with equal mean and variance
#|   from an equicorrelation model (within group observations are correlated, between
#|   group observations are independent). The nominal level of the test is 5%.
# size_p5 <- t(apply(matRes, 2:3, function(x){mean(x < 0.05)}))
# nent <- length(size_p5)
# size_p5_tibble <-
#   tibble(size = c(size_p5),
#          samp = factor(rep(ng, length.out = nent)),
#          correlation = rep(correlation, each = length(ng)))


# From Don Fraser (1958). Statistics: an introduction, pp. 342-343
nrep <- 25L
rho <- seq(0, 0.8, by = 0.01)
dims <- c(2,3,5,10)
size <- matrix(0, nrow = length(rho), ncol = length(dims))
for(i in seq_along(dims)){
  d <- dims[i]
  sigmasq <- (1-rho)
  tausq <- rho
  fact <- (nrep*tausq + sigmasq)/sigmasq

  cutoff <- qf(0.95, df1 = d-1, df2 = (nrep-1)*d)
  size[,i] <- pf(cutoff/fact, df1 = d-1, df2 = (nrep-1)*d, lower.tail = FALSE)
}

size_exact <- data.frame(
  size = c(size),
  dim = factor(rep(dims, each = length(rho))),
  rho = rep(rho, length.out = length(size)))

# ggplot(data = size_p5_tibble,
#        mapping = aes(col = samp,
#                      x = correlation,
#                      y = size)) +
#   geom_line() +
ggplot() +
  geom_line(data = size_exact,
            mapping = aes(color = dim, x = rho, y = size)) +
  geom_hline(yintercept = 0.05, alpha = 0.5) +
  labs(y = "size of test",
       x = "within group correlation",
       color = "number of groups",
       caption = "25 observations per group") +
  coord_cartesian(xlim = c(0, max(rho)),
                  ylim = c(0, 1),
                  expand = FALSE) +
  theme_minimal() +
  theme(legend.position = "bottom")


```

The lack of independence can also have drastic consequences on inference and lead to false conclusions: @fig-plotLevelIndep shows an example with correlated samples within group (or equivalently repeated measurements from individuals) with 25 observations per group. The $y$-axis shows the proportion of times the null is rejected when it shouldn't be. Here, since the data are generated from the null model (equal mean) with equal variance, the inflation in the number of spurious discoveries, false alarm or type I error is alarming and the inflation is substantial even with very limited correlation between measurements.

The first source of dependence is clustered data, meaning measurements taken from subjects that are not independent from one another (family, groups, etc.) More generally, correlation between observations can arises from space-time dependence, roughly categorized into

- longitudinal data: repeated measurements are taken from the same subjects (few time points)
- time series: observations observed at multiple time periods (many time points).


Time series require dedicated models not covered in this course. Because of autocorrelation, positive errors tend to be followed by positive errors, etc. We can plot the residuals as a function of time, and a scatterplot of lagged residuals $e_i$ versus $e_{i-1}$ ($i=2, \ldots, n$).

```{r}
#| label: fig-timeresidplot
#| cache: true
#| echo: false
#| fig-cap: 'Lagged residual plots: there is no evidence against independence in the
#|   left panel, whereas the right panel shows positively correlated residuals.'
lm1 <- lm(intention~emotion+factor(educ)+age+factor(revenue)+marital, data = intention)
data(airpassengers, package = "hecstatmod")
ols_airpass <- lm(log(passengers) ~ month + year, data = airpassengers)
par(pch = 19, col = scales::alpha("black", 0.8), bty = "l")
par(mar=c(3.1, 3.1, 1, 1), mgp=c(1.7, 0.6, 0), font.main=1, cex.main=0.8)
par(mfrow=c(1, 2))
# plot(resid(lm1), ylab = "residuals", xlab = "time index")
# car::gamLine(1:n, resid(lm1), spread = TRUE, col = hecblue)
plot(x = resid(lm1)[-1], y = resid(lm1)[-length(resid(lm1))],
     ylab = "residuals", xlab = "lagged residuals", main = "")
car::gamLine(resid(lm1)[-1], resid(lm1)[-length(resid(lm1))], spread = TRUE, col = hecblue)
# plot(resid(ols_airpass), ylab = "residuals", xlab = "time index")
# car::gamLine(1:length(ols_airpass$residuals), resid(ols_airpass), spread = TRUE, col = hecblue)
 plot(x = resid(ols_airpass)[-1], y = resid(ols_airpass)[-length(resid(ols_airpass))], ylab = "residuals", xlab = "lagged residuals", main = "")
 car::gamLine(resid(ols_airpass)[-1], resid(ols_airpass)[-length(resid(ols_airpass))], spread = TRUE, col = hecblue)

```


However, lagged residuals plots only show dependence at lag one between observations. For time series, we can look instead at a correlogram, i.e., a bar plot of the correlation between two observations $h$ units apart as a function of the lag $h$ [@Brockwell.Davis:2016, Definition 1.4.4].

For $y_1, \ldots, y_n$ and constant time lags $h=0, 1, \ldots$ units, the autocorrelation at lag $h$ is
\begin{align*}
r(h) = \frac{\gamma(h)}{\gamma(0)}, \qquad \gamma(h) = \frac{1}{n}\sum_{i=1}^{n-|h|} (y_i-\overline{y})(y_{i+h} - \overline{y})
\end{align*}

If the series is correlated, the sample autocorrelation will likely fall outside of the pointwise confidence intervals, as shown in @fig-correlogram. Presence of autocorrelation requires modelling the correlation  between observations explicitly using dedicated tools from the time series literature. We will however examine $\mathsf{AR}(1)$ models as part of the chapter on longitudinal data. See [Forecasting: Principles and Practice, section 5.3](https://otexts.com/fpp2/regression-evaluation.html) for more details.

When observations are positively correlated, the estimated standard errors reported by the software are too small. This means we are overconfident and will reject the null hypothesis more often then we should if the null is true (inflated Type I error, or false positive).

```{r}
#| label: fig-correlogram
#| echo: false
#| fig-cap: Correlogram of independent observations (left) and the ordinary residuals
#|   of the log-linear model fitted to the air passengers data (right). While the mean
#|   model of the latter is seemingly correctly specified, there is residual dependence
#|   between monthly observations and yearly (at lag 12). The blue lines give approximate
#|   pointwise 95\% confidence intervals for white noise (uncorrelated observations).
library(forecast)
set.seed(1234)
g0 <- ggAcf(rnorm(100),lag.max=25) + labs(x = "lag", y ="autocorrelation", title = "")+ ylim(c(-1,1))
g1 <- ggAcf(resid(ols_airpass),lag.max=25) + labs(x = "lag", y ="autocorrelation", title = "") + ylim(c(-1,1))
g0 + g1
```

### Linearity assumption

The second assumption of the linear model is that of  linearity, which means that the mean model is correctly specified, all relevant covariates have been included and their effect is correctly specified.
To check that the response surface of the linear model is adequate, we plot $e_i$ against $\widehat{y}_i$ or $x_{ij}$ (for $j=1, \ldots, p$). Since the linear correlation between $\boldsymbol{e}$ and $\widehat{\boldsymbol{y}}$ (or $\boldsymbol{e}$ and $\mathbf{X}_j$) is zero by construction, patterns (e.g., quadratic trend, cycles, changepoints) are indicative of misspecification of the mean model. One can add a smoother to detect patterns. @fig-regdiaglin shows three diagnostics plots, the second of which shows no pattern in the residuals, but skewed fitted values.

```{r}
#| label: fig-regdiaglin
#| echo: false
#| fig-cap: Scatterplots of residuals against fitted values. The first two plots show
#|   no departure from linearity (mean zero). The third plot shows a clear quadratic
#|   pattern, suggesting the mean model is misspecified. Note that the distribution of
#|   the fitted value need not be uniform, as in the second panel which shows more high
#|   fitted values.
par(pch = 19, col = scales::alpha("black", 0.8), bty = "l")
par(mar=c(3.1, 3.1, 1, 1), mgp=c(1.7, 0.6, 0), font.main=1, cex.main=0.8)
par(mfrow=c(1, 3))
x1 <- 1:200
x2 <- (1:200)^2
set.seed(1)
lm1 <- lm(rnorm(200)~x1)
lm2 <- lm(rnorm(200) ~ x2)
s1 <- scale(sin(x1/45)*2+rnorm(200))
lm3 <- lm(s1 ~ x1)
car::residualPlot(lm1,ylab="residuals", xlab = "fitted values", col.quad = hecblue)
car::residualPlot(lm2,ylab="residuals", xlab = "fitted values", col.quad = hecblue)
car::residualPlot(lm3,ylab="residuals", xlab = "fitted values", col.quad = hecblue)
```


If there is residual structure in plots of ordinary residuals against either (a) the fitted values or (b) the explanatory variables, a more complex model can be adjusted including interactions, nonlinear functions, \ldots If the effect of an explanatory variable is clearly nonlinear and complicated, smooth terms could be added (we won't cover generalized additive models in this course).


Plotting residuals against left-out explanatory variables can also serve to check that all of the explanatory power of the omitted covariate is already explained by the columns of $\mathbf{X}$.

If an important variable has been omitted and is not available in the dataset, then the effect of that variable is captured by both the errors (the portion orthogonal to the model matrix $\mathbf{X}$, i.e., unexplained by the covariates included in the model) and the remaining part is captured by other explanatories of the model that are correlated with the omitted variable. These variables can act as confounders. There is little that can be done in either case unless the data for the omitted variable are available, but subject-specific knowledge may help make sense of the results.

### Constant variance assumption

If the variance of the errors is the same for all observations (homoscedasticity), that of the observations $Y$ is also constant. The most common scenarios for heteroscedasticity are increases in variance with the response, or else variance that depends on explanatory variables $\mathbf{X}$, most notably categorical variables. For the former, a log-transform (or Box--Cox transformation) can help stabilize the variance, but we need the response to be positive. For the latter, we can explicitly model that variance and we will see how to include different variance per group later on. A popular strategy in the econometrics literature, is to use robust (inflated) estimators of the standard errors such as [White's sandwich estimator of the variance](https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors).

If the residuals (or observations) are heteroscedastic (non constant variance), the estimated effects of the variables (the $\beta$ parameters) are still valid in the sense that the ordinary least squares estimator $\widehat{\boldsymbol{\beta}}$ is unbiased. However, the estimated standard errors of the $\widehat{\beta}$ are no longer reliable and, consequently, the confidence intervals and the hypothesis tests for the model parameters will be incorrect. Indeed, if the variance of the errors differs from one observation to the next, we will estimate an average of the different variance terms. The standard errors of each term are incorrect (too small or too large) and the conclusions of the tests ($p$-values) will be off because the formulas of both $t$-test and $F$-test statistics include estimates of $\hat{\sigma}^2$.

Looking at the plot of jackknife studentized residuals against regressors (or fitted values) is instructive --- for example, we often see a funnel pattern when there is an increase in variance in the plot of the jackknife studentized residuals against fitted value, or else in boxplots with a categorical variable as in @fig-diagfitvalhomosce.
However, if we want to fit a local smoother to observe trends, it is better to plot the absolute value of the jackknife studentized residuals against regressors or observation number.

```{r}
#| label: fig-residhomoscedastic
#| echo: false
#| fig-cap: Plot of the absolute value of jackknife studentized residuals against observation
#|   number. The left panel is typical of homoscedastic data, whereas the right panel
#|   indicates an increase in the variance.
x1 <- 1:200
x2 <- (1:200)^2
set.seed(1)
lm1 <- lm(rnorm(200)~x1)
par(pch = 19, col = scales::alpha("black", 0.8), bty = "l")
par(mar=c(3.1, 3.1, 1, 1), mgp=c(1.7, 0.6, 0), font.main=1, cex.main=0.8)
par(mfrow=c(1, 2))
set.seed(1)
n <- length(x1)
y4 <- exp(-2+ 0.01*x1 + rnorm(n, sd = 0.25))
lm4 <- lm(y4 ~ x1)
plot(1:n, abs(rstudent(lm1)), xlab="observation number",
     ylab="|jackknife studentized residuals|")
car::gamLine(1:n, abs(rstudent(lm1)), spread = TRUE, col = hecblue)
# plot(1:n, abs(rstudent(lm3)), xlab="observation number", ylab="|jackknife studentized residuals|")
plot(1:n, abs(rstudent(lm4)), xlab="observation number", ylab="|jackknife studentized residuals|")
car::gamLine(1:n, abs(rstudent(lm4)), spread = TRUE, col = hecblue)
```


```{r}
#| label: fig-diagfitvalhomosce
#| echo: false
#| fig-cap: Plot of jackknife studentized residuals against fitted value (left) and categorical
#|   explanatory (right). Both clearly display heteroscedasticity.
par(mar=c(3.1,3.1,1.6,1),
    mgp=c(1.7,0.6,0),font.main=1,cex.main=0.8,
    mfrow= c(1,2),  pch = 19, bty = "l")
set.seed(1)
x1 <- sort(100+3*rnorm(200,0,10))
s1 <- scale(rnorm(200)*(x1-30)^2)
fithetero <- rstudent(lm(s1~x1))
plot(x1,s1,xlab="fitted values",ylab="jackknife studentized residuals")

set.seed(1)
fitlmh <- lm(c(rnorm(100, sd = 0.5), rnorm(100, sd = 3), rnorm(30, sd = 1.5)) ~ (xcat <- factor(c(rep(1,100), rep(2, 100), rep(3, 30)))))
boxplot(rstudent(fitlmh) ~ xcat, xlab = "group", ylab="jackknife studentized residuals")
```

An obvious extension of the linear model is to allow variance to vary according to explanatories, typically categorical covariates. In a likelihood framework, this is easy to do and we will cover this approach in more detail.

We can perform hypothesis tests for the homogeneity (equal) variance assumption. The most commonly used tests are Bartlett's test, the likelihood ratio test under the assumption of normally distributed data, with a Bartlett correction to improve the $\chi^2$ approximation to the null distribution. The second most popular is Levene's test (a more robust alternative, less sensitive to outliers). For both tests, the null distribution is $\mathscr{H}_0: \sigma^2_1 = \cdots = \sigma^2_K$ against the alternative that at least two differ. The Bartlett test statistic has a $\chi^2$ null distribution with $K-1$ degrees of freedom, whereas Levene's test has an $F$-distribution with ($K-1$, $n-K$) degrees of freedom: it is equivalent to computing the one-way ANOVA $F$-statistic with the absolute value of the centered residuals, $|y_{ik} - \widehat{\mu}_k|$, as observations.


What are the impacts of unequal variance if we use the $F$-test instead? For one, the pooled variance will be based on a weighted average of the variance in each group, where the weight is a function of the sample size. This can lead to size distortion (meaning that the proportion of type I error is not the nominal level $\alpha$ as claimed) and potential loss of power. The following toy example illustrates this.

:::{#exm-heterogeneity}

## Violation of the null hypothesis of equal variance


```{r}
#| label: fig-simuWelchnull
#| echo: false
#| cache: true
#| fig-cap: Histogram of the null distribution of $p$-values obtained through simulation
#|   using the classical analysis of variance $F$-test (left) and Welch's unequal variance
#|   alternative (right), based on 10 000 simulations. Each simulated sample consist
#|   of 50 observations from a $\mathsf{normal}(0, 1)$ distribution and 10 observations from
#|   $\mathsf{normal}(0, 9)$. The uniform distribution would have 5% in each of the 20 bins
#|   used for the display.
set.seed(1234)
n1 <- 50
n2 <- 10
mu1 <- 0
mu2 <- 0
sd1 <- 1
sd2 <- 3
nrep <- 1e4
pvalsF <- rep(0, nrep)
pvalsW <- rep(0, nrep)
group <- factor(c(rep(0, n1), rep(1, n2)))
for(i in seq_len(nrep)){
dat <- c(rnorm(n = n1, mean = mu1, sd = sd1),
         rnorm(n = n2, mean = mu2, sd = sd2))
pvalsW[i] <- t.test(dat ~ group)$p.value
pvalsF[i] <- t.test(dat ~ group, var.equal = TRUE)$p.value
}
g1 <- ggplot(data = tibble("pvalue" = pvalsF),
       aes(x = pvalue)) +
  # bin into 20 compartments,
  # specifying boundaries to avoid artifacts
  geom_histogram(breaks = seq(0, 1, by = 0.05),
                 aes(y = after_stat(width*density)),
                 alpha = 0.2) +
  stat_function(fun = function(x){1/20}, #uniform distribution
                col = "blue") +
  labs(x = "p-value",
       y = "percentage",
       caption = "two sample t-test (equal variance)") +
   scale_x_continuous(expand = c(0, 0),
                      limits = c(0, 1),
                      breaks = c(0,0.5,1))
g2 <- ggplot(data = tibble("pvalue" = pvalsW),
       aes(x = pvalue)) +
  # bin into 20 compartments,
  # specifying boundaries to avoid artifacts
  geom_histogram(breaks = seq(0, 1, by = 0.05),
                 aes(y = after_stat(width*density)),
                 alpha = 0.2) +
  stat_function(fun = function(x){0.05}, #uniform distribution
                col = "blue") +
  labs(x = "p-value",
       y = "percentage",
       caption = "Welch t-test (unequal variance)") +
   scale_x_continuous(expand = c(0, 0),
                      limits = c(0, 1),
                      breaks = c(0,0.5,1))
theme_set(theme_classic())
g1 + g2
```

We consider for simplicity a problem with $K=2$ groups, which is the two-sample $t$-test. We simulated 50 observations from a $\mathsf{normal}(0, 1)$ distribution and 10 observations from $\mathsf{normal}(0, 9)$, comparing the distribution of the $p$-values for the Welch and the $F$-test statistics. @fig-simuWelchnull shows the results. The percentage of $p$-values less than $\alpha=0.05$ based on 10 000  replicates is estimated to be `r 100*mean(pvalsW < 0.05)`% for the Welch statistic, not far from the level. By contrast, we reject `r 100*mean(pvalsF < 0.05)`% of the time with the one-way ANOVA global $F$-test: this is a large share of innocents sentenced to jail based on false premises! While the size distortion is not always as striking, heterogeneity should be accounted in the design by requiring sufficient sample sizes (whenever costs permits) in each group to be able to estimate the variance reliably and using an adequate statistic.

:::

There are alternative graphical ways of checking the assumption of equal variance, many including the standardized residuals $r_{ik} = (y_{ik} - \widehat{\mu}_k)/\widehat{\sigma}$ against the fitted values $\widehat{\mu}_k$. We will cover these in later sections.

Oftentimes, unequal variance occurs because the model is not additive. You could use variance-stabilizing transformations (e.g., log for multiplicative effects) to ensure approximately equal variance in each group. Another option is to use a model that is suitable for the type of response you have (including count and binary data). Lastly, it may be necessary to explicitly model the variance in more complex design (including repeated measures) where there is a learning effect over time and variability decreases as a result. Consult an expert if needed.


### Normality assumption

The normality assumption is mostly for convenience: if the errors are assumed normally distributed, then the least square and the maximum likelihood estimators of $\boldsymbol{\beta}$ coincide.
The maximum likelihood estimators of $\boldsymbol{\beta}$ are asymptotically normal under mild conditions on the model matrix and $t$-tests are surprisingly robust and unaffected by departure from the normality assumption. This means that inference is valid in large samples, regardless of the distribution of the errors/residuals (even if the null distribution are not exact). It is important to keep in mind that, for categorical explanatory variables, the sample size in each group must be sufficiently large for the central limit theorem to kick in since coefficients represent group average.

Sometimes, transformations can improve normality: if the data is right-skewed and the response is strictly positive, a log-linear model may be more adequate [@sec-transfo]. This can be assessed by looking at the quantile-quantile plot of the externally studentized residuals. If the response $Y$ is not continuous (including binary, proportion or count data), linear models give misleading answers and generalized linear models are more suitable.



The inference will be valid for large samples even if the errors are not normally distributed by virtue of the central limit theorem. If the errors $\varepsilon_i \sim \mathsf{normal}(0, \sigma^2)$, then the jacknnife studentized residuals should follow a Student distribution, with $r_i \sim \mathsf{Student}(n-p-2)$, (identically distributed, but not independent). A Student quantile-quantile plot can thus be used to check the assumption (and for $n$ large, the normal plotting positions could be used as approximation if $n-p> 50$). One can also plot a histogram of the residuals. Keep in mind that if the mean model is not correctly specified, some residuals may incorporate effect of leftover covariates.

```{r}
#| label: fig-qqplotresid
#| cache: true
#| echo: false
#| fig-cap: Histogram (left) and Student quantile-quantile plot (right) of the jackknife
#|   studentized residuals. The left panel includes a kernel density estimate (black),
#|   with the density of Student distribution (blue) superimposed. The right panel includes
#|   pointwise 95\% confidence bands calculated using a bootstrap.
par(pch = 19, col = scales::alpha("black", 0.8), bty = "l")
par(mar=c(3.1, 3.1, 1, 1), mgp=c(1.7, 0.6, 0), font.main=1, cex.main=0.8)
par(mfrow = c(1,2))
library(car)
library(qqplotr, warn.conflicts = FALSE)
set.seed(1234)
di <- "t"
dp <- list(df = lm2$df.residual)
de <- TRUE
g2 <- ggplot(data = data.frame(sample = rstudent(lm2)), mapping = aes(sample = sample)) +
 stat_qq_band(distribution = di, detrend = de, bandType = "boot", B = 9999, dparams = dp) +
 stat_qq_line(distribution = di, detrend = de) +
 stat_qq_point(distribution = di, detrend = de) +
 labs(x = "theoretical t quantiles", y = "jackknife studentized residuals\n minus theoretical quantiles")
g1 <- ggplot(data = data.frame(x = rstudent(lm2)), aes(x=x)) +
  geom_histogram(aes(x=x, y = after_stat(density))) +
  stat_function(fun = "dt", args = list(df = lm2$df.residual), col = hecblue) +
  geom_density() + labs(x = "jackknife studentized residuals")
g1 + g2
```


Quantile-quantile plots are discussed in @def-qqplot but their interpretation requires training. For example, @fig-qqplotsbad shows many common scenarios that can be diagnosed using quantile-quantile plots: discrete data is responsible for staircase patterns, positively skewed data has too high low quantiles and too low high quantiles relative to the plotting positions, heavy tailed data have high observations in either tails and bimodal data leads to jumps in the plot.

```{r}
#| label: fig-qqplotsbad
#| cache: true
#| echo: false
#| fig-cap: Quantile-quantile plots of non-normal data, showing typical look of behaviour
#|   of discrete (top left), heavy tailed  (top right), skewed (bottom left) and bimodal
#|   data (bottom right).
set.seed(1234)
par(pch = 19, col = scales::alpha("black", 0.8), bty = "l")
par(mar=c(3.1, 3.1, 1, 1), mgp=c(1.7, 0.6, 0), font.main=1, cex.main=0.8)
par(mfrow= c(2,2))
qqPlot(scale(rgeom(n = 100, prob = 0.5)),
       xlab = "theoretical quantiles",
       ylab = "empirical quantiles",
       col.lines=hecblue, id = FALSE)
set.seed(123)
qqPlot(scale(rt(200, df = 3)),
       xlab = "theoretical quantiles",
       ylab = "empirical quantiles",
       col.lines=hecblue, id = FALSE)
set.seed(432)
qqPlot(scale(rgamma(100,shape = 2)),
       xlab = "theoretical quantiles",
       ylab = "empirical quantiles",
       col.lines=hecblue, id = FALSE)
set.seed(432)
qqPlot(scale(c(rnorm(100,-3),rnorm(100,3))),
       xlab = "theoretical quantiles",
       ylab = "empirical quantiles",
       col.lines=hecblue, id = FALSE)
```

:::{#exm-diagplotcollege}

## Diagnostic plots for the $\texttt{college}$ data.

We can look at the `college` data to see if the linear model assumptions hold.


```{r}
#| label: fig-diagplotscollege
#| cache: true
#| echo: false
#| fig-cap: 'Diagnostic plots for the college data example: ordinary residuals against
#|   fitted values (top left), absolute value of the jacknnife studentized residuals
#|   against fitted values (top right), box and whiskers plot of jacknnife studentized
#|   residuals (bottom left) and detrended Student quantile-quantile plot (bottom right).
#|   There is clear group heteroscedasticity.'

library(qqplotr, warn.conflicts = FALSE)
lmcoll <- lm(salary ~ years + service + rank + sex + field, data = college)

resdat <- data.frame(rstudent = rstudent(lmcoll),
                     resid = resid(lmcoll),
                     fitted = fitted(lmcoll),
                     rank = college$rank,
                     service = college$service)
gd1 <- ggplot(resdat, aes(x = fitted, y = resid)) +
        geom_point() +
  labs(x = "fitted values", y = "ordinary residuals")
      # geom_smooth(method = "loess", size = 1.5)
gd2 <- ggplot(resdat, aes(x = fitted, y = abs(rstudent))) +
        geom_point() +
      labs(x = "fitted values",
           y = "|jackknife studentized residuals|")
gd3 <- ggplot(resdat, aes(x = rank, y = rstudent)) +
      geom_boxplot() + labs(y = "jackknife studentized residuals")
di <- "t"
dp <- list(df = lmcoll$df.residual-1)
de <- TRUE
gd4 <- ggplot(data = resdat, aes(sample = rstudent)) +
 stat_qq_band(distribution = di, dparams = dp,
              detrend = de, identity = TRUE,
              bandType = "boot", B = 9999) +
 stat_qq_line(distribution = di, dparams = dp,
              detrend = de, identity = TRUE) +
 stat_qq_point(distribution = di, dparams = dp,
               detrend = de, identity = TRUE) +
 labs(x = "theoretical quantiles",
      y = "empirical minus\n theoretical quantiles")

(gd1 + gd2) / (gd3 + gd4)
```

Based on the plots of @fig-diagplotscollege, we find that there is residual heteroscedasticity, due to rank. Since the number of years in the first rank is limited and all assistant professors were hired in the last six years, there is less disparity in their income. It is important not to mistake the pattern on the $x$-axis for the fitted value (due to the large effect of rank and field, both categorical variable) with patterns in the residuals (none apparent). Fixing the heteroscedasticity would correct the residuals and improve the appearance of the quantile-quantile plot.

:::


## Extensions of the model

### Transformation of the response {#sec-transfo}

If the response is strictly positive, there are some options that can alleviate lack of additivity, more specifically multiplicative mean-variance relationships.If the data is right-skewed and the response is strictly positive, a log-linear model may be more adequate and the parameters can be interpreted.


We can rewrite the log-linear model in the original response scale as
\begin{align*}
Y &= \exp\left(\beta_0+\sum_{j=1}^p\beta_jX_j +  \varepsilon \right) \\&= \exp\left(\beta_0+ \sum_{j=1}^p\beta_jX_j\right)\cdot \exp(\varepsilon),
\end{align*}
and thus
\begin{align*}
\mathsf{E}(Y \mid \mathbf{X}) = \exp(\beta_0 +\beta_1 X_1 +\cdots + \beta_pX_p) \times \mathsf{E}\{\exp(\varepsilon) \mid \mathbf{X}\}.
\end{align*}


If $\varepsilon \mid \mathbf{X} \sim \mathsf{normal}(\mu,\sigma^2)$, then $\mathsf{E}\{\exp(\varepsilon) \mid \mathbf{X}\}= \exp(\mu+\sigma^2/2)$ and $\exp(\varepsilon)$ follows a log-normal distribution.

An increase of one unit of $X_j$ leads to a $\beta_j$ increase of $\ln Y$ without interaction or nonlinear term for $X_j$, and this translates into a multiplicative increase of a factor $\exp(\beta_j)$ on the original data scale for $Y$. Indeed, we can compare the ratio of $\mathsf{E}(Y \mid X_1=x+1)$ to $\mathsf{E}(Y \mid X_1=x)$,
\begin{align*}
\frac{\mathsf{E}(Y \mid X_1=x+1, X_2, \ldots, X_p)}{\mathsf{E}(Y \mid X_1=x,  X_2, \ldots, X_p)} = \frac{\exp\{\beta_1(x+1)\}}{\exp(\beta_1 x)} = \exp(\beta_1).
\end{align*}
Thus, $\exp(\beta_1)$ represents the ratio of the mean of $Y$ when $X_1=x+1$ in comparison to that when $X_1=x$, *ceteris paribus* (and provided this statement is meaningful). If $\beta_j=0$, the multiplicative factor one is the identity, whereas negative values of the regression coefficient $\beta_j<0$ leads to $\exp(\beta_j)<1$. The percentage change is $1-\exp(\beta_j)$ if $\beta_j <0$ and $\exp(\beta_j)-1$ if $\beta_j>0$


Sometimes, we may wish to consider a log transformation of both the response and some of the continuous positive explanatories, when this make sense (a so-called log-log model). Consider the case where both $Y$ and $X_1$ is log-transformed, so the equation for the mean on the original data scale reads
\begin{align*}
Y= X_1^{\beta_1}\exp(\beta_0 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon)
\end{align*}
Taking the derivative of the left hand side with respect to $X_1>0$, we get
\begin{align*}
\frac{\partial Y}{\partial X_1}&= \beta_1 X_1^{\beta_1-1}\exp(\beta_0 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon)
\\&= \frac{\beta_1 Y}{X_1}
\end{align*}
and thus we can rearrange the expression so that
\begin{align*}
\frac{\partial X_1}{X_1}\beta_1 = \frac{\partial Y}{Y};
\end{align*}
this is a partial **elasticity**, so $\beta_1$ is interpreted as a $\beta_1$ percentage change in $Y$ for each percentage increase of $X_1$, *ceteris paribus*.

:::{#exm-loglog}

## Log-log model

Consider for example the Cobb--Douglas production function [@Douglas:1976], which specifies that economic output $Y$ is related to labour $L$ and capital $C$ via $\mathsf{E}(Y \mid L, C) = \beta_0C^{\beta}L^{1-\beta}$ with $\beta \in (0,1)$. If we take logarithms on both sides (since all arguments are positive), then 
$\mathsf{E}(\ln Y \mid L, C) = \beta_0^* + \beta_1 \ln C + (1-\beta_1)\ln L$. We could fit a linear model with response $\ln Y - \ln L$ and explanatory variable $\ln C - \ln L$, to obtain an estimate of the coefficient $\beta_1$, while $\beta_0^*=\ln \beta_0$. A constrained optimization would be potentially necessary to estimate the model parameters of the resulting linear model if the estimates lie outside of the parameter space.

:::

:::{#prp-boxcox}

## Box--Cox transformation

If the data are strictly positive, one can consider a Box--Cox transformation,
\begin{align*}
y(\lambda)= \begin{cases}
(y^{\lambda}-1)/\lambda, & \lambda \neq 0\\
\ln(y), & \lambda=0.
\end{cases}
\end{align*}
The cases $\lambda=-1$ (inverse), $\lambda=1$ (identity) and $\lambda=0$ (log-linear model) are perhaps the most important because they yield interpretable models.

If we assume that $\boldsymbol{Y}(\lambda) \sim \mathsf{normal}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}_n)$, then the likelihood is
\begin{align*}
L(\lambda, \boldsymbol{\beta}, \sigma; \boldsymbol{y}, \mathbf{X}) &= (2\pi\sigma^2)^{-n/2} J(\lambda, \boldsymbol{y}) \times\\& \quad \exp \left[ - \frac{1}{2\sigma^2}\{\boldsymbol{y}(\lambda) - \mathbf{X}\boldsymbol{\beta}\}^\top\{\boldsymbol{y}(\lambda) - \mathbf{X}\boldsymbol{\beta}\}\right],
\end{align*}
where $J$ denotes the Jacobian of the Box--Cox transformation, $J(\lambda, \boldsymbol{y})=\prod_{i=1}^n y_i^{\lambda-1}$.
For each given value of $\lambda$, the maximum likelihood estimator is that of the usual regression model, with $\boldsymbol{y}$ replaced by $\boldsymbol{y}(\lambda)$, namely $\widehat{\boldsymbol{\beta}}_\lambda = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y}(\lambda)$ and $\widehat{\sigma}^2_\lambda = n^{-1}\{ \boldsymbol{y}(\lambda) - \mathbf{X}\widehat{\boldsymbol{\beta}}_\lambda\}^\top\{ \boldsymbol{y}(\lambda) - \mathbf{X}\widehat{\boldsymbol{\beta}}_\lambda\}$.

The profile log likelihood is
\begin{align*}
\ell_{\mathsf{p}}(\lambda) = -\frac{n}{2}\ln(2\pi \widehat{\sigma}^2_\lambda) - \frac{n}{2} + (\lambda - 1)\sum_{i=1}^n \ln(y_i)
\end{align*}
The maximum profile likelihood estimator is the value $\lambda$ minimizes the sum of squared residuals from the linear model with $\boldsymbol{y}(\lambda)$ as response.

The Box--Cox is not a panacea and should be reserved to cases where the transformation reduces heteroscedasticity (unequal variance) or creates a linear relation between explanatories and response: theory provides a cogent explanation of the data. Rather than an *ad hoc* choice of transformation, one could choose a log transformation if the value $0$ is included within the 95% confidence interval since this improves interpretability.

:::



:::{#exm-poisonboxcox}

## Box--Cox transform for the `poison` data

@Box.Cox:1964 considered survival time for 48 animals based on a randomized trial; these data are analyzed in Example 8.25 of @Davison:2003. Three poisons were administered with four treatments; each factor combination contained four animals, chosen at random. There is strong evidence that both the choice of poison and treatment affect survival time.

We could consider a two-way analysis of variance model for these data without interaction, given the few observations for each combination. The model would be of the form
\begin{align*}
Y &= \beta_0 + \beta_1 \texttt{poison}_2 + \beta_2\texttt{poison}_3  +\beta_3\texttt{treatment}_2 \\ &\qquad+ \beta_4\texttt{treatment}_3
+\beta_5\texttt{treatment}_4 + \varepsilon
\end{align*}

The plot of fitted values against residuals shows that the model is not additive; there is also indications that the variance increases with the mean response. The model is inadequate: lowest survival times are underpredicted, meaning the residuals are positive and likewise the middle responses is positive. A formal test of non-additivity based on constructed variables  further point towards non-additivity [@Davison:2003, Example 8.24]. Overall, the model fit is poor and any conclusion drawn from it dubious.



One could consider using a Box--Cox to find a suitable transformation of the residuals so as to improve normality. An analysis of residuals in the top four plots of @fig-poisonplots show evidence of heteroscedasticity as a function of either poison and treatment. This is evident by looking at the plot of ordinary residuals, which displays increase in variance with the survival time. The quantile-quantile plot in the middle right plot shows some evidence of departure from the normality, but the non-linearity and heteroscedasticity obscure this. 

The bottom left panel of @fig-poisonplots shows the profile log likelihood for the Box--Cox transform parameter, suggesting a value of $\lambda=-1$ would be within the 95\% confidence interval. This choice has the benefit of being interpretable, as the reciprocal response $Y^{-1}$ corresponds to the speed of action of the poison depending on both poison type and treatment. The diagnostics plot at the bottom right of @fig-poisonplots also indicate that the model for the reciprocal has no residual structure and the variance appears constant. 

```{r}
#| label: fig-poisonplots
#| echo: false
#| eval: true
#| cache: true
#| fig-cap: "Diagnostic plots for the poison data: ordinary residuals (jittered)
#|   for the linear model for survival time as a function of poison and treatment (top), 
#|   fitted values against residuals (middle left), detrended quantile-quantile plot of residuals (middle right), profile log likelihood of $\\lambda$ for the Box--Cox model transformation (bottom left) and fitted values against residuals (bottom right) after reciprocal transformation."
#| fig-height: 6

poisons <- SMPracticals::poisons
poisonlm1 <- lm(time ~ poison + treat, data = poisons)
poisonlm2 <- lm(I(1/time) ~ poison + treat, data = poisons)

poisons$resid1 <- resid(poisonlm1)
poisons$rstudent1 <- rstudent(poisonlm1)
poisons$resid2 <- resid(poisonlm2)
poisons$rstudent2 <- rstudent(poisonlm2)
poisons$fitted1 <- fitted(poisonlm1)
poisons$fitted2 <- fitted(poisonlm2)
g1 <- ggplot(data = poisons) +
  geom_point(aes(x = poison, y = resid1), position = position_jitter(width = 0.05)) +
  labs(y = "ordinary residuals")
g2 <- ggplot(data = poisons) +
  geom_point(aes(x = treat, y = resid1), position = position_jitter(width = 0.05)) +
  labs(y = "ordinary residuals", x = "treatment")
g3 <- ggplot(data = poisons) +
  geom_point(aes(x = fitted1, y = resid1), position = position_jitter(width = 0.05)) +
  labs(y = "ordinary residuals", x = "fitted values")
dp <- list(df=poisonlm1$df.residual-1)
di <- "t"
de <- TRUE
library(qqplotr)
g4 <- ggplot(data = poisons, aes(sample = rstudent1)) +
 stat_qq_band(distribution = di, dparams = dp,
              detrend = de, identity = TRUE,
              bandType = "boot", B = 9999) +
 stat_qq_line(distribution = di, dparams = dp,
              detrend = de, identity = TRUE) +
 stat_qq_point(distribution = di, dparams = dp,
               detrend = de, identity = TRUE) +
 labs(x = "theoretical quantiles",
      y = "empirical minus\n theoretical quantiles")
g5 <- ggplot(data = poisons) +
  geom_point(aes(x = fitted2, y = resid2), position = position_jitter(width = 0.05)) +
  labs(y = "ordinary residuals", x = "fitted values")
boxcox_gg <- function(fitted.lm, showlambda = TRUE, lambdaSF = 3, grid = seq(-2,2, by = 0.1), scale.factor = 0.5) {
      boxcox_object <- MASS::boxcox(fitted.lm, lambda = grid, plotit = FALSE)
    x <- unlist(boxcox_object$x)
    y <- unlist(boxcox_object$y)
    xstart <- x[-1]
    ystart <- y[-1]
    xend <- x[-(length(x))]
    yend <- y[-(length(y))]
    boxcox_unlist <- data.frame(xstart, ystart, xend, yend)
    best_lambda <- x[which.max(y)]
    rounded_lambda <- round(best_lambda, lambdaSF)
    min_y <- min(y)
    accept_inds <- which(y > max(y) - 1/2 * qchisq(0.95, 1))
    accept_range <- x[accept_inds]
    conf_lo <- round(min(accept_range), lambdaSF)
    conf_hi <- round(max(accept_range), lambdaSF)
    plot <- ggplot(data = boxcox_unlist) + geom_segment(aes(x = xstart,
        y = ystart, xend = xend, yend = yend), size = scale.factor) +
        labs(x = expression(lambda), y = "profile log likelihood") +
        geom_vline(xintercept = best_lambda, linetype = "dotted",
            size = scale.factor/2) + geom_vline(xintercept = conf_lo,
        linetype = "dotted", size = scale.factor/2) + geom_vline(xintercept = conf_hi,
        linetype = "dotted", size = scale.factor/2) + geom_hline(yintercept = y[min(accept_inds)],
        linetype = "dotted", size = scale.factor/2)
    if (showlambda) {
        return(plot +
                 annotate("text", x = best_lambda, label = as.character(rounded_lambda), y = min_y) +
              annotate("text", x = conf_lo, label = as.character(conf_lo), y = min_y) +
              annotate("text", x = conf_hi,
            label = as.character(conf_hi), y = min_y))
    } else {
        return(plot)
    }
}
g6 <- boxcox_gg(poisonlm1, grid = seq(-1.5,0.1, by = 0.01))
(g1 + g2)/ (g3 + g4) / (g6 + g5)
```
:::




## Concluding remarks


Linear regression is the most famous and the most widely used statistical model around.  The name may appear reductive, but many tests statistics (*t*-tests, ANOVA, Wilcoxon, Kruskal--Wallis) [can be formulated using a linear regression](https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf), while [models as diverse as trees, principal components and deep neural networks are just linear regression model in disguise](https://threadreaderapp.com/thread/1286420597505892352.html). What changes under the hood between one fancy model to the next are the optimization method (e.g., ordinary least squares, constrained optimization or stochastic gradient descent) and the choice of explanatory variables entering the model (spline basis for nonparametric regression, indicator variable selected via a greedy search for trees, activation functions for neural networks).

<!--


:::{#rem-geometry}

## Géométrie

Le vecteur de valeurs ajustées $\widehat{\boldsymbol{y}} =\mathbf{X} \widehat{\boldsymbol{\beta}} = \mathbf{H}_{\mathbf{X}}\boldsymbol{y}$ est la projection du vecteur réponse $\boldsymbol{y}$ dans l'espace linéaire engendré par les colonnes de $\mathbf{X}$. La matrice chapeau $\mathbf{H}_{\mathbf{X}} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top$ est une matrice de projection orthogonale, car $\mathbf{H}_{\mathbf{X}}=\mathbf{H}_{\mathbf{X}}^\top$ et $\mathbf{H}_{\mathbf{X}}\mathbf{H}_{\mathbf{X}} = \mathbf{H}_{\mathbf{X}}$. Ainsi, $\mathbf{H}_{\mathbf{X}}\mathbf{X} = \mathbf{X}$. Puisque le vecteur de résidus ordinaires $\boldsymbol{e} = (e_1, \ldots, e_n)^\top$,  qui apparaît dans la somme des erreurs quadratiques, est définie comme  $\boldsymbol{y} - \widehat{\boldsymbol{y}}$ et $\widehat{\boldsymbol{y}}=\mathbf{X}\boldsymbol{\beta}$, de simples manipulations algébriques montrent que le produit scalaire entre les résidus ordinaires et les valeurs ajustées est nul, puisque
\begin{align*}
\widehat{\boldsymbol{y}}^\top\boldsymbol{e} &= \widehat{\boldsymbol{\beta}}^\top \mathbf{X}^\top (\boldsymbol{y}- \mathbf{X} \widehat{\boldsymbol{\beta}})
\\&= \boldsymbol{y}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top(\boldsymbol{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y})\\&=\boldsymbol{y}^\top\mathbf{H}_{\mathbf{X}}\boldsymbol{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y}
\\&= 0
\end{align*}
où nous utilisons la définition de $\widehat{\boldsymbol{y}}$ et $\boldsymbol{e} = \boldsymbol{y} - \widehat{\boldsymbol{y}}$ sur la première ligne, puis on substitut l'estimateur des MCO $\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{y}$ avant de distribuer les termes du produit. Une dérivation similaire montre que $\mathbf{X}^\top\boldsymbol{e}=\boldsymbol{0}_{p+1}$. Les résidus ordinaires sont donc orthogonaux à la fois à la matrice du modèle $\mathbf{X}$ et aux valeurs ajustées $\widehat{\boldsymbol{y}}$.

Une conséquence directe de ces résultats est le fait que la corrélation linéaire entre $\boldsymbol{e}$ et $\widehat{\boldsymbol{y}}$ est nulle. Cette propriété servira lors de l'élaboration de diagnostics graphiques.

Puisque le produit scalaire est zéro, la moyenne de $\boldsymbol{e}$ doit être zéro pour autant que $\mathbf{1}_n$ est dans l'espace linéaire engendré par $\mathbf{X}$.


:::


:::{#rem-invariance}

## Invariance

Une conséquence directe de l'expression de l'estimateur des MCO en terme de matrice de projection est que les valeurs ajustées $\widehat{y}_i$ pour deux matrices de modèle $\mathbf{X}_a$ et $\mathbf{X}_b$ sont les mêmes si elles engendrent le même espace linéaire, comme dans @exm-baumann-dummies; seule l''interprétation des coefficients change. Si nous incluons une ordonnée à l'origine, nous obtenons le même résultat si les colonnes explicatives sont centrées sur la moyenne.

:::


La valeur de $\widehat{\boldsymbol{\beta}}$ est telle qu'elle maximise la corrélation entre $\boldsymbol{y}$ et $\widehat{\boldsymbol{y}}$. Dans le cas d'une variable catégorielle unique, nous obtiendrons des valeurs ajustées $\widehat{y}$ qui correspondent à la moyenne de l'échantillon de chaque groupe.



:::{#def-r2}

## Coefficient de détermination {#coefR2}


Lorsque nous spécifions un modèle, les aléas  $\boldsymbol{\varepsilon}$ servent à tenir compte du fait qu'aucune relation linéaire exacte ne caractérise les données  Une fois que nous avons ajusté un modèle, nous estimons la variance $\sigma^2$; on peut alors se demander quelle part de la variance totale de l'échantillon est expliquée par le modèle.

La somme totale des carrés, définie comme la somme des carrés des résidus du modèle à ordonnée à l'origine uniquement, sert de comparaison --- le modèle le plus simple que nous puissions trouver impliquerait chaque observation par la moyenne de l'échantillon de la réponse, ce qui donne la variance expliquée $\mathsf{SC}_c = \sum_{i=1}^n (y_i - \overline{y})^2$. Nous pouvons ensuite comparer la variance des données originales avec celle des résidus du modèle avec la matrice de covariables $\mathbf{X}$, définie comme $\mathsf{SC}_e =\sum_{i=1}^n e_i^2$ avec $e_i = y_i - \widehat{\beta}_0 - \sum_{j=1}^p \widehat{\beta}_jX_j$.
Nous définissons le coefficient de détermination $R^2$, comme suit
\begin{align*}
R^2 &=1- \frac{\mathsf{SC}_e}{\mathsf{SC}_c} = \frac{\sum_{i=1}^n (y_i - \overline{y})^2- \sum_{i=1}^n e_i^2}{\sum_{i=1}^n (y_i - \overline{y})^2}.
\end{align*}
Une autre décomposition montre que $R^2 = \mathsf{cor}^2(\boldsymbol{y}, \widehat{\boldsymbol{y}})$, c'est-à-dire que le coefficient de détermination peut être interprété comme le carré de la corrélation linéaire de Pearson (@def-correlation-Pearson) entre la réponse $\boldsymbol{y}$ et les valeurs ajustées $\widehat{\boldsymbol{y}}$.

:::

Il est important de noter que le $R^2$ n'est pas un critère de qualité de l'ajustement, tout comme la log-vraisemblance. En effet, certain phénomènes sont intrinsèquement complexes et même un bon modèle ne parviendra pas à rendre compte d'une grande partie de la variabilité de la réponse. Ce n'est pas non plus parce que le $R^2$ est faible que $Y$ et et les variables explicatives $X_j$ sont indépendantes, comme l'illustre la @fig-datasaurus.


En outre, il est possible de gonfler la valeur de $R^2$ en incluant davantage de variables explicatives et en rendant le modèle plus complexe, ce qui améliore la vraisemblance et $R^2$. En effet, le coefficient n'est pas décroissant dans la dimension de $\mathbf{X}$, de sorte qu'un modèle comportant $p+1$ de covariables aura nécessairement des valeurs de $R^2$ plus élevées que si l'on n'incluait que $p$ de ces variables explicatives. Pour comparer les modèles, il est préférable d'utiliser des critères d'information ou de s'appuyer sur la performance prédictive si tel est l'objectif de la régression. Enfin, un modèle avec un $R^2$ élevé peut impliquer une corrélation élevée, mais [la relation peut être fallacieuse](http://www.tylervigen.com/spurious-correlations): la régression linéaire ne produit pas de modèles causaux!


-->
