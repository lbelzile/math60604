<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Régression linéaire | Modélisation statistique</title>
  <meta name="description" content="Ces notes forment un complément web du cours MATH 60604 (Modélisation statistique) offert à la M.Sc. en gestion (science des données et analytique d’affaires) à HEC Montréal." />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Régression linéaire | Modélisation statistique" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Ces notes forment un complément web du cours MATH 60604 (Modélisation statistique) offert à la M.Sc. en gestion (science des données et analytique d’affaires) à HEC Montréal." />
  <meta name="github-repo" content="lbelzile/math60604" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Régression linéaire | Modélisation statistique" />
  
  <meta name="twitter:description" content="Ces notes forment un complément web du cours MATH 60604 (Modélisation statistique) offert à la M.Sc. en gestion (science des données et analytique d’affaires) à HEC Montréal." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="vraisemblance.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.2/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modélisation statistique</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Remarques</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction à l’inférence statistique</a></li>
<li class="chapter" data-level="2" data-path="regression-lineaire.html"><a href="regression-lineaire.html"><i class="fa fa-check"></i><b>2</b> Régression linéaire</a></li>
<li class="chapter" data-level="3" data-path="vraisemblance.html"><a href="vraisemblance.html"><i class="fa fa-check"></i><b>3</b> Inférence basée sur la vraisemblance</a></li>
<li class="chapter" data-level="4" data-path="modeles-lineaires-generalises.html"><a href="modeles-lineaires-generalises.html"><i class="fa fa-check"></i><b>4</b> Modèles linéaires généralisés</a></li>
<li class="chapter" data-level="5" data-path="donnees-correlees-longitudinales.html"><a href="donnees-correlees-longitudinales.html"><i class="fa fa-check"></i><b>5</b> Données corrélées et longitudinales</a></li>
<li class="chapter" data-level="6" data-path="modeles-lineaires-mixtes.html"><a href="modeles-lineaires-mixtes.html"><i class="fa fa-check"></i><b>6</b> Modèles linéaires mixtes</a></li>
<li class="chapter" data-level="7" data-path="survie.html"><a href="survie.html"><i class="fa fa-check"></i><b>7</b> Analyse de survie</a></li>
<li class="appendix"><span><b>Annexe</b></span></li>
<li class="chapter" data-level="A" data-path="complement.html"><a href="complement.html"><i class="fa fa-check"></i><b>A</b> Compléments mathématiques</a></li>
<li class="chapter" data-level="B" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>B</b> Dérivations mathématiques</a></li>
<li class="chapter" data-level="" data-path="r.html"><a href="r.html"><i class="fa fa-check"></i><strong>R</strong></a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publié avec bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modélisation statistique</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-lineaire" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Régression linéaire<a href="regression-lineaire.html#regression-lineaire" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>On entend par régression linéaire un modèle pour l’espérance conditionnelle d’une variable réponse <span class="math inline">\(Y\)</span> (ou régressande) en fonction de <span class="math inline">\(p\)</span> variables explicatives (appelées parfois régresseurs ou covariables) à l’aide d’une équation de la forme
<span class="math display">\[\begin{align*}
\mathsf{E}(Y \mid \mathbf{X})=\beta_0 + \beta_1\mathrm{X}_{1} + \cdots + \beta_p \mathrm{X}_{p}.
\end{align*}\]</span>
Le fait que la moyenne est conditionnelle aux valeurs de <span class="math inline">\(\mathbf{X}\)</span> implique simplement que l’on considère les régresseurs comme constant, ou connus à l’avance.</p>
<p>En pratique, tout modèle est une approximation de la réalité, aussi on ajoute un terme d’erreur qui sert à tenir compte du fait qu’aucune relation linéaire exacte ne lie <span class="math inline">\(\mathbf{X}\)</span> et <span class="math inline">\(Y\)</span>, ou que les mesures de <span class="math inline">\(Y\)</span> contiennent des erreurs. Ce terme d’erreur aléatoire <span class="math inline">\(\varepsilon\)</span> servira de base à l’inférence car il permettra de quantifier l’adéquation entre notre modèle et les données.</p>
<p>On peut réécrire le modèle linéaire en terme de l’erreur pour un échantillon aléatoire de taille <span class="math inline">\(n\)</span>: dénotons par <span class="math inline">\(Y_i\)</span> la valeur de <span class="math inline">\(Y\)</span> pour le sujet <span class="math inline">\(i\)</span>, et <span class="math inline">\(\mathrm{X}_{ij}\)</span> la valeur de la <span class="math inline">\(j\)</span>e variable explicative du sujet <span class="math inline">\(i\)</span>. Le modèle de régression linéaire est
<span class="math display" id="eq:olsmean">\[\begin{align}
Y_i = \beta_0 + \beta_1 \mathrm{X}_{i1} + \ldots + \beta_p \mathrm{X}_{ip} +\varepsilon_{i}, \qquad i =1, \ldots, n, \tag{2.1}
\end{align}\]</span>
où <span class="math inline">\(\varepsilon_i\)</span> est le terme d’erreur additive. Si aucune hypothèse sur la loi aléatoire de l’erreur n’est spécifiée, on fixe néanmoins l’espérance du terme d’erreur à zéro car on postule qu’il n’y a pas d’erreur systématique, c’est-à-dire que <span class="math inline">\(\mathsf{E}(\varepsilon_i \mid \boldsymbol{X}_i)=0\)</span> <span class="math inline">\((i=1, \ldots, n)\)</span>.</p>
<p>La flexibilité du modèle linéaire vient de sa formulation: on spécifie l’espérance conditionnelle d’une variable continue comme <strong>combinaison linéaire de variables explicatives</strong>, dont le choix est arbitraire.
Il est important de remarquer que ce modèle est linéaire dans les coefficients <span class="math inline">\(\boldsymbol{\beta}\in \mathbb{R}_{p+1}\)</span>, pas dans les variables explicatives! les covariables sont quelconques et peuvent être des fonctions (non)-linéaires d’autres variables explicatives, par exemple <span class="math inline">\(\mathrm{X}=\log(\texttt{annees})\)</span>, <span class="math inline">\(\mathrm{X}=\texttt{puissance}^2\)</span> ou <span class="math inline">\(\mathrm{X}= \mathsf{I}_{\texttt{homme}}\cdot\mathsf{I}_{\texttt{titulaire}}\)</span>. C’est ce qui fait la flexibilité du modèle linéaire: ce dernier est principalement employé aux fins suivantes:</p>
<ol style="list-style-type: decimal">
<li>Comprendre comment et dans quelle mesure les variables explicatives <span class="math inline">\(\mathbf{X}\)</span> influencent la moyenne de la réponse <span class="math inline">\(Y\)</span> (description).</li>
<li>Quantifier l’influence des variables explicatives <span class="math inline">\(\mathbf{X}\)</span> sur la régressande <span class="math inline">\(Y\)</span> et tester leur significativité.</li>
<li>Prédire les valeurs de <span class="math inline">\(Y\)</span> pour de nouveaux ensembles de covariables <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ol>
<div id="introduction" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Introduction<a href="regression-lineaire.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Le modèle linéaire est sans conteste le modèle statistique le plus couramment employé. Le terme « modèle linéaire » est trompeur: une grande panoplie de tests statistiques (tests-<em>t</em>, analyse de variance, test de Wilcoxon ou de Kruskal–Wallis) <a href="https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf">peut être calculée à l’aide d’un modèle linéaire</a>, tandis que <a href="https://threadreaderapp.com/thread/1286420597505892352.html">des modèles aussi divers que les arbres aléatoires, la régression en composantes principales et les réseaux de neurones multicouches ne sont en réalité que de bêtes modèles linéaires</a>. Ce qui change d’un modèle à l’autre est simplement la méthode d’optimisation (moindres carrés ordinaires, optimisation sous contrainte ou par descente de gradient stochastique), de même que le choix des variables explicatives (bases de spline pour la régression nonparamétrique, variables indicatrices pour les arbres, fonctions d’activations pour les réseaux de neurones). Ce chapitre porte sur la formulation de modèles linéaires, l’interprétation des coefficients et les tests usuels reliés à ces modèles. Certains modèles bien connus, comme l’analyse de variance, seront présentés comme cas spéciaux du modèle de régression linéaire.</p>
<p>Afin de rendre plus tangible le concept et les notions qui touchent aux modèles linéaires, on présentera ces notions dans le cadre d’un exemple. On s’intéresse à la discrimination salariale dans un collège américain, au sein duquel une étude a été réalisée pour investiguer s’il existait des inégalités salariales entre hommes et femmes. Le jeu de données contient les variables suivantes</p>
<ul>
<li><code>salaire</code>: salaire de professeurs pendant l’année académique 2008–2009 (en milliers de dollars USD).</li>
<li><code>echelon</code>: échelon académique, soit adjoint (<code>adjoint</code>), aggrégé (<code>aggrege</code>) ou titulaire (<code>titulaire</code>).</li>
<li><code>domaine</code>: variable catégorielle indiquant le champ d’expertise du professeur, soit appliqué (<code>applique</code>) ou théorique (<code>theorique</code>).</li>
<li><code>sexe</code>: indicateur binaire pour le sexe, <code>homme</code> ou <code>femme</code>.</li>
<li><code>service</code>: nombre d’années de service.</li>
<li><code>annees</code>: nombre d’années depuis l’obtention du doctorat.</li>
</ul>
<p>Une analyse exploratoire des données est de mise avant d’ébaucher un modèle. Si le salaire augmente au fil des ans, on voit que l’hétérogénéité change en fonction de l’échelon et qu’il y a une relation claire entre ce dernier et le nombre d’années de service (les professeurs n’étant éligibles à des promotions qu’après un certain nombre d’années). Les professeurs adjoints qui ne sont pas promus sont généralement mis à la porte, aussi il y a moins d’occasions pour que les salaires varient sur cette échelle.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:edacollege"></span>
<img src="MATH60604_Modelisation_statistique_files/figure-html/edacollege-1.png" alt="Analyse exploratoire des données $\texttt{college}$: répartition des salaires en fonction de l'échelon et du nombre d'années de service" width="70%" />
<p class="caption">
Figure 2.1: Analyse exploratoire des données <span class="math inline">\(\texttt{college}\)</span>: répartition des salaires en fonction de l’échelon et du nombre d’années de service
</p>
</div>
Ainsi, le salaire augmente avec les années, mais la variabilité croît également. Il y a peu de femmes dans l’échantillon: moins d’information signifie moins de puissance pour détecter de petites différences de salaire. Si on fait un tableau de contingence de l’échelon et du sexe, on peut calculer la proportion relative homme/femme dans chaque échelon: 16% des profs adjoints, 16% pour les aggrégés, mais seulement 7% des titulaires alors que ces derniers sont mieux payés en moyenne.
<table>
<caption>
<span id="tab:tableaucontingence">Tableau 2.1: </span>Tableau de contingence donnant le nombre de professeurs du collège par sexe et par échelon académique.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
adjoint
</th>
<th style="text-align:right;">
aggrege
</th>
<th style="text-align:right;">
titulaire
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
femme
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
18
</td>
</tr>
<tr>
<td style="text-align:left;">
homme
</td>
<td style="text-align:right;">
56
</td>
<td style="text-align:right;">
54
</td>
<td style="text-align:right;">
248
</td>
</tr>
</tbody>
</table>
<p>Le modèle linéaire simple n’inclut qu’une variable explicative et consiste en une droite d’équation <span class="math inline">\(y=\beta_0 + \beta_1 \mathrm{X}\)</span> qui passe à travers un nuage de points. La Figure <a href="regression-lineaire.html#fig:droitenuage">2.2</a> montre la droite de régression dans le nuage de points formé par les couples <span class="math inline">\(\{\mathrm{X}_i, y_i\}\)</span>, où <span class="math inline">\(y_i\)</span> est le <code>salaire</code> et <span class="math inline">\(\mathrm{X}\)</span> est <code>service</code>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:droitenuage"></span>
<img src="MATH60604_Modelisation_statistique_files/figure-html/droitenuage-1.png" alt="Régression linéaire simple pour le salaire en fonction des années de service; la droite satisfait le critère des moindres carrés." width="70%" />
<p class="caption">
Figure 2.2: Régression linéaire simple pour le salaire en fonction des années de service; la droite satisfait le critère des moindres carrés.
</p>
</div>
<p>Une infinité de droites pourraient passer dans le nuage de points; il faut donc choisir la meilleure droite (selon un critère donné). La section aborde le choix de ce critère et l’estimation des paramètres de l’équation de la droite.</p>
<!-- ### Géométrie du modèle linéaire -->
<!-- ```{r geometriecolonne, echo = FALSE, fig.cap = "Géométrie des colonnes: le vecteur réponse est formé par une combinaison linéaire de variables explicatives, auxquels on ajoute le vecteur d'aléas. Les deux sont inconnus, aussi on choisit la projection linéaire dans l'espace formé par les variables explicatives."} -->
<!-- knitr::include_graphics('images/Illustration_orthogonal_decompo.png') -->
<!-- ``` -->
<!-- Considérons un échantillon de $n$ observations. On n'observe ni les erreurs $\boldsymbol{\varepsilon}$, ni les paramètres $\boldsymbol{\beta}$: il est donc impossible de recouvrer les (vrais) coefficients du modèle: le système d'équation spécifié par l'équation \ref{eq:olsmean} inclut $n+p+1$ inconnues, mais uniquement $n$ observations. -->
</div>
<div id="moindres-carrés-ordinaires" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Moindres carrés ordinaires<a href="regression-lineaire.html#moindres-carrés-ordinaires" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Les estimateurs des moindres carrés ordinaires <span class="math inline">\(\widehat{\boldsymbol{\beta}}=(\widehat{\beta}_0, \ldots, \widehat{\beta}_p)\)</span> sont les paramètres qui minimisent simultanément la distance euclidienne entre les observations <span class="math inline">\(Y_i\)</span> et les <strong>valeurs ajustées</strong>
<span class="math display">\[\begin{align*}
\widehat{Y}_i &amp;= \widehat{\beta}_0 + \widehat{\beta}_1 \mathrm{X}_{i1} + \cdots + \widehat{\beta}_p \mathrm{X}_{ip}, \qquad i =1, \ldots, n.
\end{align*}\]</span>
En d’autres mots, les estimateurs des moindres carrés sont la solution du problème d’optimization convexe
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}} &amp;=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}\sum_{i=1}^n (Y_i-\widehat{Y}_i)^2= \min_{\boldsymbol{\beta}} \|\boldsymbol{Y}-\mathbf{X}\boldsymbol{\beta}\|^2
\end{align*}\]</span>
Ce système d’équation a une solution explicite qui est plus facilement exprimée en notation matricielle. Soit les matrices et vecteurs
<span class="math display">\[\begin{align*}
\boldsymbol{Y} =
\begin{pmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
\end{pmatrix} ,
\;
\boldsymbol{\varepsilon} =
\begin{pmatrix}
  \varepsilon_1 \\
  \varepsilon_2 \\
  \vdots \\
  \varepsilon_n
\end{pmatrix} ,
\;
\mathbf{X} = \begin{pmatrix}
\mathrm{X}_{11} &amp; \mathrm{X}_{12} &amp; \cdots &amp; \mathrm{X}_{1p} \\
\mathrm{X}_{21} &amp; \mathrm{X}_{22} &amp; \cdots &amp; \mathrm{X}_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathrm{X}_{n1} &amp; \mathrm{X}_{n2} &amp; \cdots &amp; \mathrm{X}_{np}
\end{pmatrix} , \;
\boldsymbol{\beta} =
\begin{pmatrix}
  \beta_1 \\
  \beta_2 \\
  \vdots \\
  \beta_p
\end{pmatrix}
\end{align*}\]</span>
Le modèle en notation matricielle s’écrit de manière compacte, <span class="math display">\[\begin{align*}
\boldsymbol{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon};
\end{align*}\]</span>
chaque ligne de la matrice correspond à l’équation <a href="regression-lineaire.html#eq:olsmean">(2.1)</a> avec une observation par ligne.
L’estimateur des moindres carrés ordinaires résoud le problème d’optimisation non-contraint
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}}=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}\]</span>
Une preuve est fournie <a href="math.html#ols">dans l’Annexe</a>. Si le rang de la matrice <span class="math inline">\(\mathbf{X}\)</span> est dimension <span class="math inline">\(n \times (p+1)\)</span> est de rang <span class="math inline">\(p+1\)</span>, l’unique solution du problème d’optimisation est
<span class="math display" id="eq:ols">\[\begin{align}
\widehat{\boldsymbol{\beta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \boldsymbol{Y}. \tag{2.2}
\end{align}\]</span></p>
<p>Que représente les moindres carrés en deux dimensions? L’estimateur est celui qui minimise la somme du carré des résidus ordinaires. Le <span class="math inline">\(i\)</span>e <strong>résidu ordinaire</strong> <span class="math inline">\(e_i = y_i -\widehat{y}_i\)</span> est la distance <em>verticale</em> entre un point <span class="math inline">\(y_i\)</span> et la valeur ajustée <span class="math inline">\(\widehat{y}_i\)</span>, soit les traits bleus de la Figure <a href="regression-lineaire.html#fig:distancevert">2.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:distancevert"></span>
<img src="MATH60604_Modelisation_statistique_files/figure-html/distancevert-1.png" alt="Illustration des résidus ordinaires ajoutés à la droite de régression." width="70%" />
<p class="caption">
Figure 2.3: Illustration des résidus ordinaires ajoutés à la droite de régression.
</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-1" class="remark"><em>Remarque</em> (Géométrie des moindres carrés). </span>Si on considère les <span class="math inline">\(n\)</span> observations comme un vecteur (colonne), le terme <span class="math inline">\(\mathbf{X} \widehat{\boldsymbol{\beta}}\)</span> correspond à la projection sur l’espace linéaire engendré par les colonnes de la matrice <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathscr{S}_{\mathbf{X}}\)</span> du vecteur de réponse <span class="math inline">\(\boldsymbol{y}\)</span>. Les résidus ordinaires sont donc orthogonaux à <span class="math inline">\(\mathscr{S}_{\mathbf{X}}\)</span> par construction et les résidus sont orthogonaux aux valeurs ajustées, <span class="math inline">\(\boldsymbol{e}^\top\widehat{\boldsymbol{y}}=0\)</span>.
Une conséquence directe est que la corrélation linéaire entre <span class="math inline">\(\boldsymbol{e}\)</span> et <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> est zéro; cette propriété nous servira dans les diagnostics graphiques.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-2" class="remark"><em>Remarque</em> (Complexité du calcul des moindres carrés ordinaires). </span>Tangente: en apprentissage automatique, on utilise souvent un algorithme du gradient (stochastique) pour obtenir les estimés des moindres carrés ordinaires. Or, à moins d’avoir des tailles d’échantillons <span class="math inline">\(n\)</span> ou un nombre de covariables <span class="math inline">\(p\)</span> subséquent (pensez échelle Google), une solution approximative ne devrait pas être préférée à la solution exacte! D’un point de vue numérique, l’opération la plus coûteuse est le calcul de l’inverse de la matrice <span class="math inline">\(\mathbf{X}^\top\mathbf{X}\)</span>, qui de dimension <span class="math inline">\((p+1) \times (p+1)\)</span>. Règle générale, on n’inverse pas directement cette matrice car ce n’est pas la façon la plus numériquement stable d’obtenir la solution. <strong>R</strong> utilise la décomposition QR qui a une complexité de <span class="math inline">\(\mathrm{O}(np^2)\)</span> (l’ordre du nombre de flops ou d’opérations pour le calcul). Une alternative plus coûteuse, mais plus stable numériquement, est la décomposition en valeurs singulières (même ordre en terme de calculs).</p>
</div>
<p>Trève de disgression mathématique: tout bon logiciel calculera pour vous les estimés des moindres carrés. Retenez que l’on minimise une forme quadratique qui admet une solution explicite et unique pour autant que les colonnes de <span class="math inline">\(\mathbf{X}\)</span> ne soient pas colinéaires. Si vous avez plus d’une variable explicative, les valeurs ajustées seront situées sur un hyperplan (peu commode à représenter graphiquement). Maîtriser le langage associé à la régression (notamment les résidus ordinaires, les valeurs ajustées, etc.) est nécessaire pour la continuation.</p>
</div>
<div id="interprétation-des-paramètres-du-modèles" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Interprétation des paramètres du modèles<a href="regression-lineaire.html#interprétation-des-paramètres-du-modèles" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Que représentent les paramètres <span class="math inline">\(\boldsymbol{\beta}\)</span> du modèle linéaire? Dans le cas simple présenté dans la Figure <a href="regression-lineaire.html#fig:droitenuage">2.2</a> où l’équation de la droite est de la forme <span class="math inline">\(\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1\mathrm{X}_1\)</span>, <span class="math inline">\(\beta_0\)</span> est l’ordonnée à l’origine (la valeur moyenne de <span class="math inline">\(Y\)</span> quand <span class="math inline">\(\mathrm{X}_1=0\)</span>) et <span class="math inline">\(\beta_1\)</span> est la pente, soit l’augmentation moyenne de <span class="math inline">\(Y\)</span> quand <span class="math inline">\(\mathrm{X}_1\)</span> augmente d’une unité.</p>
<p>Dans certains cas, l’interprétation de l’ordonnée à l’origine n’est pas valide car c’est un <strong>non-sens</strong>: la valeur <span class="math inline">\(\mathrm{X}_1=0\)</span> n’est pas plausible (par exemple, si <span class="math inline">\(\mathrm{X}_1\)</span> est la taille d’un humain). De même, il peut arriver qu’il n’y ait pas d’observations dans le voisinage de <span class="math inline">\(\mathrm{X}_1=0\)</span>, même si cette valeur est plausible; on parle alors d’extrapolation.</p>
<p>Si les colonnes de <span class="math inline">\(\mathbf{X}\)</span> sont arbitraires, il est d’usage d’inclure une constante: cela revient à inclure <span class="math inline">\(\mathbf{1}_n\)</span> comme colonne de la matrice de plan d’expérience <span class="math inline">\(\mathbf{X}\)</span>. Parce que les résidus sont orthogonaux aux colonnes de <span class="math inline">\(\mathbf{X}\)</span>, leur moyenne est zéro, <span class="math inline">\(n^{-1}\mathbf{1}_n^\top\boldsymbol{e}=\bar{\boldsymbol{e}}=0\)</span>. En général, on peut obtenir des résidus centrés en incluant comme régresseurs dans la matrice <span class="math inline">\(\mathbf{X}\)</span> des vecteurs colonnes qui sont collinéaires avec <span class="math inline">\(\mathbf{1}_n\)</span>.</p>
<p>Dans notre exemple, l’équation de la droite ajustée de la Figure <a href="regression-lineaire.html#fig:droitenuage">2.2</a> est <span class="math display">\[\widehat{\texttt{salaire}} = 99.975 + 0.78\texttt{service}.\]</span>
Ainsi, le salaire moyen d’un nouveau professeur serait 99974.653 dollars, tandis que l’augmentation moyenne annuelle du salaire est 779.569 dollars.</p>
<p>Si la variable réponse <span class="math inline">\(Y\)</span> doit être <em>continue</em>, il n’y a aucune restriction pour les variables explicatives. On peut aussi considérer des variables explicatives binaires, qui sont encodées numériquement à l’aide de 0/1. Par exemple, si on s’intéresse au sexe des professeurs de l’étude,
<span class="math display">\[\texttt{sexe} = \begin{cases} 0 , &amp; \text{pour les hommes},\\
1, &amp; \text{pour les femmes.}
\end{cases}
\]</span>
L’équation du modèle linéaire simple qui n’inclut que cette variable catégorielle à deux niveaux, <span class="math inline">\(\texttt{sexe}\)</span>, s’écrit <span class="math inline">\(\texttt{salaire} = \beta_0 + \beta_1 \texttt{sexe} + \varepsilon\)</span>. Posons <span class="math inline">\(\mu_0\)</span> le salaire moyen des hommes et <span class="math inline">\(\mu_1\)</span> celui des femmes. L’ordonnée à l’origine <span class="math inline">\(\beta_0\)</span> s’interprète comme d’ordinaire: c’est le salaire moyen quand <span class="math inline">\(\texttt{sexe}=0\)</span>, autrement dit <span class="math inline">\(\beta_0=\mu_0\)</span>. On peut écrire l’équation de l’espérance conditionnelle pour chacune des catégories,
<span class="math display">\[\begin{align*}
\mathsf{E}(\texttt{salaire} \mid \texttt{sexe})= \begin{cases}
\beta_0, &amp; \texttt{sexe}=0 \text{ (homme)}, \\
\beta_0 + \beta_1 &amp; \texttt{sexe}=1 \text{ (femme)}.
\end{cases}
\end{align*}\]</span>
Un modèle linéaire qui contient uniquement une variable binaire <span class="math inline">\(\mathrm{X}\)</span> comme régresseur équivaut à spécifier une moyenne différente pour deux groupes; la moyenne des femmes est <span class="math inline">\(\mathsf{E}(\texttt{salaire} \mid \texttt{sexe}=1) = \beta_0 + \beta_1 = \mu_1\)</span> et <span class="math inline">\(\beta_1=\mu_1-\mu_0\)</span> représente la différence entre la moyenne des hommes et celles des femmes. L’estimateur des moindres carrés <span class="math inline">\(\widehat{\beta}_0\)</span> est la moyenne empirique du salaire des hommes de l’échantillon et <span class="math inline">\(\widehat{\beta}_1\)</span> est la différence des moyennes empiriques entre femmes et hommes. Cette paramétrisation en terme d’<strong>effets différentiels</strong> est particulièrement utile si on veut tester s’il y a une différence moyenne de salaire entre les deux sexe car cela revient à tester <span class="math inline">\(\mathscr{H}_0: \beta_1=0\)</span>. Si on voulait obtenir directement la moyenne, il faudrait remplacer la matrice de plan d’expérience <span class="math inline">\([\mathbf{1}_n, \texttt{sexe}]\)</span> par <span class="math inline">\([\mathbf{1}_n - \texttt{sexe}, \texttt{sexe}]\)</span> pour obtenir un modèle équivalent. Règle générale, il n’est pas recommandé de retirer l’ordonnée à l’origine même si l’espace linéaire engendré par les colonnes de <span class="math inline">\(\mathbf{X}\)</span> contient <span class="math inline">\(\mathbf{1}_n\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:graphcollegesexe"></span>
<img src="MATH60604_Modelisation_statistique_files/figure-html/graphcollegesexe-1.png" alt="Modèle linéaire simple pour les données $\texttt{college}$ en fonction de la variable binaire sexe: bien que le modèle définisse une ligne, seule la valeur en $0/1$ est réalisable." width="70%" />
<p class="caption">
Figure 2.4: Modèle linéaire simple pour les données <span class="math inline">\(\texttt{college}\)</span> en fonction de la variable binaire sexe: bien que le modèle définisse une ligne, seule la valeur en <span class="math inline">\(0/1\)</span> est réalisable.
</p>
</div>
<p>Si on ajuste un modèle de régression linéaire pour les données <code>college</code>, on obtient un salaire moyen de <span class="math inline">\(\widehat{\beta}_0=115.09\)</span> milliers de dollars USD pour les hommes et une différence moyenne de salaire entre femmes et hommes de <span class="math inline">\(\widehat{\beta}_1=14.088\)</span> milliers de dollars. Puisque l’estimé est négatif, les femmes sont moins payés: ce modèle n’est en revanche pas suffisant pour déterminer s’il y a inéquité salariale: la Figure <a href="regression-lineaire.html#fig:droitenuage">2.2</a> montre que le nombre d’années de service et l’échelon académique impactent fortement le salaire, or il n’est pas dit que la répartition des sexes au sein des échelons est comparable (et ce n’est pas le cas).</p>
<p>Même si le modèle linéaire simple définit une droite, cette dernière n’a de sens qu’en <span class="math inline">\(0\)</span> ou <span class="math inline">\(1\)</span>; la Figure <a href="regression-lineaire.html#fig:graphcollegesexe">2.4</a> montre un estimé de la densité et la répartition des points (décalés) dans l’échantillon selon le sexe, avec la moyenne de chacun. On voit bien que la droite passe par la moyenne de chaque groupe.
<!-- http://easysas.blogspot.com/2011/10/sas-how-to-draw-added-variable-plot.html
: certains logiciels, **R** en tête, modifient les sorties et ne présentent plus le coefficient de détermination car il n'est pas donné que les résidus soient centrés. --></p>
<p>Plus généralement, il est possible de considérer une variable catégorielle à <span class="math inline">\(k\)</span> niveaux. Comme pour la variable binaire, on ajoute au modèle <span class="math inline">\(k-1\)</span> variables indicatrices en plus de l’ordonnée à l’origine: si on veut modéliser <span class="math inline">\(k\)</span> moyennes, il est logique de n’inclure que <span class="math inline">\(k\)</span> paramètres. On choisira comme dans l’exemple avec le sexe une <strong>catégorie de référence</strong> dont la moyenne sera encodée par l’ordonnée à l’origine <span class="math inline">\(\beta_0\)</span>. Les autres paramètres seront des effets différentiels relatifs à cette catégorie. Prenons pour exemple l’échelon académique, une variable catégorielle ordinale à trois niveaux (adjoint, aggrégé, titulaire). On ajoute deux variables binaires <span class="math inline">\(\mathrm{X}_1 = \mathsf{I}(\texttt{echelon}=\texttt{aggrege})\)</span> et <span class="math inline">\(\mathrm{X}_2 = \mathsf{I}(\texttt{echelon}=\texttt{titulaire})\)</span>; l’élément <span class="math inline">\(i\)</span> de la colonne <span class="math inline">\(\mathrm{X}_1\)</span> vaut 1 si le professeur est aggrégé et zéro autrement. Le modèle linéaire
<span class="math display">\[\begin{align*}
\texttt{salaire} \mid \texttt{echelon}=\beta_0 + \beta_1 \mathrm{X}_1+\beta_2\mathrm{X}_2 + \varepsilon,
\end{align*}\]</span>
et l’espérance conditionnelle du salaire s’écrit
<span class="math display">\[\begin{align*}
\mathsf{E}(\texttt{salaire} \mid \texttt{echelon})= \begin{cases}
\beta_0, &amp; \texttt{echelon}=\texttt{adjoint},\\
\beta_0 + \beta_1 &amp; \texttt{echelon}=\texttt{aggrege},\\
\beta_0 + \beta_2 &amp; \texttt{echelon}=\texttt{titulaire},
\end{cases}
\end{align*}\]</span>
Ainsi, <span class="math inline">\(\beta_1\)</span> (respectivement <span class="math inline">\(\beta_2\)</span>) est la différence de salaire moyenne entre professeurs titulaires (respectivement aggrégés) et professeurs adjoints.
Le choix de la catégorie de référence est arbitraire et le modèle ajusté est le même: seule l’interprétation des coefficients change. Pour une variable ordinale, il vaut mieux choisir la plus petite ou la plus grande des modalités pour faciliter les comparaisons.</p>
<p>Les modèles que nous avons ajusté jusqu’à maintenant ne sont pas adéquats parce qu’ils ignorent des variables qui sont importantes pour expliquer le modèle: la Figure <a href="regression-lineaire.html#fig:edacollege">2.1</a> illustre en effet que l’échelon est une composante essentielle pour expliquer les variations de salaire au sein du collège. On peut (et on doit) donc inclure plusieurs variables simultanément pour avoir un modèle adéquat. Avant de procéder, on considère l’interprétation des paramètres quand on utilise plus d’une variable explicative dans le modèle.</p>
<p>Soit le modèle <span class="math inline">\(Y= \beta_0 + \beta_1 \mathrm{X}_1 + \cdots + \beta_p\mathrm{X}_p + \varepsilon\)</span>. L’ordonnée à l’origine <span class="math inline">\(\beta_0\)</span> représente la valeur moyenne de <span class="math inline">\(Y\)</span> quand <em>toutes</em> les covariables du modèle sont égales à zéro,
<span class="math display">\[\begin{align*}
\beta_0 &amp;= \mathsf{E}(Y \mid \mathrm{X}_1=0,\mathrm{X}_2=0,\ldots,\mathrm{X}_p=0).
\end{align*}\]</span>
De nouveau, cette interprétation peut ne pas être sensée ou logique selon le contexte de l’étude. Le coefficient <span class="math inline">\(\beta_j\)</span> <span class="math inline">\((j \geq 1)\)</span> peut quant à lui être interprété comme l’augmentation moyenne de l’espérance de la variable réponse <span class="math inline">\(Y\)</span> quand <span class="math inline">\(\mathrm{X}_j\)</span> augmente d’une unité, toutes choses étant égales par ailleurs (<em>ceteris paribus</em>). Par exemple, l’interprétation de <span class="math inline">\(\beta_1\)</span> est
<span class="math display">\[\begin{align*}
\beta_1 &amp;= \mathsf{E}(Y \mid \mathrm{X}_1=x_1+1,\mathrm{X}_2=x_2,\ldots,\mathrm{X}_p=x_p) \\
&amp; \qquad \qquad - \mathsf{E}(Y \mid \mathrm{X}_1=x_1,\mathrm{X}_2=x_2,\ldots,\mathrm{X}_p=x_p) \\
&amp;= \left\{\beta_0 + \beta_1 (x_1+1) + \beta_2 x_2 + \cdots +\beta_p \mathrm{X}_p \right\} \\
&amp; \qquad \qquad -\left\{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots +\beta_p \mathrm{X}_p \right\}
\end{align*}\]</span>
Il n’est pas toujours possible de fixer la valeur des autres colonnes de <span class="math inline">\(\mathbf{X}\)</span> si plusieurs colonnes contiennent des transformations ou des fonctions d’une même variable explicative. Par exemple, on pourrait par exemple considérer un polynôme d’ordre <span class="math inline">\(k\)</span> (normalement, <span class="math inline">\(k\leq 3\)</span> en pratique),
<span class="math display">\[\begin{align*}
Y=\beta_0+ \beta_1 \mathrm{X}+ \beta_2 \mathrm{X}^2 + \ldots +\beta_k \mathrm{X}^k + \varepsilon.
\end{align*}\]</span>
Si l’on inclut un terme d’ordre <span class="math inline">\(k\)</span>, <span class="math inline">\(\mathrm{X}^k\)</span>, il faut <strong>toujours</strong> inclure les termes d’ordre inférieur <span class="math inline">\(1, \mathrm{X}, \ldots, \mathrm{X}^{k-1}\)</span> pour l’interprétabilité du modèle résultant (autrement, cela revient à choisir un polynôme en imposant que certains coefficients soient zéros). L’interprétation des effets des covariables nonlinéaires (même polynomiaux) est complexe parce qu’on ne peut pas « fixer la valeur des autres variables »: l’effet d’une augmentation d’une unité de <span class="math inline">\(\mathrm{X}\)</span> <em>dépend de la valeur de cette dernière</em>.</p>
<div class="example">
<p><span id="exm:automobile" class="example"><strong>Exemple 2.1  (Données automobile) </strong></span>Considérons un modèle de régression linéaire pour l’autonomie d’essence en fonction de la puissance du moteur pour différentes voitures dont les caractéristiques sont données dans le jeu de données <code>automobiles</code>. Le modèle postulé incluant un terme quadratique est
<span class="math display">\[
\texttt{autonomie}_i = \beta_0 + \beta_1 \texttt{puissance}_i + \beta_2 \texttt{puissance}_i^2 + \varepsilon_i
\]</span></p>
</div>
<p>Afin de comparer l’ajustement du modèle quadratique, on peut inclure également la droite ajustée du modèle de régression simple qui n’inclut que puissance.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:autoquad2d"></span>
<img src="MATH60604_Modelisation_statistique_files/figure-html/autoquad2d-1.png" alt="Modèle de régression avec terme quadratique pour la puissance" width="70%" />
<p class="caption">
Figure 2.5: Modèle de régression avec terme quadratique pour la puissance
</p>
</div>
<p>À vue d’oeil, l’ajustement est meilleur pour le modèle quadratique: nous verrons plus tard à l’aide de test si cette observation est vérifiée statistiquement.
On voit aussi dans la Figure <a href="regression-lineaire.html#fig:autoquad2d">2.5</a> que l’autonomie d’essence décroît rapidement quand la puissance croît entre <span class="math inline">\(0\)</span> et <span class="math inline">\(189.35\)</span>, mais semble remonter légèrement par la suite pour les voitures qui un moteur de plus de 200 chevaux-vapeurs, ce que le modèle quadratique capture. Prenez garde en revanche à l’extrapolation là où vous n’avez pas de données (comme l’illustre remarquablement bien <a href="https://livefreeordichotomize.com/2020/05/05/model-detective/">le modèle cubique de Hassett pour le nombre de cas quotidiens de coronavirus</a>).</p>
<p>La représentation graphique du modèle polynomial de degré 2 présenté dans la Figure <a href="regression-lineaire.html#fig:autoquad2d">2.5</a> peut sembler contre-intuitive, mais c’est une projection en 2D d’un plan 3D de coordonnées <span class="math inline">\(\beta_0 + \beta_1x-y +\beta_2z =0\)</span>, où <span class="math inline">\(x=\texttt{puissance}\)</span>, <span class="math inline">\(z=\texttt{puissance}^2\)</span> et <span class="math inline">\(y=\texttt{autonomie}\)</span>. La physique et le bon-sens imposent la contrainte <span class="math inline">\(z = x^2\)</span>, et donc les valeurs ajustées vivent sur une courbe dans un sous-espace du plan ajusté, représenté en gris dans la Figure <a href="regression-lineaire.html#fig:hyperplan">2.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyperplan"></span>
<div class="plotly html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-134ba4ccbeb0f31c7653" style="width:70%;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-134ba4ccbeb0f31c7653">{"x":{"visdat":{"ab12303d6e0c":["function () ","plotlyVisDat"],"ab125f1137c3":["function () ","data"]},"cur_data":"ab125f1137c3","attrs":{"ab12303d6e0c":{"colors":"grey","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","name":"data","opacity":0.80000000000000004,"marker":{"color":"black","size":4,"hoverinfo":"skip","opacity":0.80000000000000004},"inherit":true},"ab12303d6e0c.1":{"z":{},"type":"surface","x":[46,230],"y":[2116,52900],"name":"Relation entre puissance et autonomie","opacity":0.75,"cauto":false,"surfacecolor":[0,0,0],"inherit":false},"ab125f1137c3":{"colors":"grey","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines","color":["#003C71"],"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"puissance (chevaux vapeurs)"},"yaxis":{"title":"puissance carré"},"zaxis":{"title":"autonomie d'essence (miles au gallon)"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[130,165,150,150,140,198,220,215,225,190,170,160,150,225,95,95,97,85,88,46,87,90,95,113,90,215,200,210,193,88,90,95,100,105,100,88,100,165,175,153,150,180,170,175,110,72,100,88,86,90,70,76,65,69,60,70,95,80,54,90,86,165,175,150,153,150,208,155,160,190,97,150,130,140,150,112,76,87,69,86,92,97,80,88,175,150,145,137,150,198,150,158,150,215,225,175,105,100,100,88,95,46,150,167,170,180,100,88,72,94,90,85,107,90,145,230,49,75,91,112,150,110,122,180,95,100,100,67,80,65,75,100,110,105,140,150,150,140,150,83,67,78,52,61,75,75,75,97,93,67,95,105,72,72,170,145,150,148,110,105,110,95,110,110,129,75,83,100,78,96,71,97,97,70,90,95,88,98,115,53,86,81,92,79,83,140,150,120,152,100,105,81,90,52,60,70,53,100,78,110,95,71,70,75,72,102,150,88,108,120,180,145,130,150,68,80,58,96,70,145,110,145,130,110,105,100,98,180,170,190,149,78,88,75,89,63,83,67,78,97,110,110,48,66,52,70,60,110,140,139,105,95,85,88,100,90,105,85,110,120,145,165,139,140,68,95,97,75,95,105,85,97,103,125,115,133,71,68,115,85,88,90,110,130,129,138,135,155,142,125,150,71,65,80,80,77,125,71,90,70,70,65,69,90,115,115,90,76,60,70,65,90,88,90,90,78,90,75,92,75,65,105,65,48,48,67,67,67,67,62,132,100,88,72,84,84,92,110,84,58,64,60,67,65,62,68,63,65,65,74,75,75,100,74,80,76,116,120,110,105,88,85,88,88,88,85,84,90,92,74,68,68,63,70,88,75,70,67,67,67,110,85,92,112,96,84,90,86,52,84,79,82],"y":[16900,27225,22500,22500,19600,39204,48400,46225,50625,36100,28900,25600,22500,50625,9025,9025,9409,7225,7744,2116,7569,8100,9025,12769,8100,46225,40000,44100,37249,7744,8100,9025,10000,11025,10000,7744,10000,27225,30625,23409,22500,32400,28900,30625,12100,5184,10000,7744,7396,8100,4900,5776,4225,4761,3600,4900,9025,6400,2916,8100,7396,27225,30625,22500,23409,22500,43264,24025,25600,36100,9409,22500,16900,19600,22500,12544,5776,7569,4761,7396,8464,9409,6400,7744,30625,22500,21025,18769,22500,39204,22500,24964,22500,46225,50625,30625,11025,10000,10000,7744,9025,2116,22500,27889,28900,32400,10000,7744,5184,8836,8100,7225,11449,8100,21025,52900,2401,5625,8281,12544,22500,12100,14884,32400,9025,10000,10000,4489,6400,4225,5625,10000,12100,11025,19600,22500,22500,19600,22500,6889,4489,6084,2704,3721,5625,5625,5625,9409,8649,4489,9025,11025,5184,5184,28900,21025,22500,21904,12100,11025,12100,9025,12100,12100,16641,5625,6889,10000,6084,9216,5041,9409,9409,4900,8100,9025,7744,9604,13225,2809,7396,6561,8464,6241,6889,19600,22500,14400,23104,10000,11025,6561,8100,2704,3600,4900,2809,10000,6084,12100,9025,5041,4900,5625,5184,10404,22500,7744,11664,14400,32400,21025,16900,22500,4624,6400,3364,9216,4900,21025,12100,21025,16900,12100,11025,10000,9604,32400,28900,36100,22201,6084,7744,5625,7921,3969,6889,4489,6084,9409,12100,12100,2304,4356,2704,4900,3600,12100,19600,19321,11025,9025,7225,7744,10000,8100,11025,7225,12100,14400,21025,27225,19321,19600,4624,9025,9409,5625,9025,11025,7225,9409,10609,15625,13225,17689,5041,4624,13225,7225,7744,8100,12100,16900,16641,19044,18225,24025,20164,15625,22500,5041,4225,6400,6400,5929,15625,5041,8100,4900,4900,4225,4761,8100,13225,13225,8100,5776,3600,4900,4225,8100,7744,8100,8100,6084,8100,5625,8464,5625,4225,11025,4225,2304,2304,4489,4489,4489,4489,3844,17424,10000,7744,5184,7056,7056,8464,12100,7056,3364,4096,3600,4489,4225,3844,4624,3969,4225,4225,5476,5625,5625,10000,5476,6400,5776,13456,14400,12100,11025,7744,7225,7744,7744,7744,7225,7056,8100,8464,5476,4624,4624,3969,4900,7744,5625,4900,4489,4489,4489,12100,7225,8464,12544,9216,7056,8100,7396,2704,7056,6241,6724],"z":[18,15,18,16,17,15,14,14,14,15,15,14,15,14,24,22,18,21,27,26,25,24,25,26,21,10,10,11,9,27,28,25,19,16,17,19,18,14,14,14,14,12,13,13,18,22,19,18,23,28,30,30,31,35,27,26,24,25,23,20,21,13,14,15,14,17,11,13,12,13,19,15,13,13,14,18,22,21,26,22,28,23,28,27,13,14,13,14,15,12,13,13,14,13,12,13,18,16,18,18,23,26,11,12,13,12,18,20,21,22,18,19,21,26,15,16,29,24,20,19,15,24,20,11,20,19,15,31,26,32,25,16,16,18,16,13,14,14,14,29,26,26,31,32,28,24,26,24,26,31,19,18,15,15,16,15,16,14,17,16,15,18,21,20,13,29,23,20,23,24,25,24,18,29,19,23,23,22,25,33,28,25,25,26,27,17.5,16,15.5,14.5,22,22,24,22.5,29,24.5,29,33,20,18,18.5,17.5,29.5,32,28,26.5,20,13,19,19,16.5,16.5,13,13,13,31.5,30,36,25.5,33.5,17.5,17,15.5,15,17.5,20.5,19,18.5,16,15.5,15.5,16,29,24.5,26,25.5,30.5,33.5,30,30.5,22,21.5,21.5,43.100000000000001,36.100000000000001,32.799999999999997,39.399999999999999,36.100000000000001,19.899999999999999,19.399999999999999,20.199999999999999,19.199999999999999,20.5,20.199999999999999,25.100000000000001,20.5,19.399999999999999,20.600000000000001,20.800000000000001,18.600000000000001,18.100000000000001,19.199999999999999,17.699999999999999,18.100000000000001,17.5,30,27.5,27.199999999999999,30.899999999999999,21.100000000000001,23.199999999999999,23.800000000000001,23.899999999999999,20.300000000000001,17,21.600000000000001,16.199999999999999,31.5,29.5,21.5,19.800000000000001,22.300000000000001,20.199999999999999,20.600000000000001,17,17.600000000000001,16.5,18.199999999999999,16.899999999999999,15.5,19.199999999999999,18.5,31.899999999999999,34.100000000000001,35.700000000000003,27.399999999999999,25.399999999999999,23,27.199999999999999,23.899999999999999,34.200000000000003,34.5,31.800000000000001,37.299999999999997,28.399999999999999,28.800000000000001,26.800000000000001,33.5,41.5,38.100000000000001,32.100000000000001,37.200000000000003,28,26.399999999999999,24.300000000000001,19.100000000000001,34.299999999999997,29.800000000000001,31.300000000000001,37,32.200000000000003,46.600000000000001,27.899999999999999,40.799999999999997,44.299999999999997,43.399999999999999,36.399999999999999,30,44.600000000000001,33.799999999999997,29.800000000000001,32.700000000000003,23.699999999999999,35,32.399999999999999,27.199999999999999,26.600000000000001,25.800000000000001,23.5,30,39.100000000000001,39,35.100000000000001,32.299999999999997,37,37.700000000000003,34.100000000000001,34.700000000000003,34.399999999999999,29.899999999999999,33,33.700000000000003,32.399999999999999,32.899999999999999,31.600000000000001,28.100000000000001,30.699999999999999,25.399999999999999,24.199999999999999,22.399999999999999,26.600000000000001,20.199999999999999,17.600000000000001,28,27,34,31,29,27,24,36,37,31,38,36,36,36,34,38,32,38,25,38,26,22,32,36,27,27,44,32,28,31],"type":"scatter3d","mode":"markers","name":"data","opacity":0.80000000000000004,"marker":{"color":"black","size":4,"hoverinfo":"skip","opacity":0.80000000000000004,"line":{"color":"rgba(31,119,180,1)"},"showscale":false},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"autonomie<br />surf","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(190,190,190,1)"],["1","rgba(190,190,190,1)"]],"showscale":false,"z":[[38.059191113772322,-47.719700796540657],[100.5507364554749,14.771844545161926]],"type":"surface","x":[46,230],"y":[2116,52900],"name":"Relation entre puissance et autonomie","opacity":0.75,"cauto":false,"surfacecolor":[0,0,0],"frame":null},{"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250],"y":[0,1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256,289,324,361,400,441,484,529,576,625,676,729,784,841,900,961,1024,1089,1156,1225,1296,1369,1444,1521,1600,1681,1764,1849,1936,2025,2116,2209,2304,2401,2500,2601,2704,2809,2916,3025,3136,3249,3364,3481,3600,3721,3844,3969,4096,4225,4356,4489,4624,4761,4900,5041,5184,5329,5476,5625,5776,5929,6084,6241,6400,6561,6724,6889,7056,7225,7396,7569,7744,7921,8100,8281,8464,8649,8836,9025,9216,9409,9604,9801,10000,10201,10404,10609,10816,11025,11236,11449,11664,11881,12100,12321,12544,12769,12996,13225,13456,13689,13924,14161,14400,14641,14884,15129,15376,15625,15876,16129,16384,16641,16900,17161,17424,17689,17956,18225,18496,18769,19044,19321,19600,19881,20164,20449,20736,21025,21316,21609,21904,22201,22500,22801,23104,23409,23716,24025,24336,24649,24964,25281,25600,25921,26244,26569,26896,27225,27556,27889,28224,28561,28900,29241,29584,29929,30276,30625,30976,31329,31684,32041,32400,32761,33124,33489,33856,34225,34596,34969,35344,35721,36100,36481,36864,37249,37636,38025,38416,38809,39204,39601,40000,40401,40804,41209,41616,42025,42436,42849,43264,43681,44100,44521,44944,45369,45796,46225,46656,47089,47524,47961,48400,48841,49284,49729,50176,50625,51076,51529,51984,52441,52900,53361,53824,54289,54756,55225,55696,56169,56644,57121,57600,58081,58564,59049,59536,60025,60516,61009,61504,62001,62500],"z":[56.900099702112968,56.435140608266387,55.972642586621355,55.51260563717787,55.055029759935941,54.599914954895553,54.147261222056706,53.697068561419421,53.249336972983677,52.804066456749474,52.361257012716827,51.920908640885727,51.483021341256169,51.047595113828166,50.614629958601711,50.184125875576804,49.756082864753438,49.330500926131627,48.907380059711357,48.486720265492636,48.06852154347547,47.652783893659851,47.239507316045774,46.828691810633245,46.420337377422264,46.014444016412838,45.611011727604946,45.210040510998617,44.811530366593828,44.415481294390588,44.021893294388896,43.630766366588752,43.242100510990156,42.855895727593108,42.472152016397608,42.090869377403656,41.712047810611253,41.335687316020397,40.961787893631083,40.590349543443317,40.221372265457106,39.854856059672443,39.490800926089321,39.129206864707754,38.770073875527736,38.413401958549258,38.059191113772322,37.707441341196947,37.358152640823114,37.011325012650836,36.666958456680106,36.325052972910918,35.98560856134327,35.648625221977177,35.31410295481264,34.982041759849636,34.652441637088195,34.325302586528288,34.000624608169936,33.678407702013132,33.358651868057876,33.041357106304169,32.726523416752002,32.414150799401391,32.104239254252327,31.796788781304805,31.491799380558838,31.189271052014412,30.889203795671541,30.591597611530215,30.296452499590437,30.0037684598522,29.713545492315518,29.425783596980388,29.140482773846802,28.857643022914758,28.577264344184265,28.299346737655327,28.023890203327923,27.750894741202078,27.480360351277781,27.212287033555029,26.946674788033818,26.683523614714161,26.422833513596057,26.16460448467949,25.908836527964478,25.655529643451015,25.404683831139099,25.156299091028725,24.910375423119902,24.666912827412631,24.425911303906901,24.187370852602726,23.951291473500095,23.717673166599013,23.486515931899476,23.25781976940149,23.031584679105052,22.807810661010155,22.586497715116813,22.36764584142502,22.151255039934775,21.937325310646067,21.725856653558917,21.516849068673316,21.310302555989253,21.106217115506745,20.904592747225784,20.705429451146369,20.508727227268505,20.314486075592189,20.122705996117421,19.933386988844195,19.746529053772523,19.5621321909024,19.380196400233814,19.200721681766787,19.023708035501304,18.849155461437373,18.67706395957498,18.507433529914142,18.340264172454855,18.175555887197106,18.013308674140912,17.853522533286267,17.696197464633169,17.541333468181609,17.388930543931608,17.238988691883154,17.091507912036239,16.946488204390882,16.803929568947069,16.663832005704805,16.526195514664082,16.391020095824917,16.258305749187297,16.128052474751218,16.000260272516694,15.874929142483715,15.752059084652288,15.631650099022409,15.51370218559406,15.398215344367276,15.285189575342038,15.174624878518348,15.066521253896209,14.960878701475615,14.857697221256569,14.756976813239056,14.658717477423107,14.562919213808705,14.469582022395848,14.378705903184542,14.290290856174785,14.204336881366572,14.120843978759897,14.03981214835478,13.961241390151212,13.885131704149195,13.811483090348723,13.740295548749799,13.671569079352423,13.605303682156581,13.541499357162301,13.480156104369563,13.421273923778379,13.364852815388744,13.310892779200657,13.259393815214111,13.210355923429105,13.163779103845663,13.119663356463761,13.078008681283414,13.038815078304609,13.002082547527358,12.967811088951649,12.936000702577481,12.906651388404867,12.879763146433802,12.855335976664286,12.833369879096324,12.813864853729903,12.796820900565031,12.782238019601692,12.770116210839916,12.760455474279688,12.753255809921008,12.748517217763876,12.746239697808292,12.746423250054256,12.749067874501748,12.754173571150808,12.761740340001417,12.771768181053574,12.784257094307272,12.799207079762525,12.816618137419319,12.836490267277654,12.858823469337544,12.88361774359899,12.910873090061976,12.940589508726518,12.972766999592601,13.007405562660232,13.044505197929396,13.084065905400131,13.126087685072406,13.170570536946229,13.217514461021601,13.266919457298521,13.318785525776988,13.37311266645699,13.429900879338554,13.489150164421666,13.550860521706326,13.615031951192528,13.681664452880284,13.750758026769574,13.822312672860427,13.896328391152821,13.97280518164677,14.05174304434226,14.133141979239305,14.217001986337891,14.303323065638018,14.392105217139701,14.483348440842931,14.577052736747717,14.673218104854044,14.771844545161926,14.872932057671349,14.976480642382299,15.082490299294818,15.190961028408893,15.301892829724508,15.415285703241665,15.531139648960377,15.649454666880644,15.770230757002437,15.893467919325786,16.019166153850691,16.147325460577136,16.277945839505136,16.411027290634692,16.546569813965775,16.684573409498412,16.825038077232591,16.967963817168339,17.113350629305614,17.261198513644459],"type":"scatter3d","mode":"lines","marker":{"color":"rgba(0,60,113,1)","line":{"color":"rgba(0,60,113,1)"},"showscale":false},"textfont":{"color":"rgba(0,60,113,1)"},"error_y":{"color":"rgba(0,60,113,1)"},"error_x":{"color":"rgba(0,60,113,1)"},"line":{"color":"rgba(0,60,113,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly",".hideLegend":true},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 2.6: Représentation graphique 3D du modèle de régression linéaire pour les données <span class="math inline">\(\texttt{automobile}\)</span>.
</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-3" class="remark"><em>Remarque</em> (Utilisation de bases polynomiales pour les effets nonlinéaires). </span>Règle générale, on utilise des représentations flexibles (bases de splines) plutôt que des modèles polynomiaux pour le lissage si la relation entre une variable <span class="math inline">\(Y\)</span> et une variable explicative <span class="math inline">\(\mathrm{X}\)</span> est nonlinéaire. Une compréhension de la physique du système à l’étude, ou bien un modèle théorique permet aussi de guider le choix des fonctions (non)linéaires à utiliser.</p>
</div>
<p>Le coefficient <span class="math inline">\(\beta_j\)</span> est la contribution
<em>marginale</em> de <span class="math inline">\(\mathrm{X}_j\)</span> quand les autres covariables sont incluses dans le modèle. On peut représenter graphiquement cet effet en projetant les vecteurs <span class="math inline">\(Y\)</span> et <span class="math inline">\(\mathrm{X}_j\)</span> dans le complément orthogonal de <span class="math inline">\(\mathbf{X}_{-j}\)</span>. Le diagramme de régression partielle est un diagnostic graphique qui illustre la valeur ajoutée de <span class="math inline">\(\mathrm{X}_j\)</span>: il montre en ordonnée (axe des <span class="math inline">\(y\)</span>), les résidus du modèle de régression pour <span class="math inline">\(Y\)</span> avec toutes les variables explicatives sauf <span class="math inline">\(\mathrm{X}_j\)</span>, et en abcisse (axe des <span class="math inline">\(x\)</span>), les résidus de la régression de <span class="math inline">\(\mathrm{X}_j\)</span> sur les autres variables explicatives. La droite de régression qui satisfait le critère des moindre carrés pour ce nuage de points passe par (<span class="math inline">\(0,0\)</span>) et sa pente est <span class="math inline">\(\hat{\beta}_j\)</span>. Ce diagnostic est particulièrement utile pour détecter l’impact de valeurs aberrantes ou la colinéarité.</p>
<div class="example">
<p><span id="exm:inequite-salariale" class="example"><strong>Exemple 2.2  (Inéquité salariale dans un collège américain) </strong></span>On considère les données <code>college</code> et un modèle de régression qui inclut le sexe, l’échelon académique, le nombre d’années de service et le domaine d’expertise (appliquée ou théorique).</p>
</div>
Si on multiplie le salaire par mille, le modèle linéaire postulé s’écrit
<span class="math display">\[\begin{align*}
\texttt{salaire} \times 1000 &amp;= \beta_0 + \beta_1 \texttt{sexe}_{\texttt{femme}} +\beta_2 \texttt{domaine}_{\texttt{theorique}} \\&amp;\quad +\beta_3 \texttt{echelon}_{\texttt{aggrege}}
+\beta_4 \texttt{echelon}_{\texttt{titulaire}}  +\beta_5 \texttt{service} + \varepsilon.
\end{align*}\]</span>
<table>
<caption>
<span id="tab:collegecoefs">Tableau 2.2: </span>Estimés des coefficients du modèle linéaire pour les données <span class="math inline">\(\texttt{college}\)</span> (en dollars USD, arrondis à l’unité).
</caption>
<thead>
<tr>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_0\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_1\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_2\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_3\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_4\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_5\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
86596
</td>
<td style="text-align:right;">
-4771
</td>
<td style="text-align:right;">
-13473
</td>
<td style="text-align:right;">
14560
</td>
<td style="text-align:right;">
49160
</td>
<td style="text-align:right;">
-89
</td>
</tr>
</tbody>
</table>
<p>L’interprétation des coefficients est la suivante:</p>
<ul>
<li>L’ordonnée à l’origine <span class="math inline">\(\beta_0\)</span> correspond au salaire moyen d’un professeur adjoint (un homme) qui vient de compléter ses études et qui travaille dans un domaine appliqué: on estime ce salaire à <span class="math inline">\(\widehat{\beta}_0=86596\)</span> dollars.</li>
<li>toutes choses étant égales par ailleurs (même domaine, échelon et années depuis le dernier diplôme), l’écart de salaire entre un homme et un femme est estimé à <span class="math inline">\(\widehat{\beta}_1=-4771\)</span> dollars.</li>
<li><em>ceteris paribus</em>, un(e) professeur(e) qui oeuvre dans un domaine théorique gagne <span class="math inline">\(\beta_2\)</span> dollars de plus qu’une personne du même sexe dans un domaine appliqué; on estime cette différence à <span class="math inline">\(-13473\)</span> dollars.</li>
<li><em>ceteris paribus</em>, la différence moyenne de salaire entre professeurs adjoints et aggrégés est estimée à <span class="math inline">\(\widehat{\beta}_3=14560\)</span> dollars.</li>
<li><em>ceteris paribus</em>, la différence moyenne de salaire entre professeurs adjoints et titulaires est de <span class="math inline">\(\widehat{\beta}_4=49160\)</span> dollars.</li>
<li>au sein d’un même échelon, chaque année supplémentaire de service mène à une augmentation de salaire annuelle moyenne de <span class="math inline">\(\widehat{\beta}_5=-89\)</span> dollars.</li>
</ul>
<p>On voit que les femmes sont moins payées que les hommes: reste à savoir si cette différence est statistiquement significative. L’estimé de la surprime annuelle due à l’expérience est négative, un résultat contre-intuitif au vu de la Figure <a href="regression-lineaire.html#fig:droitenuage">2.2</a> qui montrait une augmentation notable du salaire avec les années. Cette représentation graphique est trompeuse: la Figure <a href="regression-lineaire.html#fig:edacollege">2.1</a> montrait l’impact important de l’échelon académique. Une fois tous les autres facteurs pris en compte, le nombre d’années de service n’apporte que peu d’information au modèle et le diagramme de régression partielle de la Figure <a href="regression-lineaire.html#fig:avplotcollege">2.7</a> illustre l’absence de corrélation entre salaire et la partie non expliquée par les autres covariables; les gens avec un grand nombre d’années de service sont moins payés que certains de leurs collègues, ce qui explique la pente négative.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:avplotcollege"></span>
<img src="MATH60604_Modelisation_statistique_files/figure-html/avplotcollege-1.png" alt="Diagramme de régression partielle pour les années de service dans le modèle de régression linéaire pour les données $\texttt{college}$." width="70%" />
<p class="caption">
Figure 2.7: Diagramme de régression partielle pour les années de service dans le modèle de régression linéaire pour les données <span class="math inline">\(\texttt{college}\)</span>.
</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="vraisemblance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH60604_Modelisation_statistique.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
