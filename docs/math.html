<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>B Dérivations mathématiques | Modélisation statistique</title>
  <meta name="description" content="Ces notes forment un complément web du cours MATH 60604 (Modélisation statistique) offert à la M.Sc. en gestion (science des données et analytique d’affaires) à HEC Montréal." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="B Dérivations mathématiques | Modélisation statistique" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Ces notes forment un complément web du cours MATH 60604 (Modélisation statistique) offert à la M.Sc. en gestion (science des données et analytique d’affaires) à HEC Montréal." />
  <meta name="github-repo" content="lbelzile/math60604" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="B Dérivations mathématiques | Modélisation statistique" />
  
  <meta name="twitter:description" content="Ces notes forment un complément web du cours MATH 60604 (Modélisation statistique) offert à la M.Sc. en gestion (science des données et analytique d’affaires) à HEC Montréal." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="complement.html"/>
<link rel="next" href="r.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.3/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modélisation statistique</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Remarques</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction à l’inférence statistique</a></li>
<li class="chapter" data-level="2" data-path="regression-lineaire.html"><a href="regression-lineaire.html"><i class="fa fa-check"></i><b>2</b> Régression linéaire</a></li>
<li class="chapter" data-level="3" data-path="vraisemblance.html"><a href="vraisemblance.html"><i class="fa fa-check"></i><b>3</b> Inférence basée sur la vraisemblance</a></li>
<li class="chapter" data-level="4" data-path="modeles-lineaires-generalises.html"><a href="modeles-lineaires-generalises.html"><i class="fa fa-check"></i><b>4</b> Modèles linéaires généralisés</a></li>
<li class="chapter" data-level="5" data-path="donnees-correlees-longitudinales.html"><a href="donnees-correlees-longitudinales.html"><i class="fa fa-check"></i><b>5</b> Données corrélées et longitudinales</a></li>
<li class="chapter" data-level="6" data-path="modeles-lineaires-mixtes.html"><a href="modeles-lineaires-mixtes.html"><i class="fa fa-check"></i><b>6</b> Modèles linéaires mixtes</a></li>
<li class="chapter" data-level="7" data-path="survie.html"><a href="survie.html"><i class="fa fa-check"></i><b>7</b> Analyse de survie</a></li>
<li class="appendix"><span><b>Annexe</b></span></li>
<li class="chapter" data-level="A" data-path="complement.html"><a href="complement.html"><i class="fa fa-check"></i><b>A</b> Compléments mathématiques</a></li>
<li class="chapter" data-level="B" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>B</b> Dérivations mathématiques</a></li>
<li><a href="r.html#r"><strong>R</strong></a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publié avec bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modélisation statistique</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="math" class="section level1" number="9">
<h1><span class="header-section-number">B</span> Dérivations mathématiques</h1>
<p>Cette section regroupe les dérivations mathématiques optionnelles qui sont fournies par souci de complétude.</p>
<div id="ols" class="section level2" number="9.1">
<h2><span class="header-section-number">B.1</span> Dérivation de l’estimateur des moindres carrés ordinaires</h2>
<p>L’estimateur des moindres carrés ordinaires résoud le problème d’optimisation non-contraint
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}}=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}\]</span>
On peut calculer la dérivée première par rapport à <span class="math inline">\(\boldsymbol{\beta}\)</span>, égaler à zéro et isoler le maximum pour obtenir une formule explicite pour <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>,
<span class="math display">\[\begin{align*}
\mathbf{0}_n&amp;=\frac{\partial}{\partial\boldsymbol{\beta}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\\
\\&amp;=\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}\\
 \\&amp;=\mathbf{X}^\top (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\end{align*}\]</span>
en utilisant la <a href="http://www.stat.rice.edu/~dobelman/notes_papers/math/Matrix.Calculus.AppD.pdf">règle de dérivation en chaîne</a>; on peut ainsi distribuer les termes pour obtenir l’<em>équation normale</em>
<span class="math display">\[\begin{align*}
 \mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}&amp;=\mathbf{X}^\top \boldsymbol{y}.
\end{align*}\]</span>
Si <span class="math inline">\(\mathbf{X}\)</span> est une matrice de rang <span class="math inline">\(p\)</span>, alors la forme quadratique <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> est inversible et l’unique solution du problème d’optimisation est celle fournie dans l’équation <a href="regression-lineaire.html#eq:ols">(2.2)</a>.
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \boldsymbol{Y}.
\end{align*}\]</span></p>
</div>
<div id="derivationR2" class="section level2" number="9.2">
<h2><span class="header-section-number">B.2</span> Dérivation du coefficient de détermination</h2>
<p>Le point de départ de cette dérivation est la décomposition orthogonale entre le vecteur de valeurs ajustées et de résidus, <span class="math inline">\(\boldsymbol{y}=\widehat{\boldsymbol{y}} + \boldsymbol{e}\)</span>. Pourvu que la matrice du modèle <span class="math inline">\(\mathbf{X}\)</span> contienne l’équivalent de l’ordonnée à l’origine <span class="math inline">\(\mathbf{1}_n \in \mathcal{S}(\mathbf{X})\)</span>, alors la moyenne des résidus ordinaires est nulle, <span class="math inline">\(\overline{\boldsymbol{e}}=0\)</span> et il en découle que la moyenne empirique des réponses est égale à la moyenne empirique des valeurs ajustées. Puisque <span class="math inline">\(n^{-1}\sum_{i=1}^n \widehat{y}_i = n^{-1}\sum_{i=1}^n ({y}_i-e_i)=\overline{y}\)</span>,
<span class="math display">\[\begin{align*}
\widehat{\mathsf{Cor}}\left(\widehat{\boldsymbol{y}}, \boldsymbol{y}\right)
&amp;= \frac{(\boldsymbol{y} - \overline{y}\mathbf{1}_n)^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
\\&amp;= \frac{(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n) +
\boldsymbol{e}^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
\\&amp;= \frac{\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|}
\\&amp;= \frac{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\| - \|\boldsymbol{e}\|}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|}
\\&amp;= \sqrt{\frac{\mathsf{SC}_c-\mathsf{SC}_e}{\mathsf{SC}_c}}= \mathrm{R}.
\end{align*}\]</span>
Cela justifie la proposition de la <a href="#coefR2">Section 2.5</a> voulant que le carré de la corrélation entre les valeurs ajustées et la variable réponse est égal à <span class="math inline">\(R^2\)</span>.</p>
<div id="optimisation-pour-les-modèles-linéaires-généralisés" class="section level3" number="9.2.1">
<h3><span class="header-section-number">B.2.1</span> Optimisation pour les modèles linéaires généralisés</h3>
<p>Il n’existe règle générale pas de solution explicite pour l’estimateur du maximum de vraisemblance <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> pour les modèles linéaires généralisés; l’équation du score étant typiquement nonlinéaire en <span class="math inline">\(\boldsymbol{\beta}\)</span>, on doit obtenir les estimateurs du maximum de vraisemblance par le biais d’algorithmes d’optimisation numérique.</p>
<p>On dérive la log vraisemblance <span class="math inline">\(\ell = \sum_{i=1}^n \log f(y_i; \theta, \phi)\)</span> par rapport à
<span class="math inline">\(\boldsymbol{\beta}\)</span>. Pour simplifier, on considère chaque terme et coefficient à tour de rôle. Par la règle du produit,
<span class="math display">\[\begin{align*}
\frac{\partial \ell_i}{\partial \beta_j} = \frac{\partial
\eta_i}{\partial \beta_j} \frac{\partial \mu_i}{\partial \eta_i}
\frac{\partial \theta_i}{\partial \mu_i}\frac{\partial
\ell_i}{\partial \theta_i}
\end{align*}\]</span>
et les dérivations antérieures nous donnent <span class="math inline">\(\partial \ell_i/\partial \theta_i = (y_j-\mu_i)/a_i(\phi)\)</span> et <span class="math inline">\(\partial \mu_i / \partial \theta_i = b&#39;&#39;(\theta_i) = \mathsf{Va}(Y_i)/a_i(\phi)\)</span>.
La dérivée du prédicteur linéaire est <span class="math inline">\(\partial \eta_i / \partial \beta_j = \mathrm{X}_{ij}\)</span>. La seule dérivée partielle manquante, <span class="math inline">\(\partial \mu_i/\partial \eta_i\)</span>, dépend de la fonction de liaison puisque <span class="math inline">\(\eta_i = g(\mu_i)\)</span>; cette dérivée vaut un pour la fonction de liaison canonique.</p>
<p>Soit l’équation de score et la fonction d’information
<span class="math display">\[\begin{align*}
U(\boldsymbol{\beta}) = \frac{\partial \ell}{\partial \boldsymbol{\beta}}, \qquad j(\boldsymbol{\beta}) = - \frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top},
\end{align*}\]</span>
qui sont le gradient et la hessienne de la fonction de log vraisemblance; en prenant la somme de tous les termes individuels, le <span class="math inline">\(j\)</span>e élément du vecteur de score <span class="math inline">\(\boldsymbol{U}\)</span> est
<span class="math display">\[\begin{align*}
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{(y_i-\mu_i)\mathrm{X}_{ij}}{g&#39;(\mu_i)V(\mu_i)a_i(\phi)}, \qquad j=0, \ldots, p.
\end{align*}\]</span>
Puisque le maximum de vraisemblance <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> résoud l’équation du score <span class="math inline">\(U(\widehat{\boldsymbol{\beta}})=\boldsymbol{0}_{p+1}\)</span>, on peut construire un algorithme de Newton–Raphson pour obtenir ce dernier. Si on fait un développement limité de Taylor du score <span class="math inline">\(U(\widehat{\boldsymbol{\beta}})\)</span> autour de <span class="math inline">\(\boldsymbol{\beta}\)</span>,
<span class="math display">\[\begin{align*}
\boldsymbol{0}_{p+1} = U(\widehat{\boldsymbol{\beta}}) \stackrel{\cdot}{=} U(\beta) - j(\boldsymbol{\beta}) (\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta}).
\end{align*}\]</span>
Pour autant que la matrice <span class="math inline">\((p+1)\)</span> <span class="math inline">\(j(\boldsymbol{\beta}^{(t)})\)</span> soit invertible, on peut utiliser la procédure itérative suivante: en partant d’une valeur initiale <span class="math inline">\(\boldsymbol{\beta}^{(0)}\)</span>, on calcule à l’étape <span class="math inline">\(t+1\)</span>
<span class="math display">\[\begin{align*}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + j^{-1}(\boldsymbol{\beta}^{(t)})U(\boldsymbol{\beta}^{(t)}).
\end{align*}\]</span>
et on itère la formule jusqu’à convergence. La plupart des logiciels implémente une version de cet algorithme, dans lequel le négatif de la hessienne <span class="math inline">\(j(\boldsymbol{\beta})\)</span> est parfois remplacée par son espérance, <span class="math inline">\(i(\boldsymbol{\beta})\)</span>: l’algorithme résultant est dénommé score de Fisher. Pour les modèles linéaire généralisés, ces récursions peuvent être effectuées à l’aide d’une variante des moindres carrés connue sous le nom de moindres carrés itérativement pondérés.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="complement.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH60604_Modelisation_statistique.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
